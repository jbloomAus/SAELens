{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13280aba",
   "metadata": {},
   "source": [
    "# TemporalSAE Quickstart\n",
    "\n",
    "Dear Neuronpedia Team, thanks a lot for your support!\n",
    "\n",
    "We're providing the smallest SAE we have on Gemma-2-2B so far, to get started quickly. There's also LLama SAEs that'll probably perform better in Autointerp.\n",
    "\n",
    "The Temporal SAE builds on the prior that each LLM Activation x at position t is a sum two parts: some aggragation information at previous positions (x_pred) and novel information related to the input token at position t (x_novel), so x = x_pred + x_novel. We assume that only x_novel is sparse.\n",
    "\n",
    "The Temporal SAE encodes llm activation x_t at position t in two steps:\n",
    "1. Identify and remove information from the context: Apply an attention layer to reconstruct x_t only with context vectors {x_0, ..., x_{t-1}}. We'll call the attention layer output x_{t,pred}, and compute the remainder x_{t,novel} = x_t - x_{t,pred}.\n",
    "2. Feed x_{t,novel} (=\"novel_codes\") through a TopK SAE.\n",
    "\n",
    "The main focus is generating feature dashboards of the novel codes.\n",
    "\n",
    "If Neuronpedia has the resources, it would be great to run a cheap version of Autointerp on the novel codes, to verify the autointerp score is the ballpark of other SAE architectures.\n",
    "\n",
    "Aside for later: For natural lanugage inputs (not dummy_acts) it's also interesting to look at the similarity matrix kernel of the predicted codes, as shown in the paper and presentation. Displaying this on Neuronpedia is interesting, but not a priority. We can talk details later, if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ac90715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/can/dynamic_representations/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 4576.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded temporal SAEs to: /home/can/dynamic_representations/exp/temporal_saes_weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Define custom directory for temporal SAEs\n",
    "temporal_sae_dir = Path(\"./temporal_saes_weights\")\n",
    "temporal_sae_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download the temporal folder from the repository\n",
    "snapshot_download(\n",
    "    repo_id=\"ekdeepslubana/temporalSAEs\",\n",
    "    allow_patterns=\"temporal/*\",\n",
    "    local_dir=temporal_sae_dir,\n",
    "    local_dir_use_symlinks=False,\n",
    ")\n",
    "\n",
    "print(f\"Downloaded temporal SAEs to: {temporal_sae_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sqsq7fnzwbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "import os\n",
    "\n",
    "\n",
    "# Standalone TemporalSAE class definition\n",
    "def get_attention(query, key) -> th.Tensor:\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1))\n",
    "    attn_bias = th.zeros(L, S, dtype=query.dtype, device=query.device)\n",
    "    temp_mask = th.ones(L, S, dtype=th.bool, device=query.device).tril(diagonal=0)\n",
    "    attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "    attn_bias.to(query.dtype)\n",
    "\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = th.softmax(attn_weight, dim=-1)\n",
    "    return attn_weight\n",
    "\n",
    "\n",
    "### Manual Attention Implementation\n",
    "class ManualAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Manual implementation to allow tinkering with the attention mechanism.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimin,\n",
    "        n_heads=4,\n",
    "        bottleneck_factor=64,\n",
    "        bias_k=True,\n",
    "        bias_q=True,\n",
    "        bias_v=True,\n",
    "        bias_o=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dimin % (bottleneck_factor * n_heads) == 0\n",
    "\n",
    "        # attention heads\n",
    "        self.n_heads = n_heads\n",
    "        self.n_embds = dimin // bottleneck_factor  # n_heads\n",
    "        self.dimin = dimin\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.k_ctx = nn.Linear(dimin, self.n_embds, bias=bias_k)\n",
    "        self.q_target = nn.Linear(dimin, self.n_embds, bias=bias_q)\n",
    "        self.v_ctx = nn.Linear(dimin, dimin, bias=bias_v)\n",
    "\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(dimin, dimin, bias=bias_o)\n",
    "\n",
    "        # Normalize to match scale with representations\n",
    "        with th.no_grad():\n",
    "            scaling = 1 / math.sqrt(self.n_embds // self.n_heads)\n",
    "            self.k_ctx.weight.copy_(\n",
    "                scaling\n",
    "                * self.k_ctx.weight\n",
    "                / (1e-6 + th.linalg.norm(self.k_ctx.weight, dim=1, keepdim=True))\n",
    "            )\n",
    "            self.q_target.weight.copy_(\n",
    "                scaling\n",
    "                * self.q_target.weight\n",
    "                / (1e-6 + th.linalg.norm(self.q_target.weight, dim=1, keepdim=True))\n",
    "            )\n",
    "\n",
    "            scaling = 1 / math.sqrt(self.dimin // self.n_heads)\n",
    "            self.v_ctx.weight.copy_(\n",
    "                scaling\n",
    "                * self.v_ctx.weight\n",
    "                / (1e-6 + th.linalg.norm(self.v_ctx.weight, dim=1, keepdim=True))\n",
    "            )\n",
    "\n",
    "            scaling = 1 / math.sqrt(self.dimin)\n",
    "            self.c_proj.weight.copy_(\n",
    "                scaling\n",
    "                * self.c_proj.weight\n",
    "                / (1e-6 + th.linalg.norm(self.c_proj.weight, dim=1, keepdim=True))\n",
    "            )\n",
    "\n",
    "    def forward(self, x_ctx, x_target, get_attn_map=False):\n",
    "        \"\"\"\n",
    "        Compute projective attention output\n",
    "        \"\"\"\n",
    "        # Compute key and value projections from context representations\n",
    "        k = self.k_ctx(x_ctx)\n",
    "        v = self.v_ctx(x_ctx)\n",
    "\n",
    "        # Compute query projection from target representations\n",
    "        q = self.q_target(x_target)\n",
    "\n",
    "        # Split into heads\n",
    "        B, T, _ = x_ctx.size()\n",
    "        k = k.view(B, T, self.n_heads, self.n_embds // self.n_heads).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_heads, self.n_embds // self.n_heads).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.dimin // self.n_heads).transpose(1, 2)\n",
    "\n",
    "        # Attn map\n",
    "        if get_attn_map:\n",
    "            attn_map = get_attention(query=q, key=k)\n",
    "            th.cuda.empty_cache()\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_output = th.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=None, dropout_p=0, is_causal=True\n",
    "        )\n",
    "\n",
    "        # Reshape, project back to original dimension\n",
    "        d_target = self.c_proj(\n",
    "            attn_output.transpose(1, 2).contiguous().view(B, T, self.dimin)\n",
    "        )  # [batch, length, dimin]\n",
    "\n",
    "        if get_attn_map:\n",
    "            return d_target, attn_map\n",
    "        else:\n",
    "            return d_target, None\n",
    "\n",
    "\n",
    "class TemporalSAE(th.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimin=2,\n",
    "        width=5,\n",
    "        n_heads=8,\n",
    "        sae_diff_type=\"relu\",\n",
    "        kval_topk=None,\n",
    "        tied_weights=True,\n",
    "        n_attn_layers=1,\n",
    "        bottleneck_factor=64,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        dimin: (int)\n",
    "            input dimension\n",
    "        width: (int)\n",
    "            width of the encoder\n",
    "        n_heads: (int)\n",
    "            number of attention heads\n",
    "        sae_diff_type: (str)\n",
    "            type of sae to express the per-token difference\n",
    "        kval_topk: (int)\n",
    "            k in topk sae_diff_type\n",
    "        n_attn_layers: (int)\n",
    "            number of attention layers\n",
    "        \"\"\"\n",
    "        super(TemporalSAE, self).__init__()\n",
    "        self.sae_type = \"temporal\"\n",
    "        self.width = width\n",
    "        self.dimin = dimin\n",
    "        self.eps = 1e-6\n",
    "        self.lam = 1 / (4 * dimin)\n",
    "        self.tied_weights = tied_weights\n",
    "\n",
    "        ## Attention parameters\n",
    "        self.n_attn_layers = n_attn_layers\n",
    "        self.attn_layers = nn.ModuleList(\n",
    "            [\n",
    "                ManualAttention(\n",
    "                    dimin=width,\n",
    "                    n_heads=n_heads,\n",
    "                    bottleneck_factor=bottleneck_factor,\n",
    "                    bias_k=True,\n",
    "                    bias_q=True,\n",
    "                    bias_v=True,\n",
    "                    bias_o=True,\n",
    "                )\n",
    "                for _ in range(n_attn_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        ## Dictionary parameters\n",
    "        self.D = nn.Parameter(th.randn((width, dimin)))  # N(0,1) init\n",
    "        self.b = nn.Parameter(th.zeros((1, dimin)))\n",
    "        if not tied_weights:\n",
    "            self.E = nn.Parameter(th.randn((dimin, width)))  # N(0,1) init\n",
    "\n",
    "        ## SAE-specific parameters\n",
    "        self.sae_diff_type = sae_diff_type\n",
    "        self.kval_topk = kval_topk if sae_diff_type == \"topk\" else None\n",
    "\n",
    "    def forward(self, x_input, return_graph=False, inf_k=None):\n",
    "        B, L, _ = x_input.size()\n",
    "        E = self.D.T if self.tied_weights else self.E\n",
    "\n",
    "        ### Define context and target ###\n",
    "        x_input = x_input - self.b\n",
    "\n",
    "        ### Tracking variables ###\n",
    "        attn_graphs = []\n",
    "\n",
    "        ### Predictable part ###\n",
    "        z_pred = th.zeros(\n",
    "            (B, L, self.width), device=x_input.device, dtype=x_input.dtype\n",
    "        )\n",
    "        for attn_layer in self.attn_layers:\n",
    "            z_input = F.relu(th.matmul(x_input * self.lam, E))  # [batch, length, width]\n",
    "            z_ctx = th.cat(\n",
    "                (th.zeros_like(z_input[:, :1, :]), z_input[:, :-1, :].clone()), dim=1\n",
    "            )  # [batch, length, width]\n",
    "\n",
    "            # Compute codes using attention\n",
    "            z_pred_, attn_graphs_ = attn_layer(\n",
    "                z_ctx, z_input, get_attn_map=return_graph\n",
    "            )\n",
    "\n",
    "            # Take back to input space\n",
    "            z_pred_ = F.relu(z_pred_)\n",
    "            Dz_pred_ = th.matmul(z_pred_, self.D)\n",
    "            Dz_norm_ = Dz_pred_.norm(dim=-1, keepdim=True) + self.eps\n",
    "\n",
    "            # Compute projection\n",
    "            proj_scale = (Dz_pred_ * x_input).sum(dim=-1, keepdim=True) / Dz_norm_.pow(\n",
    "                2\n",
    "            )\n",
    "\n",
    "            # Add the projection to the reconstructed\n",
    "            z_pred = z_pred + (z_pred_ * proj_scale)\n",
    "\n",
    "            # Remove the projection from the input\n",
    "            x_input = x_input - proj_scale * Dz_pred_  # [batch, length, width]\n",
    "\n",
    "            # Add the attention graph if return_graph is True\n",
    "            if return_graph:\n",
    "                attn_graphs.append(attn_graphs_)\n",
    "\n",
    "        ### Novel part (identified using the residual target signal) ###\n",
    "        if self.sae_diff_type == \"relu\":\n",
    "            z_novel = F.relu(th.matmul(x_input * self.lam, E))\n",
    "\n",
    "        elif self.sae_diff_type == \"topk\":\n",
    "            kval = self.kval_topk if inf_k is None else inf_k\n",
    "            z_novel = F.relu(th.matmul(x_input * self.lam, E))\n",
    "            _, topk_indices = th.topk(z_novel, kval, dim=-1)\n",
    "            mask = th.zeros_like(z_novel)\n",
    "            mask.scatter_(-1, topk_indices, 1)\n",
    "            z_novel = z_novel * mask\n",
    "\n",
    "        elif self.sae_diff_type == \"nullify\":\n",
    "            z_novel = th.zeros_like(z_pred)\n",
    "\n",
    "        ### Reconstruction ###\n",
    "        x_recons = (\n",
    "            th.matmul(z_novel + z_pred, self.D) + self.b\n",
    "        )  # [batch, length, dimin]\n",
    "\n",
    "        ### Compute the predicted vs. novel reconstructions, sans the bias (allows to check context / dictionary's value) ###\n",
    "        with th.no_grad():\n",
    "            x_pred_recons = th.matmul(z_pred, self.D)\n",
    "            x_novel_recons = th.matmul(z_novel, self.D)\n",
    "\n",
    "        ### Return the dictionary ###\n",
    "        results_dict = {\n",
    "            \"novel_codes\": z_novel,\n",
    "            \"novel_recons\": x_novel_recons,\n",
    "            \"pred_codes\": z_pred,\n",
    "            \"pred_recons\": x_pred_recons,\n",
    "            \"attn_graphs\": th.stack(attn_graphs, dim=1) if return_graph else None,\n",
    "        }\n",
    "\n",
    "        return x_recons, results_dict\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, folder_path, dtype, device, **kwargs):\n",
    "        \"\"\"\n",
    "        Load a pretrained TemporalSAE from a folder containing conf.yaml and latest_ckpt.pt\n",
    "\n",
    "        Args:\n",
    "            folder_path: Path to folder containing conf.yaml and latest_ckpt.pt\n",
    "            dtype: Target dtype for the model\n",
    "            device: Target device for the model\n",
    "            **kwargs: Override any config parameters\n",
    "        \"\"\"\n",
    "        # Load config from yaml file\n",
    "        config_path = os.path.join(folder_path, \"conf.yaml\")\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "\n",
    "        # Extract model parameters from config\n",
    "        model_args = {\n",
    "            \"dimin\": config[\"llm\"][\"dimin\"],\n",
    "            \"width\": int(config[\"llm\"][\"dimin\"] * config[\"sae\"][\"exp_factor\"]),\n",
    "            \"n_heads\": config[\"sae\"][\"n_heads\"],\n",
    "            \"sae_diff_type\": config[\"sae\"][\"sae_diff_type\"],\n",
    "            \"kval_topk\": config[\"sae\"][\"kval_topk\"],\n",
    "            \"tied_weights\": config[\"sae\"][\"tied_weights\"],\n",
    "            \"n_attn_layers\": config[\"sae\"][\"n_attn_layers\"],\n",
    "            \"bottleneck_factor\": config[\"sae\"][\"bottleneck_factor\"],\n",
    "        }\n",
    "\n",
    "        # Override with any provided kwargs\n",
    "        model_args.update(kwargs)\n",
    "\n",
    "        # Create the model\n",
    "        autoencoder = cls(**model_args)\n",
    "\n",
    "        # Load the checkpoint\n",
    "        ckpt_path = os.path.join(folder_path, \"latest_ckpt.pt\")\n",
    "        checkpoint = th.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "        # Load the state dict\n",
    "        if \"sae\" in checkpoint:\n",
    "            autoencoder.load_state_dict(checkpoint[\"sae\"])\n",
    "        else:\n",
    "            autoencoder.load_state_dict(checkpoint)\n",
    "\n",
    "        autoencoder = autoencoder.to(device=device, dtype=dtype)\n",
    "        return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "r4uninzilkc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TemporalSAE from temporal_saes_weights/temporal\n",
      "Input dim: 2304, Width: 9216\n",
      "SAE type: topk, Attention layers: 1\n"
     ]
    }
   ],
   "source": [
    "# Load the temporal SAE\n",
    "sae_dir = temporal_sae_dir / \"temporal\"\n",
    "device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "dtype = th.float32\n",
    "\n",
    "sae = TemporalSAE.from_pretrained(folder_path=sae_dir, device=device, dtype=dtype)\n",
    "sae.eval()\n",
    "\n",
    "print(f\"Loaded TemporalSAE from {sae_dir}\")\n",
    "print(f\"Input dim: {sae.dimin}, Width: {sae.width}\")\n",
    "print(f\"SAE type: {sae.sae_diff_type}, Attention layers: {sae.n_attn_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eybfemq3tr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nExample usage:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAE forward:  29%|██▊       | 2/7 [00:00<00:00, 11.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAE forward: 100%|██████████| 7/7 [00:00<00:00, 13.31it/s]\n"
     ]
    }
   ],
   "source": [
    "@th.inference_mode()\n",
    "def batch_temporal_sae_inference(\n",
    "    sae, act_BPD, batch_size=32, device=\"cuda\", dtype=th.float32\n",
    "):\n",
    "    \"\"\"\n",
    "    Batched inference for temporal SAE\n",
    "\n",
    "    Args:\n",
    "        sae: TemporalSAE model\n",
    "        act_BPD: Input activations [B, P, D]\n",
    "        batch_size: Batch size for processing\n",
    "        device: Device to use\n",
    "        dtype: Data type\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing reconstruction outputs\n",
    "    \"\"\"\n",
    "    B, P, D = act_BPD.shape\n",
    "\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    for i in trange(0, B, batch_size, desc=\"SAE forward\"):\n",
    "        batch = act_BPD[i : i + batch_size]\n",
    "        batch = batch.to(device=device, dtype=dtype)\n",
    "\n",
    "        # Forward pass through temporal SAE\n",
    "        recon_BPD, batch_dict = sae.forward(batch, return_graph=True)\n",
    "        residuals_BPD = batch - recon_BPD\n",
    "\n",
    "        # Store results\n",
    "        results[\"novel_codes\"].append(batch_dict[\"novel_codes\"].detach().cpu())\n",
    "        # NOTE Only novel codes are relevant for AutoInterp\n",
    "        # results[\"total_recons\"].append(recon_BPD.detach().cpu())\n",
    "        # results[\"residuals\"].append(residuals_BPD.detach().cpu())\n",
    "        # results[\"novel_recons\"].append(batch_dict[\"novel_recons\"].detach().cpu())\n",
    "        # results[\"pred_codes\"].append(batch_dict[\"pred_codes\"].detach().cpu())\n",
    "        # results[\"pred_recons\"].append(batch_dict[\"pred_recons\"].detach().cpu())\n",
    "        # results[\"attn_graphs\"].append(batch_dict[\"attn_graphs\"].detach().cpu())\n",
    "\n",
    "        # # Add reconstruction bias\n",
    "        # results[\"novel_recons_plus_b\"].append(\n",
    "        #     (batch_dict[\"novel_recons\"] + sae.b).detach().cpu()\n",
    "        # )\n",
    "        # results[\"pred_recons_plus_b\"].append(\n",
    "        #     (batch_dict[\"pred_recons\"] + sae.b).detach().cpu()\n",
    "        # )\n",
    "\n",
    "    # Concatenate all batches\n",
    "    for key in results:\n",
    "        results[key] = th.cat(results[key], dim=0)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "print(\"\\\\nExample usage:\")\n",
    "dummy_acts = th.randn(100, 64, sae.dimin)  # [B=100, P=64, D=input_dim]\")\n",
    "results = batch_temporal_sae_inference(sae, dummy_acts, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bdf5ce",
   "metadata": {},
   "source": [
    "Dear Neuronpedia Team, thanks a lot for your support!\n",
    "\n",
    "The main focus is generating feature dashboards of the novel codes.\n",
    "\n",
    "If Neuronpedia has the resources, it would be great to run a cheap version of Autointerp, to verify the autointerp score is the ballpark of other SAE architectures.\n",
    "\n",
    "Aside for later: For natural lanugage inputs (not dummy_acts) it's also interesting to look at the similarity matrix kernel of the predicted codes, as shown in the paper and presentation. Displaying this on Neuronpedia is interesting, but not a priority. We can talk details later, if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee167d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0baf930",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-lens-9mfufXaM-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
