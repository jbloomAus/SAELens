{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relicating How We Train SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
    "from sae_lens.training.lm_runner import language_model_sae_runner\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gelu-1l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use test driven design. I'm going to put in the config args that should make the library replicate the SAE training result and then work backward from there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import plotly.express as px\n",
    "from sae_lens.training.sparse_autoencoder import SparseAutoencoder\n",
    "\n",
    "sparse_autoencoder = SparseAutoencoder(cfg)\n",
    "px.histogram(sparse_autoencoder.W_dec.norm(dim=1).detach().cpu()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.training.optim import L1Scheduler\n",
    "\n",
    "total_training_steps = 20_000\n",
    "l1_warmup_steps = 1_000\n",
    "final_l1_value = sparse_autoencoder.cfg.l1_coefficient\n",
    "\n",
    "l1_scheduler = L1Scheduler(\n",
    "    total_steps=sparse_autoencoder.cfg.training_tokens // sparse_autoencoder.cfg.train_batch_size,\n",
    "    l1_warm_up_steps=l1_warmup_steps,\n",
    "    sparse_autoencoder=sparse_autoencoder\n",
    ")\n",
    "\n",
    "l1_values = []\n",
    "for _ in range(total_training_steps):\n",
    "    l1_values.append(sparse_autoencoder.l1_coefficient)\n",
    "    l1_scheduler.step()\n",
    "        \n",
    "px.line(y=l1_values).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import ActivationsStore\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gelu-1l\")\n",
    "\n",
    "activation_store = ActivationsStore.from_config(model, sparse_autoencoder.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.d_in ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_store.estimated_norm_scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_store.get_buffer(32).norm(dim=-1).flatten().detach().cpu().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(activation_store.get_buffer(32).norm(dim=-1).flatten().detach().cpu()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activation_store.next_batch()\n",
    "\n",
    "feature_acts, hidden_pre = sparse_autoencoder._encode_with_hidden_pre(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_autoencoder.get_sparsity_loss_term_decoder_norm(feature_acts).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_autoencoder.cfg.n_batches_in_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "buffer_norm_means = []\n",
    "for _ in range(10):\n",
    "    buffer_norm_means.append(activation_store.get_buffer(64).squeeze().norm(dim=1).mean().item())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(x=buffer_norm_means, \n",
    "        title = f\"Buffer Norm Mean over 10 batches, mean: {np.array(buffer_norm_means).mean()} std:{np.array(buffer_norm_means).std()}\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.sqrt(sparse_autoencoder.d_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
