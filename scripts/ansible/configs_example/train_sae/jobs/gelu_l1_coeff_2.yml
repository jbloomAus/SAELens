# Name your Train SAE Job
# This job name should match the file name.
# Job are tagged on AWS by names, which allows you to run multiple jobs.
# NO DASHES IN JOB NAMES. Underscores are fine.
job_name: gelu_l1_coeff_2
wandb_project: "gelu_l1"

# The name of your completed Cache Activation job. Must match exactly.
cache_acts_job_name: gelu_1l_500

# IMPORTANT
# YAML 1.1 spec requires scientific notation to include the decimal to be parsed as a number
# 1.0e-4 is correct and will be parsed as a number. 1e-4 is NOT correct and will be parsed as a string.

# Overview on GPU vs Instance Types: https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html
# Instance Specs and Pricing: https://instances.vantage.sh/?cost_duration=daily
# g6.xlarge seems to work fine for gelu-1l, c4-tokenized-2b, 200_000 training steps, batch size 4096
instance_type: g6.xlarge

# this should match the cache activations
total_training_steps: 2_000

# set these relative to your total_training_steps
l1_warm_up_steps: 0
lr_warm_up_steps: 100
lr_decay_steps: 400

model_name: gelu-1l
hook_point: blocks.0.hook_mlp_out
hook_point_layer: 0
d_in: 512
dataset_path: NeelNanda/c4-tokenized-2b
streaming: False
context_size: 1024
is_dataset_tokenized: True
prepend_bos: True
# How big do we want our SAE to be?
expansion_factor: 64
# Dataset / Activation Store
# When we do a proper test
# training_tokens= 820_000_000, # 200k steps * 4096 batch size ~ 820M tokens (doable overnight on an A100)
# For now.
use_cached_activations: True
# training_tokens: total_training_tokens # this will be overwritten by total_training_steps * train_batch_size
train_batch_size: 4096
# Loss Function
## Reconstruction Coefficient.
mse_loss_normalization: None  # MSE Loss Normalization is not mentioned (so we use stanrd MSE Loss). But not we take an average over the batch.
## Anthropic does not mention using an Lp norm other than L1.
l1_coefficient: 2
lp_norm: 1.0
# Instead, they multiply the L1 loss contribution
# from each feature of the activations by the decoder norm of the corresponding feature.
scale_sparsity_penalty_by_decoder_norm: True
# Learning Rate
lr_scheduler_name: "constant"  # we set this independently of warmup and decay steps.
## No ghost grad term.
use_ghost_grads: False
# Initialization / Architecture
apply_b_dec_to_input: False
# encoder bias zero's. (I'm not sure what it is by default now)
# decoder bias zero's.
b_dec_init_method: zeros
normalize_sae_decoder: False
decoder_heuristic_init: True
init_encoder_as_decoder_transpose: True
# Optimizer
lr: 5.0e-5
## adam optimizer has no weight decay by default so worry about this.
adam_beta1: 0.9
adam_beta2: 0.999
# Buffer details won't matter in we cache / shuffle our activations ahead of time.
n_batches_in_buffer: 64
store_batch_size: 16
normalize_activations: False
# Feature Store
feature_sampling_window: 1000
dead_feature_window: 1000
dead_feature_threshold: 1.0e-4
# WANDB
log_to_wandb: true
wandb_log_frequency: 50
eval_every_n_wandb_logs: 10
# Misc
seed: 42
n_checkpoints: 0
checkpoint_path: "checkpoints"
dtype: torch.float32