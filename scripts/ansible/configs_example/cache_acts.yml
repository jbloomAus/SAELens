#################### CHANGE THE FOLLOWING

# Name your Cache Activations job.
# Job are tagged on AWS by names, which allows you to run multiple jobs.
# NO DASHES IN JOB NAMES. Underscores are fine.
job_name: gelu_1l_500

# Overview on GPU vs Instance Types: https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html
# Instance Specs and Pricing: https://instances.vantage.sh/?cost_duration=daily
# g6.12xlarge seems to work fine for gelu-1l, c4-tokenized-2b, 200_000 training steps, batch size 4096
instance_type: g6.xlarge

total_training_steps: 2_100

model_name: gelu-1l
hook_point: blocks.0.hook_mlp_out
hook_point_layer: 0
d_in: 512
dataset_path: NeelNanda/c4-tokenized-2b
context_size: 1024
is_dataset_tokenized: true
prepend_bos: true
train_batch_size: 4096
n_batches_in_buffer: 4
store_batch_size: 128
normalize_activations: none
shuffle_every_n_buffers: 8
n_shuffles_with_last_section: 1
n_shuffles_in_entire_dir: 1
n_shuffles_final: 1
seed: 42
dtype: torch.float16

############################# Values that will be set by the Ansible script
# device: cuda # this is ignored and instead set by the python code
# training_tokens: 81920000  # this is ignored and instead calculated by total_training_steps * train_batch_size