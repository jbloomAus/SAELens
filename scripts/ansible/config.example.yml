
#################### CHANGE THE FOLLOWING

########## Common Parameters
# This is the bucket where the activation caches and checkpoints will be stored.
# It will automatically be created if it doesn't exist.
# Choose a unique name - bucket names must be globally unique across all AWS accounts.
# 
# Paths
# - Activation Caches: {bucket_root}/cached_activations/{model_name}/{dataset_path}/{total_training_steps}
# - Checkpoints: {bucket_root}/checkpoints/{checkpoint_id}
#
# Keep the bucket name the same if you want to use the same activation cache.
# Note that AWS has a limit on the number of buckets per account.
# 
# Allows: lowercase letters, numbers, dashes, and dots
# Example: johnny.saes.1234
s3_bucket_name: johnny.saes.1234

# This image is only for us-east-1. You do not need to update it frequently.
# Deep Learning OSS Nvidia Driver AMI GPU Pytorch 2.2.0 (Amazon Linux 2) 20240507
ec2_image: ami-0fcc44e33ff6e4a5f

########## Cache Activations Job Parameters

# Name your Cache Activations job.
# Job are tagged on AWS by names, which allows you to run multiple jobs.
# NO DASHES IN JOB NAMES. Underscores are fine.
cache_acts_job_name: gelu_1l_test_500
# Overview on GPU vs Instance Types: https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html
# Instance Specs and Pricing: https://instances.vantage.sh/?cost_duration=daily
# g6.12xlarge seems to work fine for gelu-1l, c4-tokenized-2b, 200_000 training steps, batch size 4096
cache_acts_instance_type: g6.xlarge
total_training_steps: 500

model_name: gelu-1l
hook_point: blocks.0.hook_mlp_out
hook_point_layer: 0
d_in: 512
dataset_path: NeelNanda/c4-tokenized-2b
context_size: 1024
is_dataset_tokenized: true
prepend_bos: true
# training_tokens: 81920000  # this is ignored and instead calculated by total_training_steps * train_batch_size
train_batch_size: 4096
n_batches_in_buffer: 4
store_batch_size: 128
normalize_activations: false
shuffle_every_n_buffers: 8
n_shuffles_with_last_section: 1
n_shuffles_in_entire_dir: 1
n_shuffles_final: 1
# device: cuda # this is ignored and instead set by the python code
seed: 42
dtype: torch.float16


########## Train SAE Job Parameters

# Name your Train SAE Job
# Job are tagged on AWS by names, which allows you to run multiple jobs.
# NO DASHES IN JOB NAMES. Underscores are fine.
train_sae_job_name: residuals


# Change these depending on your job
train_sae_instance_type: g6.xlarge
saelens_version_or_branch: automation




#################### DON'T CHANGE THE FOLLOWING

# You should keep the following the same
ssh_key_filename: "saelens_ansible"
ssh_key_path: "~/.ssh/{{ ssh_key_filename }}"
iam_role_name: "saelens-iam-role"
sec_group: "ssh-only"
local_s3_mount_path: "/mnt/s3"
# If you change these tag names you need to change it in aws_ec2.yml too
instance_tag_service_cache_acts: "cache_acts"
instance_tag_service_train_sae: "train_sae"