---

- name: Set up AWS security group, key, IAM role, S3 bucket
  hosts: localhost
  connection: local
  gather_facts: false
  tags:
    - setup
    - cache_acts
  vars_files:
    - config.yml

  tasks:
    - name: Make Common EC2 Security Group, Key
      block:

        - name: "Generate SSH key {{ ssh_key_path }}"
          community.crypto.openssh_keypair:
            path: "{{ ssh_key_path }}"
            type: rsa
            size: 4096
            state: present
            force: false

        - name: Get security groups
          amazon.aws.ec2_security_group_info:
            filters:
              group-name: "{{ sec_group }}"
          register: ec2_security_group_infos

        - name: Create security group
          amazon.aws.ec2_security_group:
            name: "{{ sec_group }}"
            description: "SSH only"
            rules:
              - proto: tcp
                ports:
                  - 22
                cidr_ip: 0.0.0.0/0
                rule_desc: allow all on ssh port
          when: ec2_security_group_infos.security_groups == []

        - name: Get the ssh key info
          amazon.aws.ec2_key_info:
            names:
              - "{{ ssh_key_filename }}"
          register: ec2_key_infos

        - name: Create Key Pair for SSH if not exists
          amazon.aws.ec2_key:
            name: "{{ ssh_key_filename }}"
            key_material: "{{ item }}"
          with_file: "~/.ssh/{{ ssh_key_filename }}.pub"
          when: ec2_key_infos.keypairs == []
          no_log: true

        - name: Get the IAM role info
          amazon.aws.iam_role_info:
            name: "{{ iam_role_name }}"
          register: iam_roles

        - name: "Create a role and attach the S3 access policy"
          amazon.aws.iam_role:
            name: "{{ iam_role_name }}"
            create_instance_profile: true
            assume_role_policy_document: "{{ lookup('file', 'aws_s3_access_doc.json') }}"
            managed_policy:
              - arn:aws:iam::aws:policy/AmazonS3FullAccess
          when: iam_roles.iam_roles == []

    - name: Make S3 Bucket
      block:

        - name: Look for existing S3 bucket
          amazon.aws.s3_bucket_info:
            name: "{{ s3_bucket_name }}"
          register: s3_bucket

        - name: Create S3 bucket
          amazon.aws.s3_bucket:
            name: "{{ s3_bucket_name }}"
          when: s3_bucket.buckets == []

- name: "Cache Activations: Initialize - Update config file and save it under jobs"
  hosts: localhost
  gather_facts: false
  tags:
    - cache_acts
    - cache_acts_init
  vars:
    input_yaml_path: "config.yml"
    output_yaml_dir_path: "jobs/cache_acts/{{ cache_acts_job_name }}"
    output_yaml_path: "{{ output_yaml_dir_path }}/config.yml"
    new_cached_activations_path_key: "new_cached_activations_path"
    new_cached_activations_path_value: "{{ local_s3_mount_path }}/{{ s3_bucket_name }}/cached_activations/{{ cache_acts_job_name }}"
    training_tokens_key: "training_tokens"
    training_token_value: "{{ (total_training_steps | int * train_batch_size | int) | int }}"
  vars_files:
    - config.yml
  tasks:

    - name: Check that the activation cache doesn't already exist on S3
      amazon.aws.s3_object:
        bucket: "{{ s3_bucket_name }}"
        prefix: "cached_activations/{{ cache_acts_job_name }}"
        mode: list
      register: s3_list

    - name: Fail the playbook if the directory already exists
      ansible.builtin.fail:
        msg: "Error: The directory 'cached_activations/{{ cache_acts_job_name }}' already exists \
                in the bucket '{{ s3_bucket_name }}'. Specify a different job name or move/delete the existing directory on S3."
      when: s3_list.s3_keys | length > 0

    - name: Load the YAML file into a variable for modification
      ansible.builtin.slurp:
        path: "{{ input_yaml_path }}"
      register: config_yaml_file

    - name: Read yaml to dictionary
      ansible.builtin.set_fact:
        config_yaml_content: "{{ config_yaml_file['content'] | b64decode | from_yaml }}"

    - name: Update the values
      ansible.builtin.set_fact:
        updated_config_yaml_content: >-
          {{
            config_yaml_content | combine({
              training_tokens_key: training_token_value | int,
              new_cached_activations_path_key: new_cached_activations_path_value
            })
          }}

    - name: Create the job history directory
      ansible.builtin.file:
        path: "{{ output_yaml_dir_path }}"
        state: directory
        recurse: true
        mode: '0766'

    - name: Write back the modified YAML to a new file
      ansible.builtin.copy:
        content: "{{ updated_config_yaml_content | to_nice_yaml }}"
        dest: "{{ output_yaml_path }}"
        mode: '0644'

    - name: Upload the YAML to AWS too
      amazon.aws.s3_object:
        bucket: "{{ s3_bucket_name }}"
        object: "cached_activations/{{ cache_acts_job_name }}-config.yml"
        src: "{{ output_yaml_path }}"
        mode: "put"

- name: "Cache Activations: Launch instance for job {{ cache_acts_job_name }}"
  hosts: localhost
  connection: local
  gather_facts: false
  tags:
    - cache_acts
  vars_files:
    - config.yml

  tasks:
    - name: "Get EC2 instances for job {{ cache_acts_job_name }}"
      block:
        - name: Get EC2 instance information
          amazon.aws.ec2_instance_info:
            filters:
              "tag:service": "{{ instance_tag_service_cache_acts }}"
              "tag:job": "{{ cache_acts_job_name }}"
              instance-state-name: running
          register: cache_acts_instances

    - name: Start Cache Activations Instance if not running, wait for it
      amazon.aws.ec2_instance:
        name: "Cache Acts {{ cache_acts_job_name }}"
        key_name: "{{ ssh_key_filename }}"
        iam_instance_profile: "{{ iam_role_name }}"
        instance_type: "{{ cache_acts_instance_type }}"
        security_group: "{{ sec_group }}"
        wait: true
        state: running
        count: 1
        instance_initiated_shutdown_behavior: terminate
        network:
          assign_public_ip: true
        image_id: "{{ ec2_image }}"
        tags:
          service: "{{ instance_tag_service_cache_acts }}"
          job: "{{ cache_acts_job_name }}"
      when: cache_acts_instances.instances == []
      register: new_cache_acts_instances

    - name: Set the EC2 instance information again (in case we did make a new instance)
      amazon.aws.ec2_instance_info:
        filters:
          "tag:service": "{{ instance_tag_service_cache_acts }}"
          "tag:job": "{{ cache_acts_job_name }}"
          instance-state-name: running
      register: new_cache_acts_instances

    - name: Wait for SSH to come up
      ansible.builtin.wait_for:
        port: 22
        host: "{{ new_cache_acts_instances.instances[0].network_interfaces[0].association.public_dns_name }}"
        delay: 10
        sleep: 10
        timeout: 720
        state: started
      loop: "{{ new_cache_acts_instances.instances }}"

    - name: Refresh inventory to detect the new instance
      ansible.builtin.meta: refresh_inventory


- name: Configure Cache Activations Instance
  hosts: tag_service__{{ instance_tag_service_cache_acts }}:&tag_job__{{ cache_acts_job_name }}
  gather_facts: true
  tags:
    - cache_acts
    - cache_acts_config
  vars:
    ansible_user: ec2-user
    ansible_ssh_private_key_file: "{{ ssh_key_path }}"
    ansible_python_interpreter: auto_silent
    instance_storage_path: /mnt/disk
    s3_local_cache_path: "{{ instance_storage_path }}/s3-local-cache"
  vars_files:
    - config.yml

  tasks:

    - name: Wait for connection again
      wait_for_connection:
        delay: 0
        timeout: 300

    - name: Configure instance storage (for faster I/O on S3 caches vs EBS)
      block:

        - name: Make instance storage mount directory
          ansible.builtin.file:
            path: "{{ instance_storage_path }}"
            state: directory
            owner: ec2-user
            group: ec2-user
            mode: '0755'
          become: true
          become_user: root

        - name: Make the instance storage file system
          community.general.filesystem:
            fstype: xfs
            dev: /dev/nvme1n1
          become: true
          become_user: root

        - name: Mount the instance storage
          ansible.posix.mount:
            path: "{{ instance_storage_path }}"
            src: /dev/nvme1n1
            fstype: xfs
            opts: defaults
            state: mounted
          become: true
          become_user: root

        - name: Make the cache directory
          ansible.builtin.file:
            path: "{{ s3_local_cache_path }}"
            state: directory
            owner: ec2-user
            group: ec2-user
            mode: '0755'
          become: true
          become_user: root

    - name: Configure S3 Mount
      block:

        - name: Download AWS mountpoint
          ansible.builtin.get_url:
            url: https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm
            dest: /home/ec2-user/mount-s3.rpm
            mode: '0444'

        - name: Install AWS mountpoint
          ansible.builtin.yum:
            name: /home/ec2-user/mount-s3.rpm
          become: true
          become_user: root

        - name: Make S3 mount directory
          ansible.builtin.file:
            path: "{{ local_s3_mount_path }}/{{ s3_bucket_name }}"
            state: directory
            owner: root
            group: root
            mode: '0777'
          become: true
          become_user: root

        # We have to do this in this insane way for some reason.
        # The mount-s3 command doesn't work if we try to execute it on the instance directly,
        # so we ask Ansible to run a literal ssh command from local machine.
        # Either way we're just happy it works.
        - name: Mount S3 Bucket
          delegate_to: localhost
          ansible.builtin.raw: "ssh ec2-user@{{ inventory_hostname }} 'mount-s3 {{ s3_bucket_name }} {{ local_s3_mount_path }}/{{ s3_bucket_name }} \
                    --allow-overwrite \
                    --allow-delete \
                    --uid 1000 \
                    --gid 1000 \
                    --cache {{ s3_local_cache_path }}'"

    - name: Install SAELens
      block:

        - name: "Git checkout SAELens {{ saelens_version_or_branch }}"
          ansible.builtin.git:
            repo: 'https://github.com/jbloomAus/SAELens.git'
            dest: /home/ec2-user/SAELens
            version: "{{ saelens_version_or_branch }}"

        - name: Install poetry
          ansible.builtin.pip:
            name: poetry

        - name: Poetry lock
          ansible.builtin.shell:
            cmd: poetry config --local virtualenvs.in-project true && poetry lock
            chdir: /home/ec2-user/SAELens
          changed_when: true

        - name: Poetry install (this will take a few minutes)
          ansible.builtin.command:
            cmd: poetry install
            chdir: /home/ec2-user/SAELens
          changed_when: true

- name: Run Cache Activations Job
  hosts: tag_service__{{ instance_tag_service_cache_acts }}:&tag_job__{{ cache_acts_job_name }}
  gather_facts: true
  tags:
    - cache_acts
    - cache_acts_job
  vars:
    ansible_user: ec2-user
    ansible_ssh_private_key_file: "{{ ssh_key_path }}"
    ansible_python_interpreter: "/home/ec2-user/SAELens/.venv/bin/python"
    cache_acts_dir: "/home/ec2-user/SAELens/scripts/ansible/jobs/cache_acts"
  vars_files:
    - config.yml

  tasks:
    - name: Make the job directory
      ansible.builtin.file:
        path: "{{ cache_acts_dir }}/{{ cache_acts_job_name }}"
        state: directory
        owner: ec2-user
        group: ec2-user
        mode: '0777'
        recurse: true

    - name: Copy job config to the instance
      ansible.builtin.copy:
        src: "jobs/cache_acts/{{ cache_acts_job_name }}/config.yml"
        dest: "{{ cache_acts_dir }}/{{ cache_acts_job_name }}/config.yml"
        owner: ec2-user
        group: ec2-user
        mode: '0777'

    - name: Run Cache Activations Job
      ansible.builtin.command:
        cmd: "poetry run python cache_activations.py {{ cache_acts_job_name }}"
        chdir: /home/ec2-user/SAELens/scripts/ansible
      register: cache_acts_job_output
      changed_when: true

    - name: Finished job, terminate the instance
      ansible.builtin.command:
        cmd: shutdown -h +3
      become: true
      become_user: root
      changed_when: true


# ============ to do below

# - name: "Train SAE: Initialize - Update config file and save it under jobs"
  # hosts: localhost
  # gather_facts: false
  # tags:
  #   - train_sae
  #   - train_sae_init
  # vars:
  #   input_yaml_path: "config.yml"
  #   output_yaml_dir_path: "jobs/cache_acts/{{ cache_acts_job_name }}"
  #   output_yaml_path: "{{ output_yaml_dir_path }}/config.yml"
  #   new_cached_activations_path_key: "new_cached_activations_path"
  #   new_cached_activations_path_value: "{{ local_s3_mount_path }}/{{ s3_bucket_name }}/cached_activations/{{ cache_acts_job_name }}"
  #   training_tokens_key: "training_tokens"
  #   training_token_value: "{{ (total_training_steps | int * train_batch_size | int) | int }}"
  # vars_files:
  #   - config.yml
  # tasks:

  #   - name: Check that the activation cache doesn't already exist on S3
  #     amazon.aws.s3_object:
  #       bucket: "{{ s3_bucket_name }}"
  #       prefix: "cached_activations/{{ cache_acts_job_name }}"
  #       mode: list
  #     register: s3_list

  #   - name: Fail the playbook if the directory already exists
  #     ansible.builtin.fail:
  #       msg: "Error: The directory 'cached_activations/{{ cache_acts_job_name }}' already exists \
  #               in the bucket '{{ s3_bucket_name }}'. Specify a different job name or move/delete the existing directory on S3."
  #     when: s3_list.s3_keys | length > 0

  #   - name: Load the YAML file into a variable for modification
  #     ansible.builtin.slurp:
  #       path: "{{ input_yaml_path }}"
  #     register: config_yaml_file

  #   - name: Read yaml to dictionary
  #     ansible.builtin.set_fact:
  #       config_yaml_content: "{{ config_yaml_file['content'] | b64decode | from_yaml }}"

  #   - name: Update the values
  #     ansible.builtin.set_fact:
  #       updated_config_yaml_content: >-
  #         {{
  #           config_yaml_content | combine({
  #             training_tokens_key: training_token_value | int,
  #             new_cached_activations_path_key: new_cached_activations_path_value
  #           })
  #         }}

  #   - name: Create the job history directory
  #     ansible.builtin.file:
  #       path: "{{ output_yaml_dir_path }}"
  #       state: directory
  #       recurse: true
  #       mode: '0766'

  #   - name: Write back the modified YAML to a new file
  #     ansible.builtin.copy:
  #       content: "{{ updated_config_yaml_content | to_nice_yaml }}"
  #       dest: "{{ output_yaml_path }}"
  #       mode: '0644'

  #   - name: Upload the YAML to AWS too
  #     amazon.aws.s3_object:
  #       bucket: "{{ s3_bucket_name }}"
  #       object: "cached_activations/{{ cache_acts_job_name }}-config.yml"
  #       src: "{{ output_yaml_path }}"
  #       mode: "put"