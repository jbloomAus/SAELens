{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook with Example Config for Different Models / Hooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning: This notebook is a WIP and may not reflect current valid / optimal hyperparameters.\n",
    "\n",
    "# We are hoping to provide more serious training examples / advice soon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sae_lens import (\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    LoggingConfig,\n",
    "    StandardTrainingSAEConfig,\n",
    ")\n",
    "from sae_lens.llm_sae_training_runner import LanguageModelSAETrainingRunner\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Stories - 1L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    sae=StandardTrainingSAEConfig(\n",
    "        d_in=1024,  # the width of the mlp output.\n",
    "        d_sae=16384,  # the width of the SAE.\n",
    "        l1_coefficient=0.0015,  # will control how sparse the feature activations are\n",
    "        lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n",
    "    ),\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"tiny-stories-1L-21M\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "    hook_name=\"blocks.0.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n",
    "    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.\n",
    "    is_dataset_tokenized=True,\n",
    "    # SAE Parameters\n",
    "    # Training Parameters\n",
    "    lr=0.0008,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=10000,  # this can help avoid too many dead features initially.\n",
    "    train_batch_size_tokens=4096,\n",
    "    context_size=128,  # will control the lenght of the prompts we feed to the model. Larger is better but slower.\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n",
    "    training_tokens=1_000_000\n",
    "    * 25,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    store_batch_size_prompts=32,\n",
    "    # Resampling protocol\n",
    "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
    "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
    "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
    "    # WANDB\n",
    "    logger=LoggingConfig(\n",
    "        log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "        wandb_project=\"sae_lens_tutorial\",\n",
    "        wandb_log_frequency=10,\n",
    "    ),\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "# look at the next cell to see some instruction for what to do while this is running.\n",
    "sae = LanguageModelSAETrainingRunner(cfg).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 - Small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    sae=StandardTrainingSAEConfig(\n",
    "        d_in=768,\n",
    "        d_sae=32 * 768,\n",
    "        l1_coefficient=0.008,\n",
    "    ),\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"gpt2-small\",\n",
    "    hook_name=\"blocks.8.hook_resid_pre\",\n",
    "    dataset_path=\"apollo-research/Skylion007-openwebtext-tokenizer-gpt2\",\n",
    "    is_dataset_tokenized=True,\n",
    "    prepend_bos=True,  # should experiment with turning this off.\n",
    "    # SAE Parameters\n",
    "    # Training Parameters\n",
    "    lr=0.0004,\n",
    "    lr_scheduler_name=\"constant\",\n",
    "    train_batch_size_tokens=4096,\n",
    "    context_size=256,\n",
    "    lr_warm_up_steps=5000,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=128,\n",
    "    training_tokens=1_000_000 * 200,  # 200M tokens seems doable overnight.\n",
    "    store_batch_size_prompts=32,\n",
    "    feature_sampling_window=2500,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold=1e-8,\n",
    "    # WANDB\n",
    "    logger=LoggingConfig(\n",
    "        log_to_wandb=True,\n",
    "        wandb_project=\"gpt2_small_experiments_april\",\n",
    "        wandb_entity=None,\n",
    "        wandb_log_frequency=100,\n",
    "    ),\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=5,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "sae = LanguageModelSAETrainingRunner(cfg).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-lens-CSfAEFdT-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
