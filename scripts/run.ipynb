{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook with Example Config for Different Models / Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning: This notebook is a WIP and may not reflect current valid / optimal hyperparameters.\n",
    "# We are hoping to provide more serious training examples / advice soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
    "from sae_lens.training.lm_runner import language_model_sae_runner\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Stories - 1L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 16384-L1-0.0015-LR-0.0008-Tokens-2.500e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.002048\n",
      "Total training steps: 6103\n",
      "Total wandb updates: 610\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 6 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Loaded pretrained model tiny-stories-1L-21M into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Run name: 16384-L1-0.0015-LR-0.0008-Tokens-2.500e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.002048\n",
      "Total training steps: 6103\n",
      "Total wandb updates: 610\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 6 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Run name: 16384-L1-0.0015-LR-0.0008-Tokens-2.500e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.002048\n",
      "Total training steps: 6103\n",
      "Total wandb updates: 610\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 6 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/mats_sae_training/scripts/wandb/run-20240416_135218-opqs9dgl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/sae_lens_tutorial/runs/opqs9dgl' target=\"_blank\">16384-L1-0.0015-LR-0.0008-Tokens-2.500e+07</a></strong> to <a href='https://wandb.ai/jbloom/sae_lens_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/sae_lens_tutorial' target=\"_blank\">https://wandb.ai/jbloom/sae_lens_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/sae_lens_tutorial/runs/opqs9dgl' target=\"_blank\">https://wandb.ai/jbloom/sae_lens_tutorial/runs/opqs9dgl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective value: 1781464.6250:   4%|▍         | 4/100 [00:00<00:00, 206.25it/s]\n",
      "/home/paperspace/mats_sae_training/sae_lens/training/sparse_autoencoder.py:176: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n",
      "135| MSE Loss 0.257 | L1 1.354:   1%|          | 552960/50000000 [00:13<19:08, 43042.90it/s]  /home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (v0kr8hz9) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "6104| MSE Loss 0.072 | L1 0.024: : 25001984it [18:07, 22981.57it/s]\n",
      "12208| MSE Loss 0.070 | L1 0.024: 100%|█████████▉| 49999872/50000000 [20:15<00:00, 30551.50it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb94759b99e14133aece0058a423e305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='128.448 MB of 128.448 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████████</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▄▄▅▆▆▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▅▂▅▅█▆▆▇▅▅▆▅▄▅▅▄▅▃▄▄▄▄▃▆▆▄▁▄▆▃▆▃▅▆▂▃▆▄▃▅</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>█▅▅▄▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▆▂▂▁▇▄▇▆▃▃▅▅▆▇▃▆▃▄▄▄█▂▂▄▄▃▁▅▄▄▂▅▄█▃▄▄▄▅▆</td></tr><tr><td>metrics/explained_variance</td><td>▁▄▄▆▆▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>▆▁▄▇███▇▇▆▆▆▆▆▆▆▆▅▅▅▅▅▄▅▄▄▅▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>metrics/l0</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l2_norm</td><td>█▁▂▃▄▄▃▅▅▄▅▅▄▅▄▅▅▅▅▄▆▅▆▅▆▆▅▆▆▇█▇▇▆▆▇▆▆▇▇</td></tr><tr><td>metrics/l2_ratio</td><td>█▁▂▃▃▃▃▄▄▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▆▆▅▆▆▆▆▆▆▅▆▆▅▆▆▆</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▅▃▃▂▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▁▁▂▆███▅██</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▁▁▁█▇█▂▆▆</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▅▅▁▁▁▁▅▁▁▁▁▁▅▅▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▂▃▂▄▄▅▇▇▇▇██▆▇▆███▆▅▅█▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0008</td></tr><tr><td>details/n_training_tokens</td><td>49971200</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>15.59199</td></tr><tr><td>losses/mse_loss</td><td>0.07019</td></tr><tr><td>losses/overall_loss</td><td>0.09358</td></tr><tr><td>metrics/CE_loss_score</td><td>0.86351</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>8.5168</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>3.00156</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>2.12988</td></tr><tr><td>metrics/explained_variance</td><td>0.56934</td></tr><tr><td>metrics/explained_variance_std</td><td>0.14386</td></tr><tr><td>metrics/l0</td><td>19.32129</td></tr><tr><td>metrics/l2_norm</td><td>15.93428</td></tr><tr><td>metrics/l2_ratio</td><td>0.86545</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-4.81775</td></tr><tr><td>sparsity/below_1e-5</td><td>6329</td></tr><tr><td>sparsity/below_1e-6</td><td>81</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>29.02307</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">16384-L1-0.0015-LR-0.0008-Tokens-2.500e+07</strong> at: <a href='https://wandb.ai/jbloom/sae_lens_tutorial/runs/opqs9dgl' target=\"_blank\">https://wandb.ai/jbloom/sae_lens_tutorial/runs/opqs9dgl</a><br/> View project at: <a href='https://wandb.ai/jbloom/sae_lens_tutorial' target=\"_blank\">https://wandb.ai/jbloom/sae_lens_tutorial</a><br/>Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_135218-opqs9dgl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12208| MSE Loss 0.070 | L1 0.024: : 50003968it [20:27, 30551.50it/s]                            /home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (opqs9dgl) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    }
   ],
   "source": [
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"tiny-stories-1L-21M\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "    hook_point=\"blocks.0.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n",
    "    hook_point_layer=0,  # Only one layer in the model.\n",
    "    d_in=1024,  # the width of the mlp output.\n",
    "    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.\n",
    "    is_dataset_tokenized=True,\n",
    "    # SAE Parameters\n",
    "    mse_loss_normalization=None,  # We won't normalize the mse loss,\n",
    "    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.\n",
    "    b_dec_init_method=\"geometric_median\",  # The geometric median can be used to initialize the decoder weights.\n",
    "    apply_b_dec_to_input=False,  # We won't apply the decoder to the input.\n",
    "    # Training Parameters\n",
    "    lr=0.0008,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=10000,  # this can help avoid too many dead features initially.\n",
    "    l1_coefficient=0.0015,  # will control how sparse the feature activations are\n",
    "    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n",
    "    train_batch_size=4096,\n",
    "    context_size=128,  # will control the lenght of the prompts we feed to the model. Larger is better but slower.\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n",
    "    training_tokens=1_000_000\n",
    "    * 25,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    finetuning_method=\"decoder\",\n",
    "    finetuning_tokens=1_000_000 * 25,\n",
    "    store_batch_size=32,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=False,\n",
    "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
    "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
    "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
    "    # WANDB\n",
    "    log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"sae_lens_tutorial\",\n",
    "    wandb_log_frequency=10,\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# look at the next cell to see some instruction for what to do while this is running.\n",
    "sparse_autoencoder_dictionary = language_model_sae_runner(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 - Small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 24576-L1-0.008-LR-0.0004-Tokens-2.000e+08\n",
      "n_tokens_per_buffer (millions): 1.048576\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 48828\n",
      "Total wandb updates: 488\n",
      "n_tokens_per_feature_sampling_window (millions): 2621.44\n",
      "n_tokens_per_dead_feature_window (millions): 5242.88\n",
      "We will reset the sparsity calculation 19 times.\n",
      "Number tokens in sparsity calculation window: 1.02e+07\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee8922d83f04003a2f1441eeb30200d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28394fed8604083b39e23ea2add98a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 24576-L1-0.008-LR-0.0004-Tokens-2.000e+08\n",
      "n_tokens_per_buffer (millions): 1.048576\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 48828\n",
      "Total wandb updates: 488\n",
      "n_tokens_per_feature_sampling_window (millions): 2621.44\n",
      "n_tokens_per_dead_feature_window (millions): 5242.88\n",
      "We will reset the sparsity calculation 19 times.\n",
      "Number tokens in sparsity calculation window: 1.02e+07\n",
      "Run name: 24576-L1-0.008-LR-0.0004-Tokens-2.000e+08\n",
      "n_tokens_per_buffer (millions): 1.048576\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 48828\n",
      "Total wandb updates: 488\n",
      "n_tokens_per_feature_sampling_window (millions): 2621.44\n",
      "n_tokens_per_dead_feature_window (millions): 5242.88\n",
      "We will reset the sparsity calculation 19 times.\n",
      "Number tokens in sparsity calculation window: 1.02e+07\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pq5q3x9s) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea686292dff7449a9846fcfa29d6ff74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.064 MB of 0.064 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▁▂▂▃▄▄▅▅▆▆▇▇███████████████████████████</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▆▇▇▇█████████</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▄▂▁▄▄▂▄▄▁▄▁▄▂█</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>█▄▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▂▇▄█▅▃▄▇▆▅▄▆▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▁▃▄▆▆▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▄█▇▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l2_norm</td><td>▁▅▆▇▇▇█▇██████</td></tr><tr><td>metrics/l2_ratio</td><td>▁▅▆▇▇▇▇▇██████</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▃▂▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▅██</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▄█</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▄▅▆█</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0004</td></tr><tr><td>details/n_training_tokens</td><td>59801600</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>160.66861</td></tr><tr><td>losses/mse_loss</td><td>1.68098</td></tr><tr><td>losses/overall_loss</td><td>2.96633</td></tr><tr><td>metrics/CE_loss_score</td><td>0.96258</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>11.49633</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>3.62324</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>3.3166</td></tr><tr><td>metrics/explained_variance</td><td>0.78709</td></tr><tr><td>metrics/explained_variance_std</td><td>0.05978</td></tr><tr><td>metrics/l0</td><td>50.03076</td></tr><tr><td>metrics/l2_norm</td><td>102.32782</td></tr><tr><td>metrics/l2_ratio</td><td>0.8864</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-5.31744</td></tr><tr><td>sparsity/below_1e-5</td><td>19194</td></tr><tr><td>sparsity/below_1e-6</td><td>11736</td></tr><tr><td>sparsity/dead_features</td><td>60</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>640.44727</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">24576-L1-0.008-LR-0.0004-Tokens-2.000e+08</strong> at: <a href='https://wandb.ai/jbloom/gpt2_small_experiments_april/runs/pq5q3x9s' target=\"_blank\">https://wandb.ai/jbloom/gpt2_small_experiments_april/runs/pq5q3x9s</a><br/> View project at: <a href='https://wandb.ai/jbloom/gpt2_small_experiments_april' target=\"_blank\">https://wandb.ai/jbloom/gpt2_small_experiments_april</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_155117-pq5q3x9s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pq5q3x9s). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd02fd0295cc4afda9bb0e1367c87f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112805799995032, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/mats_sae_training/scripts/wandb/run-20240416_165827-vbwoyzi8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/gpt2_small_experiments_april/runs/vbwoyzi8' target=\"_blank\">24576-L1-0.008-LR-0.0004-Tokens-2.000e+08</a></strong> to <a href='https://wandb.ai/jbloom/gpt2_small_experiments_april' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/gpt2_small_experiments_april' target=\"_blank\">https://wandb.ai/jbloom/gpt2_small_experiments_april</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/gpt2_small_experiments_april/runs/vbwoyzi8' target=\"_blank\">https://wandb.ai/jbloom/gpt2_small_experiments_april/runs/vbwoyzi8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective value: 46608928.0000:   2%|▏         | 2/100 [00:00<00:01, 55.75it/s]\n",
      "/home/paperspace/mats_sae_training/sae_lens/training/sparse_autoencoder.py:176: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n",
      "120| MSE Loss 31.151 | L1 65.750:   0%|          | 487424/300000000 [00:15<1:28:10, 56617.16it/s]/home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (4elmsny3) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "2407| MSE Loss 0.070 | L1 0.027:  20%|█▉        | 9859072/50000000 [3:33:05<14:27:36, 771.10it/s]\n",
      "73243| MSE Loss 1.416 | L1 1.255: : 300003328it [2:44:02, 54947.70it/s]                               "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6111ba99afb144ae82bab7723efb2c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='721.959 MB of 721.959 MB uploaded (0.005 MB deduped)\\r'), FloatProgress(value=1.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▄▆█████████████████████████████████████</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▅▃▄▃▅▄▄▃▄▄▂▂▂▃▄▃▂▆▃▆▃▆▃▆▅▁▁▅▂▃▃▄▃▅▄▃█▄▃▆</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>█▄▂▂▂▂▂▁▂▁▁▂▂▁▁▁▂▁▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▁▅▆▂▆▄▅▁▄▄▄▇▆▄▃▄▇▄▂▆▆█▄▅▅▄▄▄▅▄▅▅▃▅▄▅▂▃▂▄</td></tr><tr><td>metrics/explained_variance</td><td>▁▆▇▇▇▇▇█████████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▄▃▂▂▂▂▁▂▁▂▁▁▂▂▁▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l2_norm</td><td>▁▄▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▇▆▆▇▆▆▆▇▆█████████████</td></tr><tr><td>metrics/l2_ratio</td><td>▁▄▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█████████████</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▅▄▄▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▅██████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▃▅▆▇██████████████████████</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▃▄▅▆▆▇▇▇▇███████████████████</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0004</td></tr><tr><td>details/n_training_tokens</td><td>299827200</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>162.07342</td></tr><tr><td>losses/mse_loss</td><td>1.42934</td></tr><tr><td>losses/overall_loss</td><td>2.72593</td></tr><tr><td>metrics/CE_loss_score</td><td>0.97257</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>11.42603</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>3.61949</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>3.39944</td></tr><tr><td>metrics/explained_variance</td><td>0.82112</td></tr><tr><td>metrics/explained_variance_std</td><td>0.0526</td></tr><tr><td>metrics/l0</td><td>50.53198</td></tr><tr><td>metrics/l2_norm</td><td>108.35806</td></tr><tr><td>metrics/l2_ratio</td><td>0.94604</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-7.89094</td></tr><tr><td>sparsity/below_1e-5</td><td>18079</td></tr><tr><td>sparsity/below_1e-6</td><td>18075</td></tr><tr><td>sparsity/dead_features</td><td>16912</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>27024.85938</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">24576-L1-0.008-LR-0.0004-Tokens-2.000e+08</strong> at: <a href='https://wandb.ai/jbloom/gpt2_small_experiments_april/runs/vbwoyzi8' target=\"_blank\">https://wandb.ai/jbloom/gpt2_small_experiments_april/runs/vbwoyzi8</a><br/> View project at: <a href='https://wandb.ai/jbloom/gpt2_small_experiments_april' target=\"_blank\">https://wandb.ai/jbloom/gpt2_small_experiments_april</a><br/>Synced 7 W&B file(s), 0 media file(s), 15 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_165827-vbwoyzi8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73243| MSE Loss 1.416 | L1 1.255: : 300003328it [2:44:12, 54947.70it/s]/home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (vbwoyzi8) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    }
   ],
   "source": [
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"gpt2-small\",\n",
    "    hook_point=\"blocks.8.hook_resid_pre\",\n",
    "    hook_point_layer=8,\n",
    "    d_in=768,\n",
    "    dataset_path=\"apollo-research/Skylion007-openwebtext-tokenizer-gpt2\",\n",
    "    is_dataset_tokenized=True,\n",
    "    prepend_bos=True,  # should experiment with turning this off.\n",
    "    # SAE Parameters\n",
    "    expansion_factor=32,  # determines the dimension of the SAE.\n",
    "    b_dec_init_method=\"geometric_median\",  # geometric median is better but slower to get started\n",
    "    apply_b_dec_to_input=False,\n",
    "    # Training Parameters\n",
    "    adam_beta1=0,\n",
    "    adam_beta2=0.999,\n",
    "    lr=0.0004,\n",
    "    l1_coefficient=0.008,\n",
    "    lr_scheduler_name=\"constant\",\n",
    "    train_batch_size=4096,\n",
    "    context_size=256,\n",
    "    lr_warm_up_steps=5000,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=128,\n",
    "    training_tokens=1_000_000 * 200,  # 200M tokens seems doable overnight.\n",
    "    finetuning_method=\"decoder\",\n",
    "    finetuning_tokens=1_000_000 * 100,\n",
    "    store_batch_size=32,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=False,\n",
    "    feature_sampling_window=2500,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold=1e-8,\n",
    "    # WANDB\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"gpt2_small_experiments_april\",\n",
    "    wandb_entity=None,\n",
    "    wandb_log_frequency=100,\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=5,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
