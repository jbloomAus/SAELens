{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bb9606783a40e98facc3cdfd8532da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b608839ee14d61b0ba2f0774374ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2628ed41347942d8833b3fc4aedb43ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27aa11e48d04a44b1a0be9c29c5e492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d57424f4264c8bbf9701eb98bcd0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e4ab302f9b4936a236486f1562159e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e447fad07cf946fc89fb04f9d843fe12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18d284d4bf64cd4bf924352a1e76ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5713a667fb5a4ea9a8f30d2923f5de7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6e94ffd7944496b54d94555ab08a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9474c0d68e64317b8634746bde04dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c607a350a347989c6f4df72f022eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275cce94b936411dadb71903eac295f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model = HookedTransformer.from_pretrained_no_processing(\"meta-llama/Meta-Llama-3-8B\", device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945f5ea3fa0c4318968a219c2bda0cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layers.3/cfg.json:   0%|          | 0.00/92.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046b7a4a9dd7413c859d1bd8de238569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sae.safetensors:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_lens import SAE\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience. \n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"sae-llama-3-8b-eai\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = \"blocks.3.hook_resid_post\", # won't always be a hook point\n",
    "    device = \"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.cfg.context_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(\"I like to eat apples and bananas\", names_filter = [sae.cfg.hook_name])\n",
    "feature_acts = sae.encode(cache[sae.cfg.hook_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4117efefbd8044b79f63a6c6b732d030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7680a49bdb2f4a4ab4fe1082d7ebfaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/3.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import ActivationsStore\n",
    "\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    # fairly conservative parameters here so can use same for larger\n",
    "    # models without running out of memory.\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=4,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metrics/l2_norm': 2.7873101234436035, 'metrics/l2_ratio': 0.893515408039093, 'metrics/l2_norm_in': 5.343474388122559, 'metrics/CE_loss_score': -0.0256333872966934, 'metrics/ce_loss_without_sae': 2.8757602870464325, 'metrics/ce_loss_with_sae': 11.989570021629333, 'metrics/ce_loss_with_ablation': 11.761783599853516}\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import run_evals\n",
    "\n",
    "metrics = run_evals(sae, activation_store, model, n_eval_batches=8, eval_batch_size_prompts=8)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-8fbe40e2-78bc\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-8fbe40e2-78bc\",\n",
       "      TokenLogProbs,\n",
       "      {\"prompt\": [\"<|begin_of_text|>\", \"1\", \" \", \"2\", \" \", \"3\", \" \", \"4\", \" \", \"5\", \" \", \"6\", \" \", \"7\", \" \", \"8\", \" \", \"9\", \" \", \"10\", \" \", \"11\", \" \", \"12\"], \"topKLogProbs\": [[-1.1855189800262451, -2.2635552883148193, -2.7761037349700928, -3.571357488632202, -4.160661697387695, -4.277589797973633, -4.573404312133789, -4.851010322570801, -4.889860153198242, -4.998664855957031], [-0.7220155596733093, -3.2538816928863525, -3.387751817703247, -3.6722776889801025, -3.9429256916046143, -4.074188232421875, -4.2364654541015625, -4.373985290527344, -4.387679100036621, -4.580226898193359], [-2.1286439895629883, -2.2889251708984375, -2.3705062866210938, -2.9385128021240234, -3.519704818725586, -4.0599822998046875, -4.313755989074707, -4.330897331237793, -4.376910209655762, -4.424673080444336], [-0.20045900344848633, -4.686463832855225, -4.7907938957214355, -4.989243030548096, -5.1022467613220215, -5.321669101715088, -5.657449245452881, -5.680965900421143, -5.8464436531066895, -5.8505473136901855], [-0.028724193572998047, -5.268666744232178, -5.58502721786499, -5.631362438201904, -6.415660381317139, -6.75131368637085, -6.837277889251709, -7.74310827255249, -7.744761943817139, -7.7575554847717285], [-0.12368418276309967, -4.7342000007629395, -5.0104851722717285, -5.218432903289795, -5.224313259124756, -6.0112128257751465, -6.325361728668213, -6.409498691558838, -6.447262287139893, -6.545773983001709], [-0.016338258981704712, -5.607444763183594, -6.107883453369141, -6.198463439941406, -6.907678604125977, -7.065422058105469, -7.101154327392578, -7.663351058959961, -8.219338417053223, -8.618905067443848], [-0.06442601978778839, -4.620747089385986, -4.88545560836792, -5.3189921379089355, -5.633164882659912, -6.410407543182373, -7.1825995445251465, -7.2039408683776855, -7.208119869232178, -7.227827548980713], [-0.008644777350127697, -5.863903522491455, -6.893373966217041, -6.908030033111572, -7.263914585113525, -8.049906730651855, -8.363103866577148, -8.67514705657959, -8.943359375, -9.228532791137695], [-0.0956132635474205, -4.499499320983887, -4.763647079467773, -5.289255142211914, -5.566125869750977, -5.56797981262207, -5.8718109130859375, -6.167787551879883, -6.301527976989746, -6.399867057800293], [-0.016639266163110733, -5.26703405380249, -5.677880764007568, -7.013725757598877, -7.183773517608643, -7.854629993438721, -7.970543384552002, -8.209405899047852, -8.276863098144531, -8.320544242858887], [-0.0673811286687851, -4.578975677490234, -4.779263496398926, -5.628861427307129, -6.12813663482666, -6.251727104187012, -6.614238739013672, -6.654447555541992, -6.953889846801758, -7.080574035644531], [-0.011309798806905746, -5.409699440002441, -6.767141342163086, -7.248251914978027, -7.385335922241211, -7.742281913757324, -7.743090629577637, -8.651128768920898, -8.653759956359863, -8.769566535949707], [-0.044062815606594086, -4.9239301681518555, -5.819080352783203, -5.999011039733887, -6.317224502563477, -6.346514701843262, -6.7918701171875, -6.937108993530273, -7.245876312255859, -7.261510848999023], [-0.008332961238920689, -6.49343729019165, -6.749897480010986, -7.078700542449951, -7.712348461151123, -7.774880886077881, -8.216479301452637, -8.609100341796875, -8.843048095703125, -9.057126998901367], [-0.04599068686366081, -5.058365345001221, -5.89590311050415, -6.100708484649658, -6.418325901031494, -6.446488857269287, -6.883681774139404, -7.049584865570068, -7.171384334564209, -7.2520060539245605], [-0.003986389376223087, -7.41904354095459, -7.5069684982299805, -7.608067512512207, -8.220268249511719, -8.758659362792969, -8.825607299804688, -9.260278701782227, -9.389827728271484, -9.457817077636719], [-0.14301662147045135, -3.7458484172821045, -4.674102783203125, -4.886180877685547, -5.06007194519043, -5.431365013122559, -5.627002716064453, -5.6862382888793945, -5.804477691650391, -6.062702178955078], [-0.021902693435549736, -4.764576435089111, -6.1357035636901855, -7.044876575469971, -7.113098621368408, -7.151342868804932, -7.283039569854736, -7.576583385467529, -8.187294006347656, -8.24666690826416], [-0.14038409292697906, -3.9058032035827637, -3.922635555267334, -4.585139751434326, -4.800888538360596, -5.1977057456970215, -5.214705944061279, -5.679104328155518, -5.811737537384033, -6.0074143409729], [-0.01634330302476883, -4.810545921325684, -6.4947614669799805, -7.520255088806152, -7.6398115158081055, -7.976287841796875, -8.001672744750977, -8.009892463684082, -8.607202529907227, -8.860658645629883], [-0.04640684276819229, -4.874860763549805, -5.614428520202637, -5.694490432739258, -5.791067123413086, -5.9103851318359375, -6.039061546325684, -6.433710098266602, -6.530948638916016, -7.189517021179199], [-0.005394426174461842, -5.989019870758057, -7.87720251083374, -8.078045845031738, -8.297078132629395, -8.880559921264648, -9.050943374633789, -9.191675186157227, -9.262900352478027, -9.36365795135498]], \"topKTokens\": [[\"Question\", \"def\", \"#\", \"The\", \"import\", \"Tags\", \"A\", \"package\", \"Home\", \"This\"], [\".\", \" \", \")\", \"st\", \" What\", \",\", \" How\", \"/\", \"-\", \":\"], [\"2\", \"201\", \"1\", \"200\", \"3\", \"4\", \" The\", \"5\", \"202\", \" \"], [\" \", \":\", \".\", \" Previous\", \" ...\", \" .\", \" Next\", \"-\", \"/\", \" A\"], [\"3\", \"2\", \"1\", \"4\", \" \", \"5\", \"6\", \"10\", \"7\", \"0\"], [\" \", \" ...\", \" Next\", \" Previous\", \" OT\", \" \\u2026\", \" [\", \" \\u00a0\", \" $\", \" The\"], [\"4\", \"5\", \"1\", \" \", \"6\", \"3\", \"2\", \"7\", \"8\", \"9\"], [\" \", \" OT\", \" Next\", \" Previous\", \" ...\", \" \\u2026\", \" x\", \" \\u00a0\", \" O\", \" X\"], [\"5\", \"6\", \" \", \"1\", \" T\", \"7\", \"567\", \"2\", \"56\", \"0\"], [\" \", \" ...\", \" Next\", \" X\", \" Click\", \" \\u2026\", \" ..\", \" R\", \" Previous\", \"\\n\"], [\"6\", \"7\", \"1\", \" \", \"5\", \"8\", \"2\", \"99\", \"0\", \"10\"], [\" \", \" Next\", \" X\", \" ...\", \" \\u2026\", \" R\", \" A\", \" OT\", \" O\", \"\\n\"], [\"7\", \"8\", \" \", \"1\", \"6\", \"9\", \"10\", \"0\", \"11\", \"\\u00a0\"], [\" \", \" Next\", \" X\", \" R\", \" A\", \" ...\", \"\\n\", \" >\", \" ```\\n\", \" Previous\"], [\"8\", \" \", \"9\", \"1\", \" R\", \"7\", \"0\", \"10\", \"08\", \"6\"], [\" \", \" Next\", \" X\", \" ...\", \" A\", \" R\", \"\\n\", \" [\", \" >\", \" T\"], [\"9\", \" \", \"10\", \"1\", \"09\", \"90\", \"8\", \"0\", \"\\u00a0\", \"91\"], [\" \", \" A\", \" Next\", \"\\n\", \" L\", \" X\", \" ...\", \" R\", \" >\", \" T\"], [\"10\", \"0\", \"1\", \" \", \" A\", \" X\", \"11\", \" R\", \"101\", \"010\"], [\" \", \"\\n\", \" Next\", \" A\", \" X\", \" >\", \" ...\", \" |\", \" ```\\n\", \" >\\n\"], [\"11\", \" R\", \"1\", \"10\", \"12\", \"111\", \" \", \" A\", \"\\u00a0\", \" Next\"], [\" \", \" Next\", \"\\n\", \"-\", \" ```\\n\", \" ...\", \" [\", \" A\", \" X\", \" >\"], [\"12\", \"1\", \"11\", \"13\", \" R\", \"2\", \" \", \"0\", \"\\u00a0\", \"4\"]], \"correctTokenRank\": [38, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"correctTokenLogProb\": [-6.186188697814941, -3.2538816928863525, -2.1286439895629883, -0.20045900344848633, -0.028724193572998047, -0.12368418276309967, -0.016338258981704712, -0.06442601978778839, -0.008644777350127697, -0.0956132635474205, -0.016639266163110733, -0.0673811286687851, -0.011309798806905746, -0.044062815606594086, -0.008332961238920689, -0.04599068686366081, -0.003986389376223087, -0.14301662147045135, -0.021902693435549736, -0.14038409292697906, -0.01634330302476883, -0.04640684276819229, -0.005394426174461842]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x3a0962850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import circuitsvis as cv \n",
    "prompt = \"1 2 3 4 5 6 7 8 9 10 11 12\"\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "display(cv.logits.token_log_probs(model.to_tokens(prompt), model(prompt)[0].log_softmax(dim=-1), model.to_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fa2200dece0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "torch.set_grad_enabled(False)  # Disable gradient calculation for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c52da988394e599d1ce5f386ee8575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HuggingFace Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0217a21a9c849a094ad20de3ce98ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hf_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[128000,  10267,    596,   1456,   2555,   5115,   2204,     13,    578,\n",
      "           3575,    584,    617,    374,    430]], device='cuda:0')\n",
      "Output: torch.Size([1, 14, 128256])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor(tokenizer.encode(\"Let's try something quite different. The problem we have is that\")).unsqueeze(0).to(device)\n",
    "print(f\"Input: {input}\")\n",
    "\n",
    "layers=[1, 4, 7] # offset by one so we get the output of 0, 3, and 6\n",
    "all_hidden_states = [[] for _ in layers]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = hf_model(\n",
    "        input,\n",
    "        output_hidden_states=True\n",
    "        )\n",
    "\n",
    "    hidden_list = outputs.hidden_states[:-1]  # Exclude the final layer\n",
    "\n",
    "    # Select only the specified layers\n",
    "    hidden_list = [hidden_list[i] for i in layers]\n",
    "\n",
    "# Store the hidden states\n",
    "for i, hidden in enumerate(hidden_list):\n",
    "    all_hidden_states[i].append(hidden.cpu())\n",
    "\n",
    "# Concatenate all batches for each layer\n",
    "all_hidden_states = [torch.cat(states, dim=0) for states in all_hidden_states]\n",
    "\n",
    "print(f\"Output: {outputs.logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TransformerLens Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015be8e522be49ad848ba98814b7e970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(model_name, device=\"cuda\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 4096])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, cache = model.run_with_cache(input, prepend_bos=False)\n",
    "cache['blocks.3.hook_resid_post'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation match: False\n",
      "\n",
      "\n",
      "HuggingFaceExtractor:\n",
      "tensor([ 0.0596,  0.0595, -0.0433, -0.0446, -0.1433,  0.0310,  0.0668, -0.0174,\n",
      "        -0.0274,  0.0760])\n",
      "HookedTransformer:\n",
      "tensor([ 0.0504,  0.0605, -0.0522, -0.0435, -0.1374,  0.0271,  0.0637, -0.0272,\n",
      "        -0.0220,  0.0875], device='cuda:0')\n",
      "\n",
      "Mean value of HF activations: -0.004082582890987396\n",
      "Mean value of TL activations: -0.004054217599332333\n"
     ]
    }
   ],
   "source": [
    "assert cache['blocks.3.hook_resid_post'].shape == all_hidden_states[1].shape\n",
    "activation_match = torch.allclose(cache['blocks.6.hook_resid_post'].cpu(), all_hidden_states[2], atol=1e-6)\n",
    "# print samples of each\n",
    "print(f\"Activation match: {activation_match}\\n\")\n",
    "\n",
    "print(\"\\nHuggingFaceExtractor:\")\n",
    "print(all_hidden_states[2][0, 5, :10])\n",
    "\n",
    "print(\"HookedTransformer:\")\n",
    "print(cache['blocks.6.hook_resid_post'][0, 5, :10])\n",
    "\n",
    "print(f\"\\nMean value of HF activations: {all_hidden_states[2].mean()}\")\n",
    "print(f\"Mean value of TL activations: {cache['blocks.6.hook_resid_post'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden states match: False\n",
      "Number of different elements: 22718 out of 57344\n",
      "Maximum absolute difference: 0.13030779361724854\n",
      "Mean absolute difference: 0.010224463418126106\n",
      "Median absolute difference: 0.00719045102596283\n",
      "\n",
      "Top 10 largest differences:\n",
      "At index (0, 6, 4055):\n",
      "  HookedTransformer value: 0.5900180339813232\n",
      "  HiddenStateExtractor value: 0.7203258275985718\n",
      "  Absolute difference: 0.13030779361724854\n",
      "  Relative difference: 0.1809011846780777\n",
      "\n",
      "At index (0, 8, 4055):\n",
      "  HookedTransformer value: 1.271518588066101\n",
      "  HiddenStateExtractor value: 1.1430875062942505\n",
      "  Absolute difference: 0.12843108177185059\n",
      "  Relative difference: 0.10100606083869934\n",
      "\n",
      "At index (0, 13, 103):\n",
      "  HookedTransformer value: 0.285245805978775\n",
      "  HiddenStateExtractor value: 0.18846748769283295\n",
      "  Absolute difference: 0.09677831828594208\n",
      "  Relative difference: 0.3392804265022278\n",
      "\n",
      "At index (0, 7, 1800):\n",
      "  HookedTransformer value: 0.020996078848838806\n",
      "  HiddenStateExtractor value: -0.0670798271894455\n",
      "  Absolute difference: 0.0880759060382843\n",
      "  Relative difference: 1.3130013942718506\n",
      "\n",
      "At index (0, 8, 3516):\n",
      "  HookedTransformer value: -0.09787607192993164\n",
      "  HiddenStateExtractor value: -0.18285872042179108\n",
      "  Absolute difference: 0.08498264849185944\n",
      "  Relative difference: 0.46474483609199524\n",
      "\n",
      "At index (0, 12, 3488):\n",
      "  HookedTransformer value: 0.21808791160583496\n",
      "  HiddenStateExtractor value: 0.13379399478435516\n",
      "  Absolute difference: 0.0842939168214798\n",
      "  Relative difference: 0.38651347160339355\n",
      "\n",
      "At index (0, 12, 873):\n",
      "  HookedTransformer value: -0.11184568703174591\n",
      "  HiddenStateExtractor value: -0.027747904881834984\n",
      "  Absolute difference: 0.08409778028726578\n",
      "  Relative difference: 0.7519090175628662\n",
      "\n",
      "At index (0, 9, 3516):\n",
      "  HookedTransformer value: 0.25340989232063293\n",
      "  HiddenStateExtractor value: 0.17037102580070496\n",
      "  Absolute difference: 0.08303886651992798\n",
      "  Relative difference: 0.32768598198890686\n",
      "\n",
      "At index (0, 5, 4055):\n",
      "  HookedTransformer value: 0.6758919358253479\n",
      "  HiddenStateExtractor value: 0.593126654624939\n",
      "  Absolute difference: 0.08276528120040894\n",
      "  Relative difference: 0.12245342135429382\n",
      "\n",
      "At index (0, 13, 4055):\n",
      "  HookedTransformer value: 0.3931175470352173\n",
      "  HiddenStateExtractor value: 0.3105863332748413\n",
      "  Absolute difference: 0.08253121376037598\n",
      "  Relative difference: 0.20994029939174652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compare_hidden_states(state1, state2, name1=\"HookedTransformer\", name2=\"HiddenStateExtractor\", rtol=1e-5, atol=1e-2):\n",
    "    \"\"\"\n",
    "    Compare two hidden states and return detailed information about their differences.\n",
    "    \n",
    "    Args:\n",
    "    state1, state2: The hidden states to compare (torch.Tensor)\n",
    "    name1, name2: Names of the sources of the hidden states (str)\n",
    "    rtol: Relative tolerance for comparison\n",
    "    atol: Absolute tolerance for comparison\n",
    "    \n",
    "    Returns:\n",
    "    A dictionary containing information about the comparison\n",
    "    \"\"\"\n",
    "    if state1.shape != state2.shape:\n",
    "        return {\n",
    "            \"match\": False,\n",
    "            \"reason\": \"Shapes do not match\",\n",
    "            f\"{name1}_shape\": state1.shape,\n",
    "            f\"{name2}_shape\": state2.shape\n",
    "        }\n",
    "    \n",
    "    # Convert to numpy for easier handling\n",
    "    array1 = state1.detach().cpu().numpy()\n",
    "    array2 = state2.detach().cpu().numpy()\n",
    "    \n",
    "    # Check if the arrays are close\n",
    "    is_close = np.isclose(array1, array2, rtol=rtol, atol=atol)\n",
    "    match = np.all(is_close)\n",
    "    \n",
    "    if match:\n",
    "        return {\"match\": True}\n",
    "    \n",
    "    # Find where values are not close\n",
    "    not_close = ~is_close\n",
    "    diff_indices = np.where(not_close)\n",
    "    \n",
    "    # Prepare a list of differences\n",
    "    differences = []\n",
    "    for index in zip(*diff_indices):\n",
    "        value1 = array1[index]\n",
    "        value2 = array2[index]\n",
    "        abs_diff = abs(value1 - value2)\n",
    "        rel_diff = abs_diff / max(abs(value1), abs(value2))\n",
    "        differences.append({\n",
    "            \"index\": index,\n",
    "            \"value1\": value1,\n",
    "            \"value2\": value2,\n",
    "            \"absolute_difference\": abs_diff,\n",
    "            \"relative_difference\": rel_diff\n",
    "        })\n",
    "    \n",
    "    # Sort differences by absolute difference\n",
    "    differences.sort(key=lambda x: x[\"absolute_difference\"], reverse=True)\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    abs_diffs = np.abs(array1 - array2)\n",
    "    \n",
    "    return {\n",
    "        \"match\": False,\n",
    "        \"num_different_elements\": np.sum(not_close),\n",
    "        \"total_elements\": array1.size,\n",
    "        \"max_absolute_difference\": np.max(abs_diffs),\n",
    "        \"mean_absolute_difference\": np.mean(abs_diffs),\n",
    "        \"median_absolute_difference\": np.median(abs_diffs),\n",
    "        \"top_10_differences\": differences[:10]\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Assuming you have your hidden states in these variables\n",
    "    hooked_transformer_state = cache['blocks.6.hook_resid_post'].cpu()\n",
    "    hidden_state_extractor_state = all_hidden_states[2]\n",
    "\n",
    "    comparison_result = compare_hidden_states(hooked_transformer_state, hidden_state_extractor_state)\n",
    "\n",
    "    print(f\"Hidden states match: {comparison_result['match']}\")\n",
    "    if not comparison_result['match']:\n",
    "        print(f\"Number of different elements: {comparison_result['num_different_elements']} out of {comparison_result['total_elements']}\")\n",
    "        print(f\"Maximum absolute difference: {comparison_result['max_absolute_difference']}\")\n",
    "        print(f\"Mean absolute difference: {comparison_result['mean_absolute_difference']}\")\n",
    "        print(f\"Median absolute difference: {comparison_result['median_absolute_difference']}\")\n",
    "        print(\"\\nTop 10 largest differences:\")\n",
    "        for diff in comparison_result['top_10_differences']:\n",
    "            print(f\"At index {diff['index']}:\")\n",
    "            print(f\"  HookedTransformer value: {diff['value1']}\")\n",
    "            print(f\"  HiddenStateExtractor value: {diff['value2']}\")\n",
    "            print(f\"  Absolute difference: {diff['absolute_difference']}\")\n",
    "            print(f\"  Relative difference: {diff['relative_difference']}\")\n",
    "            print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71d708ffdbd40f08e876a4739643fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, in a galaxy far, far away, there was a little boy who loved to play with his toys. One day, he decided to play a game with his toys. He would hide them in different places around the house, and then he would go on a treasure hunt to find them'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"Once upon a time, in a galaxy far, far away\", max_new_tokens=50, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Once upon a time, in a galaxy far, far away, there was a little girl who loved to read. She loved to read so much that she would read anything she could get her hands on. She would read books, magazines, newspapers, cereal boxes, and even the back of the shampoo bottle. She would read so much that she would often forget to eat or sleep. Her parents were worried about her, but they knew that reading was her passion, and they supported her in her\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate text using a specified Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "    model_name (str): The name of the Hugging Face model to use.\n",
    "    prompt (str): The initial text to start the generation.\n",
    "    max_length (int): The maximum length of the generated text (including the prompt).\n",
    "    temperature (float): Controls randomness in generation. Higher values increase randomness.\n",
    "    top_p (float): Controls diversity of generation. Lower values make output more focused.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    #model_name = \"gpt2\"  # You can change this to any other Hugging Face model\n",
    "    prompt = \"Once upon a time, in a galaxy far, far away\"\n",
    "\n",
    "    generated_text = generate_text(hf_model, tokenizer, prompt)\n",
    "    print(f\"Generated text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
