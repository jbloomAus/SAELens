{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import os\n",
    "from typing import Optional, List, Dict, Callable, Tuple, Union\n",
    "import tqdm.notebook as tqdm\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import importlib\n",
    "import json\n",
    "\n",
    "from sae_analysis.visualizer import data_fns, model_fns, html_fns\n",
    "# from model_fns import AutoEncoderConfig, AutoEncoder\n",
    "from sae_analysis.visualizer.data_fns import get_feature_data, FeatureData\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def imshow(x, **kwargs):\n",
    "    x_numpy = utils.to_numpy(x)\n",
    "    px.imshow(x_numpy, **kwargs).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SAE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch\n",
    "import torch as t\n",
    "import einops\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from jaxtyping import Float\n",
    "from transformer_lens import ActivationCache\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import pprint\n",
    "import json \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "SAVE_DIR = Path(\"/workspace/1L-Sparse-Autoencoder/checkpoints\")\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        d_hidden = cfg[\"dict_size\"]\n",
    "        l1_coeff = cfg[\"l1_coeff\"]\n",
    "        dtype = DTYPES[cfg[\"enc_dtype\"]]\n",
    "        torch.manual_seed(cfg[\"seed\"])\n",
    "        self.W_enc = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(cfg[\"act_size\"], d_hidden, dtype=dtype)))\n",
    "        self.W_dec = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_hidden, cfg[\"act_size\"], dtype=dtype)))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(cfg[\"act_size\"], dtype=dtype))\n",
    "\n",
    "        self.W_dec.data[:] = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.l1_coeff = l1_coeff\n",
    "\n",
    "        self.to(cfg[\"device\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_cent = x - self.b_dec\n",
    "        acts = F.relu(x_cent @ self.W_enc + self.b_enc)\n",
    "        x_reconstruct = acts @ self.W_dec + self.b_dec\n",
    "        l2_loss = (x_reconstruct.float() - x.float()).pow(2).sum(-1).mean(0)\n",
    "        l1_loss = self.l1_coeff * (acts.float().abs().sum())\n",
    "        loss = l2_loss + l1_loss\n",
    "        return loss, x_reconstruct, acts, l2_loss, l1_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def make_decoder_weights_and_grad_unit_norm(self):\n",
    "        W_dec_normed = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "        W_dec_grad_proj = (self.W_dec.grad * W_dec_normed).sum(-1, keepdim=True) * W_dec_normed\n",
    "        self.W_dec.grad -= W_dec_grad_proj\n",
    "        # Bugfix(?) for ensuring W_dec retains unit norm, this was not there when I trained my original autoencoders.\n",
    "        self.W_dec.data = W_dec_normed\n",
    "    \n",
    "    def get_version(self):\n",
    "        version_list = [int(file.name.split(\".\")[0]) for file in list(SAVE_DIR.iterdir()) if \"pt\" in str(file)]\n",
    "        if len(version_list):\n",
    "            return 1+max(version_list)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def save(self):\n",
    "        version = self.get_version()\n",
    "        torch.save(self.state_dict(), SAVE_DIR/(str(version)+\".pt\"))\n",
    "        with open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"w\") as f:\n",
    "            json.dump(cfg, f)\n",
    "        print(\"Saved as version\", version)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, version):\n",
    "        cfg = (json.load(open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"r\")))\n",
    "        pprint.pprint(cfg)\n",
    "        self = cls(cfg=cfg)\n",
    "        self.load_state_dict(torch.load(SAVE_DIR/(str(version)+\".pt\")))\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_hf(cls, version, device_override=None):\n",
    "        \"\"\"\n",
    "        Loads the saved autoencoder from HuggingFace. \n",
    "        \n",
    "        Version is expected to be an int, or \"run1\" or \"run2\"\n",
    "\n",
    "        version 25 is the final checkpoint of the first autoencoder run,\n",
    "        version 47 is the final checkpoint of the second autoencoder run.\n",
    "        \"\"\"\n",
    "        if version==\"run1\":\n",
    "            version = 25\n",
    "        elif version==\"run2\":\n",
    "            version = 47\n",
    "        \n",
    "        cfg = utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}_cfg.json\")\n",
    "        if device_override is not None:\n",
    "            cfg[\"device\"] = device_override\n",
    "\n",
    "        pprint.pprint(cfg)\n",
    "        self = cls(cfg=cfg)\n",
    "        self.load_state_dict(utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}.pt\", force_is_torch=True))\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "# load gpt2-small\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\").to('mps')\n",
    "\n",
    "\n",
    "\n",
    "point, layer = \"resid_pre\", 10\n",
    "dic = utils.download_file_from_hf(\"jacobcd52/gpt2-small-sparse-autoencoders\", f\"gpt2-small_6144_{point}_{layer}.pt\", force_is_torch=True)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    \"dict_size\": 6144,\n",
    "    \"act_size\": 768,\n",
    "    \"l1_coeff\": 0.001,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"seed\": 0,\n",
    "    \"device\": \"mps\",\n",
    "    \"model_batch_size\": 1028,\n",
    "}\n",
    "encoder_resid_pre_10 = AutoEncoder(cfg)\n",
    "encoder_resid_pre_10.load_state_dict(dic)\n",
    "\n",
    "point, layer = \"resid_pre\", 11\n",
    "dic = utils.download_file_from_hf(\"jacobcd52/gpt2-small-sparse-autoencoders\", f\"gpt2-small_6144_{point}_{layer}.pt\", force_is_torch=True)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    \"dict_size\": 6144,\n",
    "    \"act_size\": 768,\n",
    "    \"l1_coeff\": 0.001,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"seed\": 0,\n",
    "    \"device\": \"mps\",\n",
    "    \"model_batch_size\": 1028,\n",
    "}\n",
    "encoder_resid_pre_11 = AutoEncoder(cfg)\n",
    "encoder_resid_pre_11.load_state_dict(dic)\n",
    "\n",
    "data = load_dataset(\"stas/openwebtext-10k\", split=\"train\")\n",
    "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(22)\n",
    "\n",
    "from transformer_lens import utils\n",
    "\n",
    "example_prompt = \"After Jack and Mary went to the store, Jack gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "vocab_dict = model.tokenizer.vocab\n",
    "vocab_dict = {v: k.replace(\"Ä \", \" \").replace(\"\\n\", \"\\\\n\") for k, v in vocab_dict.items()}\n",
    "\n",
    "vocab_dict_filepath = Path(os.getcwd()) / \"vocab_dict.json\"\n",
    "if not vocab_dict_filepath.exists():\n",
    "    with open(vocab_dict_filepath, \"w\") as f:\n",
    "        json.dump(vocab_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
    "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "all_tokens = tokenized_data[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.sparse_autoencoder import SparseAutoencoder\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SparseAutoencoderConfig:\n",
    "    d_sae: int\n",
    "    d_in: int\n",
    "    l1_coefficient: float\n",
    "    dtype: str\n",
    "    seed: int\n",
    "    device: str\n",
    "    model_batch_size: int\n",
    "    hook_point: str = \"blocks.10.hook_resid_pre\"\n",
    "    hook_point_layer: int = 10\n",
    "    \n",
    "cfg = {\n",
    "    \"d_sae\": 6144,\n",
    "    \"d_in\": 768,\n",
    "    \"l1_coefficient\": 0.001,\n",
    "    \"dtype\": torch.float32,\n",
    "    \"seed\": 0,\n",
    "    \"device\": \"mps\",\n",
    "    \"model_batch_size\": 1028,\n",
    "}\n",
    "\n",
    "\n",
    "sparse_autoencoder_cfg = SparseAutoencoderConfig(**cfg)\n",
    "sparse_autoencoder = SparseAutoencoder(sparse_autoencoder_cfg)\n",
    "\n",
    "\n",
    "point, layer = \"resid_pre\", 10\n",
    "dic = utils.download_file_from_hf(\"jacobcd52/gpt2-small-sparse-autoencoders\", f\"gpt2-small_6144_{point}_{layer}.pt\", force_is_torch=True)\n",
    "sparse_autoencoder.load_state_dict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_features = [\n",
    "    1735, 1096, 1228, 5528, 1095, 4406, 4408, 848, 175, 4287,\n",
    "    176, 1266, 5337, 1761, 2500, 2700, 4624, 1435, 4072, 872,\n",
    "    3243, 1760, 4752, 3349, 5527, 4795\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(data_fns)\n",
    "importlib.reload(html_fns)\n",
    "from sae_analysis.visualizer.data_fns import get_feature_data, FeatureData\n",
    "\n",
    "# Currently, don't think much more time can be squeezed out of it. Maybe the best saving would be to\n",
    "# make the entire sequence indexing parallelized, but that's possibly not worth it right now.\n",
    "\n",
    "max_batch_size = 512\n",
    "total_batch_size = 4096*6\n",
    "feature_idx = interesting_features\n",
    "# max_batch_size = 512\n",
    "# total_batch_size = 16384\n",
    "# feature_idx = list(range(1000))\n",
    "\n",
    "tokens = all_tokens[:total_batch_size]\n",
    "\n",
    "feature_data: Dict[int, FeatureData] = get_feature_data(\n",
    "    encoder=sparse_autoencoder,\n",
    "    # encoder_B=sparse_autoencoder,\n",
    "    model=model,\n",
    "    hook_point=sparse_autoencoder.cfg.hook_point,\n",
    "    hook_point_layer=sparse_autoencoder.cfg.hook_point_layer,\n",
    "    tokens=tokens,\n",
    "    feature_idx=feature_idx,\n",
    "    max_batch_size=max_batch_size,\n",
    "    left_hand_k = 3,\n",
    "    buffer = (5, 5),\n",
    "    n_groups = 10,\n",
    "    first_group_size = 20,\n",
    "    other_groups_size = 5,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "\n",
    "for test_idx in list(interesting_features):\n",
    "    html_str = feature_data[test_idx].get_all_html()\n",
    "    with open(f\"data_{test_idx:04}.html\", \"w\") as f:\n",
    "        f.write(html_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for test_idx in list(interesting_features):\n",
    "    html_str = feature_data[test_idx].get_all_html()\n",
    "    with open(f\"data_{test_idx:04}.html\", \"w\") as f:\n",
    "        f.write(html_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
