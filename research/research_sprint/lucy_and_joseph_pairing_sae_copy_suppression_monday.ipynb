{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating your SAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import wandb\n",
    "import json\n",
    "import plotly.express as px\n",
    "from transformer_lens import utils\n",
    "from datasets import load_dataset\n",
    "from typing import  Dict\n",
    "from pathlib import Path\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "from sae_analysis.visualizer import data_fns, html_fns\n",
    "from sae_analysis.visualizer.data_fns import get_feature_data, FeatureData\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\" \n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def imshow(x, **kwargs):\n",
    "    x_numpy = utils.to_numpy(x)\n",
    "    px.imshow(x_numpy, **kwargs).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load your Autoencoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.sparse_autoencoder import SparseAutoencoder\n",
    "# Load model from Huggingface\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('jbloom/mats_sae_training_gpt2_small/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_6144:v2', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "# Load in Model\n",
    "path = \"checkpoints/bu20al09/lilac_plant_final_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152.pt\"\n",
    "model, sparse_autoencoder_10M, activations_loader = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path\n",
    ")\n",
    "path = \"overnight_sae_resid_pre_10_gpt_2_small.pt\"\n",
    "sparse_autoencoder_200M = SparseAutoencoder.load_from_pretrained(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0 Test and Reconstruction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch_tokens = activations_loader.get_batch_tokens()\n",
    "    print(batch_tokens.shape)\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "    activations =  cache[sparse_autoencoder_10M.cfg.hook_point]\n",
    "    \n",
    "    sae_out_10M, feature_acts_10M, loss, mse_loss, l1_loss = sparse_autoencoder_10M(\n",
    "        cache[sparse_autoencoder_10M.cfg.hook_point]\n",
    "    )\n",
    "    # del cache\n",
    "    \n",
    "    l2_norms_of_input = torch.norm(activations[:,1:], dim=-1)\n",
    "    l2_norms_of_sae_out = torch.norm(sae_out_10M[:,1:], dim=-1)\n",
    "    print(\"l2_norms_of_input\", l2_norms_of_input.mean().item())\n",
    "    print(\"l2_norms_of_sae_out\", l2_norms_of_sae_out.mean().item())\n",
    "    \n",
    "    l0 = (feature_acts_10M > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()\n",
    "\n",
    "    sae_out_100M, feature_acts_100M, loss, mse_loss, l1_loss = sparse_autoencoder_200M(\n",
    "        cache[sparse_autoencoder_200M.cfg.hook_point]\n",
    "    )\n",
    "    # del cache\n",
    "    \n",
    "    l2_norms_of_input = torch.norm(activations[:,1:], dim=-1)\n",
    "    l2_norms_of_sae_out = torch.norm(sae_out_100M[:,1:], dim=-1)\n",
    "    print(\"l2_norms_of_input\", l2_norms_of_input.mean().item())\n",
    "    print(\"l2_norms_of_sae_out\", l2_norms_of_sae_out.mean().item())\n",
    "    \n",
    "    l0 = (feature_acts_100M > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monday Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reconstr_hook(mlp_out, hook, new_mlp_out):\n",
    "    return new_mlp_out\n",
    "\n",
    "def zero_abl_hook(mlp_out, hook):\n",
    "    return torch.zeros_like(mlp_out)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
    "    print(\n",
    "        \"reconstr\",\n",
    "        model.run_with_hooks(\n",
    "            batch_tokens,\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    utils.get_act_name(\"resid_pre\", 10),\n",
    "                    partial(reconstr_hook, new_mlp_out=sae_out_10M),\n",
    "                )\n",
    "            ],\n",
    "            return_type=\"loss\",\n",
    "        ).item(),\n",
    "    )\n",
    "    print(\n",
    "        \"reconstr\",\n",
    "        model.run_with_hooks(\n",
    "            batch_tokens,\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    utils.get_act_name(\"resid_pre\", 10),\n",
    "                    partial(reconstr_hook, new_mlp_out=sae_out_100M),\n",
    "                )\n",
    "            ],\n",
    "            return_type=\"loss\",\n",
    "        ).item(),\n",
    "    )\n",
    "    print(\n",
    "        \"Zero\",\n",
    "        model.run_with_hooks(\n",
    "            batch_tokens,\n",
    "            return_type=\"loss\",\n",
    "            fwd_hooks=[(utils.get_act_name(\"resid_pre\", 10), zero_abl_hook)],\n",
    "        ).item(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cache.apply_ln_to_stack(x_reconstruct[0],layer=10).mean()\n",
    "# example_batch = torch.randint(0,32,(1,)).item(); example_position = torch.randint(0, 10, (1,)).item()\n",
    "# print(example_batch, example_position)\n",
    "# print(model.to_str_tokens(batch_tokens[example_batch])[max(example_position-5,0):min(example_position+3,128)])\n",
    "# px.line(feature_acts[example_batch,example_position].cpu().numpy()).show()\n",
    "# lnd_activations = cache.apply_ln_to_stack(activations, layer=10)\n",
    "# _, feature_acts_after_ln, _, _, _ = sparse_autoencoder(\n",
    "#         lnd_activations\n",
    "#     )\n",
    "# px.line(feature_acts_after_ln[example_batch,example_position].cpu().numpy()).show()\n",
    "# vals, inds = torch.topk(feature_acts[example_batch,example_position].detach(), 10)\n",
    "# px.bar(\n",
    "#     x=utils.to_numpy(vals),\n",
    "#     y=[str(i.item()) for i in inds],\n",
    "#     orientation=\"h\",\n",
    "# ).show()\n",
    "# utils.test_prompt(\n",
    "#     prompt = model.to_string(batch_tokens[example_batch][1:example_position+1]),\n",
    "#     answer = model.to_string(batch_tokens[example_batch][example_position+1]),\n",
    "#     model = model)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def plot_feature_unembed_bar(feature_id, sparse_autoencoder, feature_name = \"\"):\n",
    "    \n",
    "    # norm_unembed = model.W_U / model.W_U.norm(dim=0)[None: None]\n",
    "    # feature_unembed = sparse_autoencoder.W_dec[feature_id] @ norm_unembed\n",
    "    feature_unembed = sparse_autoencoder.W_dec[feature_id] @  model.W_U\n",
    "    # torch.topk(unembed_4795,10)\n",
    "\n",
    "    feature_unembed_df = pd.DataFrame(\n",
    "        feature_unembed.detach().cpu().numpy(),\n",
    "        columns = [feature_name],\n",
    "        index = [model.tokenizer.decode(i) for i in list(range(50257))]\n",
    "    )\n",
    "\n",
    "    feature_unembed_df = feature_unembed_df.sort_values(feature_name, ascending=False).reset_index().rename(columns={'index': 'token'})\n",
    "    fig = px.bar(feature_unembed_df.head(20).sort_values(feature_name, ascending=True),\n",
    "                 color_continuous_midpoint=0,\n",
    "                 color_continuous_scale=\"RdBu\",\n",
    "            y = 'token', x = feature_name, orientation='h', color = feature_name, hover_data=[feature_name])\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=500,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    # fig.write_image(f\"figures/{str(feature_id)}_{feature_name}.png\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_feature_unembed_bar(14076, sparse_autoencoder_200M, feature_name = str(14076))\n",
    "# for i in inds:\n",
    "#     plot_feature_unembed_bar(int(i), sparse_autoencoder, feature_name = str(i.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Capability Test\n",
    "\n",
    "Validating model performance on specific tasks when using the reconstructed activation is quite important when studying specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"When Chris and David went to the play, David handed a club to\"\n",
    "example_answer = \" Chris\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
    "\n",
    "logits, cache = model.run_with_cache(example_prompt, prepend_bos=True)\n",
    "tokens = model.to_tokens(example_prompt)\n",
    "sae_out, feature_acts, loss, mse_loss, l1_loss = sparse_autoencoder_10M(\n",
    "    cache[sparse_autoencoder_10M.cfg.hook_point]\n",
    ")\n",
    "\n",
    "def reconstr_hook(mlp_out, hook, new_mlp_out):\n",
    "    return new_mlp_out\n",
    "\n",
    "def reconstr_key_hook(mlp_out, hook, reconstructed_key):\n",
    "    return reconstructed_key\n",
    "\n",
    "def reconstr_query_hook(mlp_out, hook, reconstructed_query):\n",
    "    return reconstructed_query\n",
    "\n",
    "\n",
    "def zero_abl_hook(mlp_out, hook):\n",
    "    return torch.zeros_like(mlp_out)\n",
    "\n",
    "print(\"Orig\", model(tokens, return_type=\"loss\").item())\n",
    "print(\n",
    "    \"reconstr\",\n",
    "    model.run_with_hooks(\n",
    "        tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                utils.get_act_name(\"resid_pre\", 10),\n",
    "                partial(reconstr_hook, new_mlp_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    ")\n",
    "print(\n",
    "    \"Zero\",\n",
    "    model.run_with_hooks(\n",
    "        tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(utils.get_act_name(\"resid_pre\", 10), zero_abl_hook)],\n",
    "    ).item(),\n",
    ")\n",
    "\n",
    "\n",
    "with model.hooks(\n",
    "    fwd_hooks=[\n",
    "        (\n",
    "            utils.get_act_name(\"resid_pre\", 10),\n",
    "            partial(reconstr_hook, new_mlp_out=sae_out),\n",
    "        )\n",
    "    ]\n",
    "):\n",
    "    utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((feature_acts[0,-1].detach() > 0 ).float().sum())\n",
    "px.line(feature_acts[0,-1].detach().cpu().numpy()).show()\n",
    "vals, inds = torch.topk(feature_acts[0,-1].detach().cpu(),10)\n",
    "px.bar(x=[str(i) for i in inds], y=vals).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ln_activation = cache.apply_ln_to_stack(cache[sparse_autoencoder_10M.cfg.hook_point], layer=10)\n",
    "_, feature_acts_post_ln_activations, _, _, _ = sparse_autoencoder_10M(\n",
    "    post_ln_activation\n",
    ")\n",
    "\n",
    "print((feature_acts_post_ln_activations[0,-1].detach() > 0 ).float().sum())\n",
    "px.line(feature_acts_post_ln_activations[0,-1].detach().cpu().numpy()).show()\n",
    "vals, inds = torch.topk(feature_acts_post_ln_activations[0,-1].detach().cpu(),10)\n",
    "px.bar(x=[str(i) for i in inds], y=vals).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"When John and Mary went to the shops, John gave the bag to\"\n",
    "example_answer = \" Mary\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
    "\n",
    "logits_original, cache_original = model.run_with_cache(example_prompt, prepend_bos=True)\n",
    "tokens = model.to_tokens(example_prompt)\n",
    "sae_out, feature_acts, loss, mse_loss, l1_loss = sparse_autoencoder_10M(\n",
    "    cache_original[sparse_autoencoder_10M.cfg.hook_point]\n",
    ")\n",
    "\n",
    "def reconstr_hook(mlp_out, hook, new_mlp_out):\n",
    "    print(mlp_out.shape, new_mlp_out.shape)\n",
    "    mlp_out[:,-1,:] = new_mlp_out[:,-1,:]\n",
    "    return mlp_out\n",
    "\n",
    "def mean_ablation_hook(mlp_out, hook, new_mlp_out):\n",
    "    print(mlp_out.shape, new_mlp_out.shape)\n",
    "    mlp_out[:,-1,:] = new_mlp_out[:,-1,:].mean(dim=1)\n",
    "    return mlp_out\n",
    "\n",
    "def reconstr_key_hook(mlp_out, hook, reconstructed_key):\n",
    "    return reconstructed_key\n",
    "\n",
    "def reconstr_query_hook(mlp_out, hook, reconstructed_query):\n",
    "    return reconstructed_query\n",
    "\n",
    "model.reset_hooks()\n",
    "with model.hooks(\n",
    "    fwd_hooks=[\n",
    "        (\n",
    "            utils.get_act_name(\"resid_pre\", 10),\n",
    "            partial(mean_ablation_hook, new_mlp_out=sae_out),\n",
    "        )\n",
    "    ]\n",
    "):\n",
    "    utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
    "    logits_reconstructed, cache_reconstructed_res_stream = model.run_with_cache(example_prompt, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.attention import attention_patterns\n",
    "patterns = cache_original[\"blocks.10.attn.hook_pattern\"][0]\n",
    "attention_patterns(tokens=model.to_str_tokens(example_prompt), attention=patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = cache_reconstructed_res_stream[\"blocks.10.attn.hook_pattern\"][0]\n",
    "attention_patterns(tokens=model.to_str_tokens(example_prompt), attention=patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chris_token = model.to_single_token(\" John\")\n",
    "david_token = model.to_single_token(\" Mary\")\n",
    "chris_david_dir = model.W_U[:,chris_token] - model.W_U[:,david_token]\n",
    "print(chris_david_dir.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_original[sparse_autoencoder_10M.cfg.hook_point][0, -1] @ chris_david_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_out[0, -1] @ chris_david_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_original[0, -1, chris_token] - logits_original[0, -1, david_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_reconstructed[0, -1, chris_token] - logits_reconstructed[0, -1, david_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- do a basic decomp of qk with john/mary\n",
    "- make the reverse ioi example and look at whether it points to specific features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(cache_original[\"blocks.10.attn.hook_attn_scores\"][0,7].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(cache_reconstructed_res_stream[\"blocks.10.attn.hook_attn_scores\"][0,7].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_reconstructed_res_stream[\"blocks.10.attn.hook_attn_scores\"][0,7,-1,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_out_keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "(sae_out_keys.T @ model.blocks[10].attn.QK[7] @ sae_out_query) # / np.sqrt(model.cfg.d_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 0\n",
    "sae_out_keys = sparse_autoencoder_200M(cache_original[\"blocks.10.hook_resid_pre\"][example,4])[0]\n",
    "sae_out_query = sparse_autoencoder_200M(cache_original[\"blocks.10.hook_resid_pre\"][example,-1])[0]\n",
    "key_resid_pre_feature_acts = sparse_autoencoder_200M(cache_original[\"blocks.10.hook_resid_pre\"][example,4])[1]\n",
    "query_resid_pre_feature_acts = sparse_autoencoder_200M(cache_original[\"blocks.10.hook_resid_pre\"][example,-1])[1]\n",
    "\n",
    "\n",
    "firing_key_features = (key_resid_pre_feature_acts > 0).float()\n",
    "firing_query_features = (query_resid_pre_feature_acts > 0).float()\n",
    "indices_keys = torch.nonzero(firing_key_features).flatten()\n",
    "print(indices_keys)\n",
    "indices_queries = torch.nonzero(firing_query_features).flatten()\n",
    "print(indices_queries)\n",
    "\n",
    "decomposed_keys = key_resid_pre_feature_acts[indices_keys, None] * sparse_autoencoder_200M.W_dec[indices_keys]\n",
    "decomposed_queries = query_resid_pre_feature_acts[indices_queries, None] * sparse_autoencoder_200M.W_dec[indices_queries]\n",
    "print(decomposed_keys.shape, decomposed_queries.shape)\n",
    "\n",
    "sae_qk_circuit_instance = decomposed_keys @ model.blocks[10].attn.QK[7] @ decomposed_queries.T\n",
    "sae_qk_circuit_instance = sae_qk_circuit_instance.AB.detach().cpu()\n",
    "print(sae_qk_circuit_instance.shape)\n",
    "print((sae_qk_circuit_instance > 0).sum())\n",
    "print(\"Score Sum\", sae_qk_circuit_instance.sum().item())\n",
    "\n",
    "fig = px.imshow(sae_qk_circuit_instance.numpy(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "\n",
    "# add xtick and y tick labels with the key and query indices\n",
    "fig.update_xaxes(\n",
    "    tickvals=list(range(len(indices_queries))),\n",
    "    ticktext=[str(i.item()) for i in indices_queries],\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    tickvals=list(range(len(indices_keys))),\n",
    "    ticktext=[str(i.item()) for i in indices_keys],\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "px.bar(x=[str(i.item()) for i in indices_queries],  y = sae_qk_circuit_instance.sum(0).numpy()).show()\n",
    "px.bar(x=[str(i.item()) for i in indices_keys],  y = sae_qk_circuit_instance.sum(1).numpy()).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 0\n",
    "sae_out_keys = sparse_autoencoder_200M(cache_original[\"blocks.10.hook_resid_pre\"][example,4])[0]\n",
    "sae_out_query = sparse_autoencoder_200M(cache_original[\"blocks.10.hook_resid_pre\"][example,-1])[0]\n",
    "key_resid_pre_feature_acts = sparse_autoencoder_200M(cache_original[\"blocks.10.hook_resid_pre\"][example,4])[1]\n",
    "query_resid_pre_feature_acts = sparse_autoencoder_200M(cache_original[\"blocks.10.hook_resid_pre\"][example,-1])[1]\n",
    "\n",
    "firing_key_features = (key_resid_pre_feature_acts > 0).float()\n",
    "firing_query_features = (query_resid_pre_feature_acts > 0).float()\n",
    "indices_keys = torch.nonzero(firing_key_features).flatten()\n",
    "print(indices_keys)\n",
    "indices_queries = torch.nonzero(firing_query_features).flatten()\n",
    "print(indices_queries)\n",
    "\n",
    "decomposed_keys = key_resid_pre_feature_acts[indices_keys, None] * sparse_autoencoder_200M.W_dec[indices_keys]\n",
    "decomposed_queries = query_resid_pre_feature_acts[indices_queries, None] * sparse_autoencoder_200M.W_dec[indices_queries]\n",
    "print(decomposed_keys.shape, decomposed_queries.shape)\n",
    "\n",
    "\n",
    "\n",
    "sae_qk_circuit_instance = decomposed_keys @ model.blocks[10].attn.QK[7] @ decomposed_queries.T\n",
    "sae_qk_circuit_instance = sae_qk_circuit_instance.AB.detach().cpu()\n",
    "print(sae_qk_circuit_instance.shape)\n",
    "print((sae_qk_circuit_instance > 0).sum())\n",
    "\n",
    "fig = px.imshow(sae_qk_circuit_instance.numpy(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "\n",
    "# add xtick and y tick labels with the key and query indices\n",
    "fig.update_xaxes(\n",
    "    tickvals=list(range(len(indices_queries))),\n",
    "    ticktext=[str(i.item()) for i in indices_queries],\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    tickvals=list(range(len(indices_keys))),\n",
    "    ticktext=[str(i.item()) for i in indices_keys],\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "px.bar(x=[str(i.item()) for i in indices_queries],  y = sae_qk_circuit_instance.sum(0).numpy()).show()\n",
    "px.bar(x=[str(i.item()) for i in indices_keys],  y = sae_qk_circuit_instance.sum(1).numpy()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "values, indices = torch.topk(sae_qk_circuit_instance.AB.detach().flatten(),25)\n",
    "d_enc = sparse_autoencoder_200M.cfg.d_sae\n",
    "start_topk_ind = (indices // d_enc)\n",
    "end_topk_ind = (indices % d_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Feature Interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, inds = torch.topk(feature_acts[0,-1].detach().cpu(),10)\n",
    "px.bar(x=[str(i) for i in inds], y=vals).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = model.tokenizer.vocab\n",
    "vocab_dict = {v: k.replace(\"Ä \", \" \").replace(\"\\n\", \"\\\\n\") for k, v in vocab_dict.items()}\n",
    "\n",
    "vocab_dict_filepath = Path(os.getcwd()) / \"vocab_dict.json\"\n",
    "if not vocab_dict_filepath.exists():\n",
    "    with open(vocab_dict_filepath, \"w\") as f:\n",
    "        json.dump(vocab_dict, f)\n",
    "        \n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\") # currently use this dataset to avoid deal with tokenization while streaming\n",
    "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "all_tokens = tokenized_data[\"tokens\"]\n",
    "\n",
    "\n",
    "# Currently, don't think much more time can be squeezed out of it. Maybe the best saving would be to\n",
    "# make the entire sequence indexing parallelized, but that's possibly not worth it right now.\n",
    "\n",
    "max_batch_size = 512\n",
    "total_batch_size = 4096*5\n",
    "feature_idx = list(inds.flatten().cpu().numpy())\n",
    "# max_batch_size = 512\n",
    "# total_batch_size = 16384\n",
    "# feature_idx = list(range(1000))\n",
    "\n",
    "tokens = all_tokens[:total_batch_size]\n",
    "\n",
    "feature_data: Dict[int, FeatureData] = get_feature_data(\n",
    "    encoder=sparse_autoencoder_200M,\n",
    "    # encoder_B=sparse_autoencoder,\n",
    "    model=model,\n",
    "    hook_point=sparse_autoencoder_200M.cfg.hook_point,\n",
    "    hook_point_layer=sparse_autoencoder_200M.cfg.hook_point_layer,\n",
    "    tokens=tokens,\n",
    "    feature_idx=feature_idx,\n",
    "    max_batch_size=max_batch_size,\n",
    "    left_hand_k = 3,\n",
    "    buffer = (5, 5),\n",
    "    n_groups = 10,\n",
    "    first_group_size = 20,\n",
    "    other_groups_size = 5,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "\n",
    "for test_idx in list(inds.flatten().cpu().numpy()):\n",
    "    html_str = feature_data[test_idx].get_all_html()\n",
    "    with open(f\"data_{test_idx:04}.html\", \"w\") as f:\n",
    "        f.write(html_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce a number of html files which each contain a dashboard showing feature activation on the sample data. It currently doesn't process that much data so it isn't that useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuesday Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_prompt = \"When John and Mary went to the shops, John gave the bag to\"\n",
    "example_answer = \" Mary\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logits_original, cache_original = model.run_with_cache(example_prompt, prepend_bos=True)\n",
    "tokens = model.to_tokens(example_prompt)\n",
    "sae_out, feature_acts, loss, mse_loss, l1_loss = sparse_autoencoder_200M(\n",
    "    cache_original[sparse_autoencoder_200M.cfg.hook_point]\n",
    ")\n",
    "\n",
    "\n",
    "def reconstr_hook(mlp_out, hook, new_mlp_out):\n",
    "    # print(mlp_out.shape, new_mlp_out.shape)\n",
    "    return new_mlp_out\n",
    "\n",
    "model.reset_hooks()\n",
    "with model.hooks(\n",
    "    fwd_hooks=[\n",
    "        (\n",
    "            utils.get_act_name(\"resid_pre\", 10),\n",
    "            partial(reconstr_hook, new_mlp_out=sae_out),\n",
    "        )\n",
    "    ]\n",
    "):\n",
    "    utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
    "    logits_reconstructed, cache_reconstructed_res_stream = model.run_with_cache(example_prompt, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reconstructed_q = cache_reconstructed_res_stream[\"blocks.10.attn.hook_q\"].detach()\n",
    "reconstructed_k = cache_reconstructed_res_stream[\"blocks.10.attn.hook_k\"].detach()\n",
    "\n",
    "\n",
    "def reconstr_key_hook(key, hook, reconstructed_key):\n",
    "    return reconstructed_key\n",
    "\n",
    "def reconstr_query_hook(query, hook, reconstructed_query):\n",
    "    print(\"reconstr_query_hook\", query.shape, reconstructed_query.shape)\n",
    "    if query.shape == reconstructed_query.shape:\n",
    "        return reconstructed_query\n",
    "    else:\n",
    "        new_query = torch.concat(\n",
    "            [query[:,0].unsqueeze(1), reconstructed_query],\n",
    "            dim=1\n",
    "        )\n",
    "        return new_query\n",
    "\n",
    "model.reset_hooks()\n",
    "with model.hooks(\n",
    "    fwd_hooks=[\n",
    "        (\n",
    "            \"blocks.10.attn.hook_q\",\n",
    "            partial(reconstr_query_hook, reconstructed_query=reconstructed_q),\n",
    "        )\n",
    "    ]\n",
    "):\n",
    "    utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
    "    _, cache_reconstructed_queries = model.run_with_cache(example_prompt, prepend_bos=True)\n",
    "\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_k = cache_reconstructed_res_stream[\"blocks.10.attn.hook_k\"].detach()\n",
    "    \n",
    "def reconstr_key_hook(key, hook, reconstructed_key):\n",
    "    print(\"reconstr_key_hook\", key.shape, reconstructed_key.shape)\n",
    "    if key.shape == reconstructed_key.shape:\n",
    "        return reconstructed_key\n",
    "    else:\n",
    "        new_key = torch.concat(\n",
    "            [key[:,0].unsqueeze(1), reconstructed_key],\n",
    "            dim=1\n",
    "        )\n",
    "        return new_key\n",
    "\n",
    "model.reset_hooks()\n",
    "with model.hooks(\n",
    "    fwd_hooks=[\n",
    "        (\n",
    "            \"blocks.10.attn.hook_k\",\n",
    "            partial(reconstr_key_hook, reconstructed_key=reconstructed_k),\n",
    "        )\n",
    "    ]\n",
    "):\n",
    "    utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
    "    _, cache_reconstructed_keys = model.run_with_cache(example_prompt, prepend_bos=True)\n",
    "\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the MSE Loss by Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse_loss_cache_df(original_cache, intervention_cache):\n",
    "    mse_loss = lambda x, y: (x - y).pow(2).mean()\n",
    "    keys = []\n",
    "    values = []\n",
    "    for key in cache_original.keys():\n",
    "        keys.append(key)\n",
    "        values.append(mse_loss(original_cache[key], intervention_cache[key]).item())\n",
    "    df = pd.DataFrame({\"key\": keys, \"mse_loss\": values})\n",
    "    \n",
    "    # get the index of the first non-zero mse_loss\n",
    "    first_non_zero_idx = df[df[\"mse_loss\"] > 0].index[0]\n",
    "    # filter from there onward\n",
    "    df = df.iloc[first_non_zero_idx:]\n",
    "    return df\n",
    "\n",
    "df = get_mse_loss_cache_df(cache_original, cache_reconstructed_res_stream)\n",
    "px.line(df, x=\"key\", y=\"mse_loss\").show()\n",
    "\n",
    "df = get_mse_loss_cache_df(cache_original, cache_reconstructed_queries)\n",
    "px.line(df, x=\"key\", y=\"mse_loss\").show()\n",
    "\n",
    "df = get_mse_loss_cache_df(cache_original, cache_reconstructed_keys)\n",
    "px.line(df, x=\"key\", y=\"mse_loss\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize attn patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.attention import attention_patterns\n",
    "patterns_original = cache_original[\"blocks.10.attn.hook_pattern\"][0]\n",
    "attention_patterns(tokens=model.to_str_tokens(example_prompt), attention=patterns_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns_reconstructed = cache_reconstructed_res_stream[\"blocks.10.attn.hook_pattern\"][0].detach().cpu()\n",
    "# patterns_reconstructed = cache_reconstructed_keys[\"blocks.10.attn.hook_pattern\"][0].detach().cpu()\n",
    "patterns_reconstructed = cache_reconstructed_queries[\"blocks.10.attn.hook_pattern\"][0].detach().cpu()\n",
    "attention_patterns(tokens=model.to_str_tokens(example_prompt), attention=patterns_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize change in Attn Scores/Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_cache = cache_reconstructed_keys\n",
    "scores_original = cache_original[\"blocks.10.attn.hook_attn_scores\"][0].detach().cpu()\n",
    "scores_reconstructed = intervention_cache[\"blocks.10.attn.hook_attn_scores\"][0].detach().cpu()\n",
    "patterns_original = cache_original[\"blocks.10.attn.hook_pattern\"][0].detach().cpu()\n",
    "patterns_reconstructed = intervention_cache[\"blocks.10.attn.hook_pattern\"][0].detach().cpu()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def tensor_to_long_data_frame(tensor_result, dimension_names, value_name = \"Score\"):\n",
    "    assert len(tensor_result.shape) == len(\n",
    "        dimension_names\n",
    "    ), \"The number of dimension names must match the number of dimensions in the tensor\"\n",
    "\n",
    "    tensor_2d = tensor_result.reshape(-1).detach().cpu()\n",
    "    df = pd.DataFrame(tensor_2d.detach().numpy(), columns=[value_name])\n",
    "\n",
    "    indices = pd.MultiIndex.from_tuples(\n",
    "        list(np.ndindex(tensor_result.shape)),\n",
    "        names=dimension_names,\n",
    "    )\n",
    "    df.index = indices\n",
    "    \n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    # set all dimensions except Score to categorical\n",
    "    for i in range(len(dimension_names)):\n",
    "        df[dimension_names[i]] = df[dimension_names[i]].astype(\"category\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "scores_df = tensor_to_long_data_frame(scores_original, [\"Head\", \"Query\", \"Key\"], value_name = \"Original\")\n",
    "scores_df[\"Reconstructed\"] = scores_reconstructed.flatten().detach().cpu().numpy()\n",
    "scores_df[\"Pattern Original\"] = patterns_original.flatten().detach().cpu().numpy()\n",
    "scores_df[\"Pattern Reconstructed\"] = patterns_reconstructed.flatten().detach().cpu().numpy()\n",
    "scores_df = scores_df[scores_df[\"Original\"] != float(\"inf\")]\n",
    "scores_df = scores_df[scores_df[\"Reconstructed\"] != float(\"inf\")]\n",
    "scores_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pandas default width/height\n",
    "fig = px.scatter(scores_df, x=\"Original\", y=\"Reconstructed\", color = \"Head\", hover_data=[\"Head\", \"Query\", \"Key\"])\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(scores_df, x=\"Pattern Original\", y=\"Pattern Reconstructed\", color = \"Head\", hover_data=[\"Head\", \"Query\", \"Key\"], log_x=True, log_y=True)\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(scores_df, x=\"Pattern Original\", y=\"Pattern Reconstructed\", color = \"Key\", hover_data=[\"Head\", \"Query\", \"Key\"], log_x=True, log_y=True)\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import einsum\n",
    "\n",
    "\n",
    "def kl_divergence_attention(y_true, y_pred):\n",
    "\n",
    "    # Compute log probabilities for KL divergence\n",
    "    log_y_true = torch.log(y_true)\n",
    "    log_y_pred = torch.log(y_pred)\n",
    "\n",
    "    return y_true * (log_y_true - log_y_pred)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(patterns_original.shape)\n",
    "kl_result = kl_divergence_attention(patterns_original, patterns_reconstructed)\n",
    "kl_result[kl_result.isnan()] = 0\n",
    "fig = px.imshow(kl_result.sum(dim=-1).detach().cpu(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\",\n",
    "                labels = dict(x=\"Query\", y=\"Head\"), text_auto=\".2f\")\n",
    "fig.layout.coloraxis.colorbar.title = \"KL Divergence\"\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
