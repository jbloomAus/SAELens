{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O8tQblzOVHu"
      },
      "source": [
        "# A very basic SAE Training Tutorial\n",
        "\n",
        "Please note that it is very easy for tutorial code to go stale so please have a low bar for raising an issue in the"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shAFb9-lOVHu"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LeRi_tw2dhae"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab # type: ignore\n",
        "    from google.colab import output\n",
        "    %pip install sae-lens transformer-lens circuitsvis\n",
        "except:\n",
        "    from IPython import get_ipython # type: ignore\n",
        "    ipython = get_ipython(); assert ipython is not None\n",
        "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
        "    ipython.run_line_magic(\"autoreload\", \"2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy-b3CcSOVHu",
        "outputId": "58ce28d0-f91f-436d-cf87-76bb26e2ecaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/daniel/.cache/pypoetry/virtualenvs/sae-lens-kHvvStyh-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "from sae_lens.config import LanguageModelTranscoderRunnerConfig\n",
        "from sae_lens.sae_training_runner import TranscoderTrainingRunner\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe2nlqf-OVHv"
      },
      "source": [
        "# Model Selection and Evaluation (Feel Free to Skip)\n",
        "\n",
        "We'll use the runner to train an SAE on a TinyStories Model. This is a very small model so we can train an SAE on it quite quickly. Before we get started, let's load in the model with `transformer_lens` and see what it can do.\n",
        "\n",
        "TransformerLens gives us 2 functions that are useful here (and circuits viz provides a third):\n",
        "1. `transformer_lens.utils.test_prompt` will help us see when the model can infer one token.\n",
        "2. `HookedTransformer.generate` will help us see what happens when we sample from the model.\n",
        "3. `circuitsvis.logits.token_log_probs` will help us visualize the log probs of tokens at several positions in a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hFz6JUMuOVHv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/daniel/.cache/pypoetry/virtualenvs/sae-lens-kHvvStyh-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model tiny-stories-1L-21M into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "from transformer_lens import HookedTransformer\n",
        "\n",
        "model = HookedTransformer.from_pretrained(\n",
        "    \"tiny-stories-1L-21M\"\n",
        ")  # This will wrap huggingface models and has lots of nice utilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUiXrjdUOVHv"
      },
      "source": [
        "### Getting a vibe for a model using `model.generate`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZfKT5aDOVHv"
      },
      "source": [
        "Let's start by generating some stories using the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G4ad4Zz1OVHv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Once upon a time there was a colorful rock. The powerful elephant was walking near a cave when she bumped into it. Each time, the elephant tried to push down on his trunk, but it didn't work. Then one day the elephant saw a zoo with the officers\""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'Once upon a time passes through a park, the family praises their kindness. They say they are applauding everyone and the room is cheering them on.\\nWords: applaud, alligator, comfortable\\nStory: \\n\\nOne day, there was a little girl.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'Once upon a time, there was a pretty girl named Lily. She loved to explore and discover new things. One day, she decided to go on a hike in the forest.\\n\\nAs she was walking, she met a little bird who was very kind. The'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'Once upon a time, a small girl named Tim hopped on a big gray stair. He had never seen a stair before and he was so excited!\\n\\nTim hopped and hopped all around the block and gasped in delight. He was so happy and counted to ten.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'Once upon a time, there was a little girl named True. She had an emergency. She reversed things! It was her own special thing to do and she could drive the car again. She drove it in her hand all the time.\\n\\nShe would never forget'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# here we use generate to get 10 completeions with temperature 1. Feel free to play with the prompt to make it more interesting.\n",
        "for i in range(5):\n",
        "    display(\n",
        "        model.generate(\n",
        "            \"Once upon a time\",\n",
        "            stop_at_eos=False,  # avoids a bug on MPS\n",
        "            temperature=1,\n",
        "            verbose=False,\n",
        "            max_new_tokens=50,\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDKr8o1xOVHv"
      },
      "source": [
        "One thing we notice is that the model seems to be able to repeat the name of the main character very consistently. It can output a pronoun intead but in some stories will repeat the protagonists name. This seems like an interesting capability to analyse with SAEs. To better understand the models ability to remember the protagonists name, let's extract a prompt where the next character is determined and use the \"test_prompt\" utility from TransformerLens to check the ranking of the token for that name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsfJX-YpOVHv"
      },
      "source": [
        "### Spot checking model abilities with `transformer_lens.utils.test_prompt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TpmPoj7uOVHv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized prompt: ['<|endoftext|>', 'Once', ' upon', ' a', ' time', ',', ' there', ' was', ' a', ' little', ' girl', ' named', ' Lily', '.', ' She', ' lived', ' in', ' a', ' big', ',', ' happy', ' little', ' girl', '.', ' On', ' her', ' big', ' adventure', ',']\n",
            "Tokenized answer: [' Lily']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
              "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.81</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.46</span><span style=\"font-weight: bold\">% Token: | Lily|</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Performance on answer token:\n",
              "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.81\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m13.46\u001b[0m\u001b[1m% Token: | Lily|\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 0th token. Logit: 20.48 Prob: 71.06% Token: | she|\n",
            "Top 1th token. Logit: 18.81 Prob: 13.46% Token: | Lily|\n",
            "Top 2th token. Logit: 17.35 Prob:  3.11% Token: | the|\n",
            "Top 3th token. Logit: 17.26 Prob:  2.86% Token: | her|\n",
            "Top 4th token. Logit: 16.74 Prob:  1.70% Token: | there|\n",
            "Top 5th token. Logit: 16.43 Prob:  1.25% Token: | they|\n",
            "Top 6th token. Logit: 15.80 Prob:  0.66% Token: | all|\n",
            "Top 7th token. Logit: 15.64 Prob:  0.56% Token: | things|\n",
            "Top 8th token. Logit: 15.28 Prob:  0.39% Token: | one|\n",
            "Top 9th token. Logit: 15.24 Prob:  0.38% Token: | lived|\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Lily'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Lily'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformer_lens.utils import test_prompt\n",
        "\n",
        "# Test the model with a prompt\n",
        "test_prompt(\n",
        "    \"Once upon a time, there was a little girl named Lily. She lived in a big, happy little girl. On her big adventure,\",\n",
        "    \" Lily\",\n",
        "    model,\n",
        "    prepend_space_to_answer=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGzOvReDOVHv"
      },
      "source": [
        "In the output above, we see that the model assigns ~ 70% probability to \"she\" being the next token, and a 13% chance to \" Lily\" being the next token. Other names like Lucy or Anna are not highly ranked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HookedTransformer(\n",
            "  (embed): Embed()\n",
            "  (hook_embed): HookPoint()\n",
            "  (pos_embed): PosEmbed()\n",
            "  (hook_pos_embed): HookPoint()\n",
            "  (blocks): ModuleList(\n",
            "    (0): TransformerBlock(\n",
            "      (ln1): LayerNormPre(\n",
            "        (hook_scale): HookPoint()\n",
            "        (hook_normalized): HookPoint()\n",
            "      )\n",
            "      (ln2): LayerNormPre(\n",
            "        (hook_scale): HookPoint()\n",
            "        (hook_normalized): HookPoint()\n",
            "      )\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_pattern): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (hook_pre): HookPoint()\n",
            "        (hook_post): HookPoint()\n",
            "      )\n",
            "      (hook_attn_in): HookPoint()\n",
            "      (hook_q_input): HookPoint()\n",
            "      (hook_k_input): HookPoint()\n",
            "      (hook_v_input): HookPoint()\n",
            "      (hook_mlp_in): HookPoint()\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_mid): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "  )\n",
            "  (ln_final): LayerNormPre(\n",
            "    (hook_scale): HookPoint()\n",
            "    (hook_normalized): HookPoint()\n",
            "  )\n",
            "  (unembed): Unembed()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er3H1TDoOVHw"
      },
      "source": [
        "# Training an SAE\n",
        "\n",
        "Now we're ready to train out SAE. We'll make a runner config, instantiate the runner and the rest is taken care of for us!\n",
        "\n",
        "During training, you use weights and biases to check key metrics which indicate how well we are able to optimize the variables we care about.\n",
        "\n",
        "To get a better sense of which variables to look at, you can read my (Joseph's) post [here](https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream) and especially look at my weights and biases report [here](https://links-cdn.wandb.ai/wandb-public-images/links/jbloom/uue9i416.html).\n",
        "\n",
        "A few tips:\n",
        "- Feel free to reorganize your wandb dashboard to put L0, CE_Loss_score, explained variance and other key metrics in one section at the top.\n",
        "- Make a [run comparer](https://docs.wandb.ai/guides/app/features/panels/run-comparer) when tuning hyperparameters.\n",
        "- You can download the resulting sparse autoencoder / sparsity estimate from wandb and upload them to huggingface if you want to share your SAE with other.\n",
        "    - cfg.json (training config)\n",
        "    - sae_weight.safetensors (model weights)\n",
        "    - sparsity.safetensors (sparsity estimate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCHtPycOOVHw"
      },
      "source": [
        "## MLP Out\n",
        "\n",
        "I've tuned the hyperparameters below for a decent SAE which achieves 86% CE Loss recovered and an L0 of ~85, and runs in about 2 hours on an M3 Max. You can get an SAE that looks better faster if you only consider L0 and CE loss but it will likely have more dense features and more dead features. Here's a link to my output with two runs with two different L1's: https://wandb.ai/jbloom/sae_lens_tutorial ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oAsZCAdJOVHw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run name: 16384-L1-5-LR-5e-05-Tokens-1.229e+08\n",
            "n_tokens_per_buffer (millions): 0.262144\n",
            "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
            "Total training steps: 30000\n",
            "Total wandb updates: 1000\n",
            "n_tokens_per_feature_sampling_window (millions): 1048.576\n",
            "n_tokens_per_dead_feature_window (millions): 1048.576\n",
            "We will reset the sparsity calculation 30 times.\n",
            "Number tokens in sparsity calculation window: 4.10e+06\n",
            "Loaded pretrained model tiny-stories-1L-21M into HookedTransformer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdtch1997\u001b[0m (\u001b[33msae-experiments\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/daniel/ml_workspace/SAELens/tutorials/wandb/run-20240615_190734-yc4n5b77</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sae-experiments/sae_lens_tutorial/runs/yc4n5b77' target=\"_blank\">16384-L1-5-LR-5e-05-Tokens-1.229e+08</a></strong> to <a href='https://wandb.ai/sae-experiments/sae_lens_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/sae-experiments/sae_lens_tutorial' target=\"_blank\">https://wandb.ai/sae-experiments/sae_lens_tutorial</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/sae-experiments/sae_lens_tutorial/runs/yc4n5b77' target=\"_blank\">https://wandb.ai/sae-experiments/sae_lens_tutorial/runs/yc4n5b77</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training SAE:   0%|          | 0/122880000 [00:00<?, ?it/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7498f1772210>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/daniel/.cache/pypoetry/virtualenvs/sae-lens-kHvvStyh-py3.11/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
            "    def _clean_thread_parent_frames(\n",
            "\n",
            "  File \"/home/daniel/ml_workspace/SAELens/sae_lens/sae_training_runner.py\", line 32, in interrupt_callback\n",
            "    raise InterruptedException()\n",
            "sae_lens.sae_training_runner.InterruptedException: \n"
          ]
        }
      ],
      "source": [
        "total_training_steps = 30_000  # probably we should do more\n",
        "batch_size = 4096\n",
        "total_training_tokens = total_training_steps * batch_size\n",
        "\n",
        "lr_warm_up_steps = 0\n",
        "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
        "l1_warm_up_steps = total_training_steps // 20  # 5% of training\n",
        "\n",
        "cfg = LanguageModelTranscoderRunnerConfig(\n",
        "    # Data Generating Function (Model + Training Distibuion)\n",
        "    model_name=\"tiny-stories-1L-21M\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n",
        "    hook_name=\"blocks.0.ln2.hook_normalized\",\n",
        "    hook_name_out=\"blocks.0.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n",
        "    hook_layer=0,  # Only one layer in the model.\n",
        "    hook_layer_out=0,  # Only one layer in the model.\n",
        "    d_in=1024,  # the width of the mlp input.\n",
        "    d_out=1024,  # the width of the mlp output.\n",
        "    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.\n",
        "    is_dataset_tokenized=True,\n",
        "    streaming=True,  # we could pre-download the token dataset if it was small.\n",
        "    # SAE Parameters\n",
        "    mse_loss_normalization=None,  # We won't normalize the mse loss,\n",
        "    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.\n",
        "    b_dec_init_method=\"zeros\",  # The geometric median can be used to initialize the decoder weights.\n",
        "    apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.\n",
        "    normalize_sae_decoder=False,\n",
        "    scale_sparsity_penalty_by_decoder_norm=True,\n",
        "    decoder_heuristic_init=True,\n",
        "    init_encoder_as_decoder_transpose=True,\n",
        "    normalize_activations=\"expected_average_only_in\",\n",
        "    # Training Parameters\n",
        "    lr=5e-5,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
        "    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)\n",
        "    adam_beta2=0.999,\n",
        "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
        "    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.\n",
        "    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.\n",
        "    l1_coefficient=5,  # will control how sparse the feature activations are\n",
        "    l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.\n",
        "    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n",
        "    train_batch_size_tokens=batch_size,\n",
        "    context_size=256,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.\n",
        "    # Activation Store Parameters\n",
        "    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n",
        "    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
        "    store_batch_size_prompts=16,\n",
        "    # Resampling protocol\n",
        "    use_ghost_grads=False,  # we don't use ghost grads anymore.\n",
        "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
        "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
        "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
        "    # WANDB\n",
        "    log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
        "    wandb_project=\"sae_lens_tutorial\",\n",
        "    wandb_log_frequency=30,\n",
        "    eval_every_n_wandb_logs=20,\n",
        "    # Misc\n",
        "    device=device,\n",
        "    seed=42,\n",
        "    n_checkpoints=0,\n",
        "    checkpoint_path=\"checkpoints\",\n",
        "    dtype=\"float32\"\n",
        ")\n",
        "# look at the next cell to see some instruction for what to do while this is running.\n",
        "sparse_autoencoder = TranscoderTrainingRunner(cfg).run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
