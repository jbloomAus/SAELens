{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading trained SAEs to Huggingface\n",
    "\n",
    "After you've trained a set of SAEs for a model, you'll likely want to upload the SAEs to Huggingface for others to use as well. SAELens provides a helper function `upload_saes_to_huggingface()` which makes this easy to do. This notebook demonstrates how to use this function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate, we're just going to download some of Joseph's GPT2 SAEs and re-upload them. Of course, in practice you'd want to train your own SAEs and upload those instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "# Load some SAEs from the gpt2-small-res-jb release\n",
    "# In practice you'll want to train your own SAEs to upload to huggingface\n",
    "layer_0_sae = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.0.hook_resid_pre\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "layer_1_sae = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.1.hook_resid_pre\",\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the SAEs\n",
    "\n",
    "The upload function takes a dictionary of SAEs, where the keys will become the sae_id of the SAE on Huggingface. It's a good idea to use the hook point of the SAE as the sae_id, so it's clear to users where the SAE should be attached. The values of this dictionary can either be the SAEs themselves, or a path to a directory where the SAEs have been saved using `sae.save_model()`. Below, we'll do both! One of the SAEs will be uploaded directly from the SAE object, and the other will be uploaded from a directory.\n",
    "\n",
    "Note that you'll need to be logged in to your huggingface account either by running `huggingface-cli login` in the terminal or by setting the `HF_TOKEN` environment variable to your API token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import upload_saes_to_huggingface\n",
    "\n",
    "layer_1_sae_path = \"layer_1_sae\"\n",
    "layer_1_sae.save_model(layer_1_sae_path)\n",
    "\n",
    "saes_dict = {\n",
    "    \"blocks.0.hook_resid_pre\": layer_0_sae,  # values can be an SAE object\n",
    "    \"blocks.1.hook_resid_pre\": layer_1_sae_path,  # or a path to a saved SAE\n",
    "}\n",
    "\n",
    "upload_saes_to_huggingface(\n",
    "    saes_dict,\n",
    "    # change this to your own huggingface username and repo\n",
    "    hf_repo_id=\"chanind/sae-lens-upload-demo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our uploaded SAEs\n",
    "\n",
    "Now that the SAEs are uploaded, anyone can load them using `SAE.from_pretrained()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "uploaded_sae_layer_0 = SAE.from_pretrained(\n",
    "    release=\"chanind/sae-lens-upload-demo\",\n",
    "    sae_id=\"blocks.0.hook_resid_pre\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "uploaded_sae_layer_1 = SAE.from_pretrained(\n",
    "    release=\"chanind/sae-lens-upload-demo\",\n",
    "    sae_id=\"blocks.1.hook_resid_pre\",\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-lens-CSfAEFdT-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
