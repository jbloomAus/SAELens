{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretokenizing Datasets\n",
    "\n",
    "An easy way to speed up SAE training is to pretokenize the training dataset so it's already tokenized and batched for all subsequent training runs. Pretokenized datasets can also be uploaded to Huggingface so they're easily shared between with other researchers experimenting with SAEs.\n",
    "\n",
    "This notebook will show how to pretokenize a dataset and upload it to Huggingface using the built-in pretokenize runner in SAELens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core hyperparameters and special tokens\n",
    "\n",
    "Pretokenizing a dataset will encode each sentence in the dataset into tokens, form these into batches of length `context_size` tokens, and optionally shuffle the batches. This process shares a lot of similarities with language model (LM) training, and indeed we want our SAEs to be trained using datasets that behave similarly to how the original LM was trained.\n",
    "\n",
    "Importantly, we want the special tokens used during pretokenization to match the special tokens used by the LM during training. For instance, if the LM was trained with a `<bos>` token at the start of each batch, with each sentence in the batch separated by an `<eos>` token, then our pretokenized dataset should match that behavior.\n",
    "\n",
    "In SAELens, we can control this special token behavior with the following 3 options in the config:\n",
    "- `begin_batch_token`: If not `None`, this token will be prepended to the start of each batch. By default this is the `<bos>` token\n",
    "- `begin_sequence_token`: If not `None`, this token will be prepended to the start of every sentence, regardless of where in the batch the sentence starts. By default this is `None`\n",
    "- `sequence_separator_token`: If not `None`, this tokken will be placed between sentences in a batch to act as a separator. By default, this is the `<eos>` token.\n",
    "\n",
    "For each of the above options, you can pass the string `\"bos\"`, `\"eos\"`, or `\"sep\"` for the `<bos>`, `<eos>`, or `<sep>` tokens, respectively. You can also pass in a token ID as an int if you need to customize this further.\n",
    "\n",
    "The special tokens you should use will vary depending on the LM, so you should check how the original LM was trained and select pretokenization settings that match as closely as possible. For some LMs like GPT2, the `<bos>` and `<eos>` tokens are actually the same token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading to Huggingface\n",
    "\n",
    "If you want to upload your dataset to Huggingface, you just need to set the param `hf_repo_id`. This should be of the form `<huggingface username>/<dataset repo name>`. It's a good idea to name the repo descriptively, including the tokenizer you used and the word `tokenized`. For instance, if you're tokenizing a dataset called `web-text` with `gpt2`, and your username is `hf_user`, you might pass a `hf_repo_id` like `hf_user/web-text-tokenized-gpt2`.\n",
    "\n",
    "You can also save the dataset locally by setting `save_path` to a path on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import PretokenizeRunner, PretokenizeRunnerConfig\n",
    "\n",
    "cfg = PretokenizeRunnerConfig(\n",
    "    tokenizer_name=\"gpt2\",\n",
    "    dataset_path=\"NeelNanda/c4-10k\", # this is just a tiny test dataset\n",
    "    shuffle=True,\n",
    "    num_proc=4, # increase this number depending on how many CPUs you have\n",
    "\n",
    "    # tweak these settings depending on the model\n",
    "    context_size=128,\n",
    "    begin_batch_token=\"bos\",\n",
    "    begin_sequence_token=None,\n",
    "    sequence_separator_token=\"eos\",\n",
    "\n",
    "    # uncomment to upload to huggingface\n",
    "    # hf_repo_id=\"your-username/c4-10k-tokenized-gpt2\"\n",
    "\n",
    "    # uncomment to save the dataset locally\n",
    "    # save_path=\"./c4-10k-tokenized-gpt2\"\n",
    ")\n",
    "\n",
    "dataset = PretokenizeRunner(cfg).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting our new dataset\n",
    "\n",
    "Our dataset now contains a single key `input_ids` with tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenized_row = dataset['input_ids'][5]\n",
    "\n",
    "print(f\"Row has {len(tokenized_row)} tokens\")\n",
    "print(f\"Decoded: {tokenizer.decode(tokenized_row)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-lens-CSfAEFdT-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
