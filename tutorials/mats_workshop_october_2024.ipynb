{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATS Workshop October 2024: \"Hands-On Tutorial with SAELens & Neuronpedia\"\n",
    "\n",
    "This notebook is the companion notebook to a session run for Neel Nanda's MATS scholars.\n",
    "\n",
    "The slides for this session are [here](https://docs.google.com/presentation/d/13c-oTOz5n_VPIo9H_6RJiGgT75E1sSC1d0MYNfR07Q4/edit?usp=sharing). \n",
    "\n",
    "Much of this content is directly taken from the [ARENA tutorials](https://colab.research.google.com/drive/1ePkM8oBHIEZ2kcqAiA3waeAmz8RSdHmq#scrollTo=GQgKfUNekdPv). I've skipped some sections for speed and added some which are less code-y and more about using Neuronpedia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to install a fresh enviroment, the following commands might be useful:\n",
    "\n",
    "```bash\n",
    "conda create -n sae-lens-workshop python=3.12 -y\n",
    "conda activate sae-lens-workshop\n",
    "pip install sae-lens sae-dashboard ipykernel tabulate\n",
    "```\n",
    "\n",
    "If you think you are likely to want to edit SAE Lens, then using poetry to install the package is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sae-lens ipykernel tabulate plotly-express nbformat\n",
    "! pip install git+https://github.com/callummcdougall/sae_vis.git@callum/v3\n",
    "! pip install frozendict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens sae-vis plotly-express nbformat\n",
    "except:\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Imports for displaying vis in Colab / notebook\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# For the most part I'll try to import functions and classes near where they are used\n",
    "# to make it clear where they come from.\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For displaying sae-vis inline\n",
    "\n",
    "import http.server\n",
    "import socketserver\n",
    "import threading\n",
    "import webbrowser\n",
    "\n",
    "from google.colab import output\n",
    "\n",
    "PORT = 8010\n",
    "\n",
    "\n",
    "def display_vis_inline(filename: Path, height: int = 850):\n",
    "    \"\"\"\n",
    "    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each\n",
    "    vis has a unique port without having to define a port within the function.\n",
    "    \"\"\"\n",
    "    global PORT\n",
    "\n",
    "    def serve(directory):\n",
    "        os.chdir(directory)\n",
    "        handler = http.server.SimpleHTTPRequestHandler\n",
    "        with socketserver.TCPServer((\"\", PORT), handler) as httpd:\n",
    "            print(f\"Serving files from {directory} on port {PORT}\")\n",
    "            httpd.serve_forever()\n",
    "\n",
    "    thread = threading.Thread(target=serve, args=(\"/content\",))\n",
    "    thread.start()\n",
    "\n",
    "    filename = str(filename).split(\"/content\")[-1]\n",
    "\n",
    "    output.serve_kernel_port_as_iframe(\n",
    "        PORT, path=filename, height=height, cache_in_notebook=True\n",
    "    )\n",
    "\n",
    "    PORT += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Open Source SAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core details:\n",
    "- There are lots of open-source SAEs or many different models.\n",
    "  - They vary in their architecture, training methods and other details. \n",
    "- A subset of them are supported by SAE Lens / Neuronpedia\n",
    "  - This comes down to whether or not we have the code for doing the forward pass.\n",
    "  - And whether or not we have the config for them.\n",
    "- In SAELens, we group SAEs by Hugginface Repo + path within the repo.\n",
    "  - \"release\" -> repo\n",
    "  - \"id\" -> a path. \n",
    "  - \"model\" -> We store the model (t-lens string), but we don't necessarily need to do this. \n",
    "  - \"neuronpedia_id\" -> Has a different id system so we store id's here for ease of reference.\n",
    "- We were previously considering storing information here for testing that they were running correctly but we'll probably back this out (eg: explained variance).\n",
    "\n",
    "**Which SAEs should you use for your experiments?**\n",
    "1. Smallest model that can do the task.\n",
    "2. Try SAEs of different sizes depending on what features you're interested in.\n",
    "3. Neuronpedia support is nice but not essential.\n",
    "4. SAE performance should meet at least some minimal bar (we'll talk about this later)\n",
    "\n",
    "Links:\n",
    "- Source of ground truth: https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n",
    "- SAE Lens Docs: https://jbloomaus.github.io/SAELens/sae_table/\n",
    "- Neuronpedia: https://www.neuronpedia.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(get_pretrained_saes_directory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_rows = [\n",
    "    [data.model, data.release, data.repo_id, len(data.saes_map)] for data in get_pretrained_saes_directory().values()\n",
    "]\n",
    "\n",
    "# Print all SAE releases, sorted by base model\n",
    "print(\n",
    "    tabulate(\n",
    "        sorted(metadata_rows, key=lambda x: x[0]),\n",
    "        headers=[\"model\", \"release\", \"repo_id\", \"n_saes\"],\n",
    "        tablefmt=\"simple_outline\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_value(value):\n",
    "    return \"{{{0!r}: {1!r}, ...}}\".format(*next(iter(value.items()))) if isinstance(value, dict) else repr(value)\n",
    "\n",
    "release = get_pretrained_saes_directory()[\"gpt2-small-res-jb\"]\n",
    "\n",
    "print(\n",
    "    tabulate(\n",
    "        [[k, format_value(v)] for k, v in release.__dict__.items()],\n",
    "        headers=[\"Field\", \"Value\"],\n",
    "        tablefmt=\"simple_outline\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[id, path, release.neuronpedia_id[id]] for id, path in release.saes_map.items()]\n",
    "\n",
    "print(\n",
    "    tabulate(\n",
    "        data,\n",
    "        headers=[\"SAE id\", \"SAE path (HuggingFace)\", \"Neuronpedia ID\"],\n",
    "        tablefmt=\"simple_outline\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loading SAEs\n",
    "\n",
    "- We can use from_pretrained to get HookedSAETransformer and SAEs (similar method on each).\n",
    "- In order to work with SAEs, it's essential to know you are running them correctly:\n",
    "  - This means that you're doing the forward pass correctly\n",
    "  - That you're using the correct inputs. (eg: context_size / dataset it was trained on)\n",
    "  - That you're using the correct model / running it correctly (eg: `from_pretrained_kwargs`)\n",
    "- Much of the work that we do with SAE Lens is try to store all of the information required to do this. \n",
    "- Note that the information to reproduce the SAE training must be sourced from elsewhere (since people train SAEs all sorts of ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from sae_lens import HookedSAETransformer, SAE\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "gpt2: HookedSAETransformer = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "gpt2_sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    device=str(device),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sae` object is an instance of the `SAE` (Sparse Autoencoder) class. There are many different SAE architectures which may have different weights or activation functions. In order to simplify working with SAEs, SAELens handles most of this complexity for you. You can run the cell below to see each of the SAE config parameters for the one we'll be using.\n",
    "\n",
    "<details>\n",
    "<summary>Click to read a description of each of the SAE config parameters.</summary>\n",
    "\n",
    "1. `architecture`: Specifies the type of SAE architecture being used, in this case, the standard architecture (encoder and decoder with hidden activations, as opposed to a gated SAE).\n",
    "2. `d_in`: Defines the input dimension of the SAE, which is 768 in this configuration.\n",
    "3. `d_sae`: Sets the dimension of the SAE's hidden layer, which is 24576 here. This represents the number of possible feature activations.\n",
    "4. `activation_fn_str`: Specifies the activation function used in the SAE, which is ReLU in this case. TopK is another option that we will not cover here.\n",
    "5. `apply_b_dec_to_input`: Determines whether to apply the decoder bias to the input, set to True here.\n",
    "6. `finetuning_scaling_factor`: Indicates whether to use a scaling factor to weight initialization and the forward pass. This is not usually used and was introduced to support a [solution for shrinkage](https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes).\n",
    "7. `context_size`: Defines the size of the context window, which is 128 tokens in this case. In turns out SAEs trained on small activations from small prompts [often don't perform well on longer prompts](https://www.lesswrong.com/posts/baJyjpktzmcmRfosq/stitching-saes-of-different-sizes).\n",
    "8. `model_name`: Specifies the name of the model being used, which is 'gpt2-small' here. [This is a valid model name in TransformerLens](https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html).\n",
    "9. `hook_name`: Indicates the specific hook in the model where the SAE is applied.\n",
    "10. `hook_layer`: Specifies the layer number where the hook is applied, which is layer 7 in this case.\n",
    "11. `hook_head_index`: Defines which attention head to hook into; not relevant here since we are looking at a residual stream SAE.\n",
    "12. `prepend_bos`: Determines whether to prepend the beginning-of-sequence token, set to True.\n",
    "13. `dataset_path`: Specifies the path to the dataset used for training or evaluation. (Can be local or a huggingface dataset.)\n",
    "14. `dataset_trust_remote_code`: Indicates whether to trust remote code (from HuggingFace) when loading the dataset, set to True.\n",
    "15. `normalize_activations`: Specifies how to normalize activations, set to 'none' in this config.\n",
    "16. `dtype`: Defines the data type for tensor operations, set to 32-bit floating point.\n",
    "17. `device`: Specifies the computational device to use.\n",
    "18. `sae_lens_training_version`: Indicates the version of SAE Lens used for training, set to None here.\n",
    "19. `activation_fn_kwargs`: Allows for additional keyword arguments for the activation function. This would be used if e.g. the `activation_fn_str` was set to `topk`, so that `k` could be specified.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    tabulate(\n",
    "        gpt2_sae.cfg.__dict__.items(),\n",
    "        headers=[\"name\", \"value\"],\n",
    "        tablefmt=\"simple_outline\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Let's look at some examples for other SAEs (especially different architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_sae_topk_open_ai, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-resid-post-v5-32k\",\n",
    "    sae_id=\"blocks.0.hook_resid_post\",\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "print(\n",
    "    tabulate(\n",
    "        gpt2_sae_topk_open_ai.cfg.__dict__.items(),\n",
    "        headers=[\"name\", \"value\"],\n",
    "        tablefmt=\"simple_outline\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_scope_16k_sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "    sae_id=\"layer_0/width_16k/canonical\",\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "print(\n",
    "    tabulate(\n",
    "        gemma_scope_16k_sae.cfg.__dict__.items(),\n",
    "        headers=[\"name\", \"value\"],\n",
    "        tablefmt=\"simple_outline\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Running SAEs with SAE Lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core details:\n",
    "- The suggested way to run SAEs is with HookedSAETransformer.\n",
    "- You can render dashboards from Neuronpedia in your notebook and use our servers to test activations.\n",
    "\n",
    "Key Links:\n",
    "- ARENA Exercises: https://colab.research.google.com/drive/1ePkM8oBHIEZ2kcqAiA3waeAmz8RSdHmq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import utils\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "utils.test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Test our prompt, to see what the model says\n",
    "with gpt2.saes(saes=[gpt2_sae]):\n",
    "    utils.test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Same thing, done in a different way\n",
    "gpt2.add_sae(gpt2_sae)\n",
    "utils.test_prompt(prompt, answer, gpt2)\n",
    "gpt2.reset_saes()  # Remember to always do this!\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = gpt2(prompt, return_type=\"logits\")\n",
    "logits_sae = gpt2.run_with_saes(prompt, saes=[gpt2_sae], return_type=\"logits\")\n",
    "answer_token_id = gpt2.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {gpt2.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {gpt2.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = gpt2.run_with_cache_with_saes(prompt, saes=[gpt2_sae])\n",
    "\n",
    "for name, param in cache.items():\n",
    "    if \"hook_sae\" in name:\n",
    "        print(f\"{name:<43}: {tuple(param.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from IPython.display import IFrame\n",
    "\n",
    "def display_dashboard(\n",
    "    sae_release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    feature_idx=0,\n",
    "    width=800,\n",
    "    height=600,\n",
    "):\n",
    "    release = get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = f\"https://neuronpedia.org/{neuronpedia_id}/{feature_idx}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "    display(IFrame(url, width=width, height=height))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top activations on final token\n",
    "_, cache = gpt2.run_with_cache_with_saes(\n",
    "    prompt,\n",
    "    saes=[gpt2_sae],\n",
    "    stop_at_layer=gpt2_sae.cfg.hook_layer + 1,\n",
    ")\n",
    "sae_acts_post = cache[f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\"][0, -1, :]\n",
    "\n",
    "# Plot line chart of feature activations\n",
    "px.line(\n",
    "    sae_acts_post.cpu().numpy(),\n",
    "    title=f\"Feature activations at the final token position ({sae_acts_post.nonzero().numel()} alive)\",\n",
    "    labels={\"index\": \"Feature\", \"value\": \"Activation\"},\n",
    "    width=1000,\n",
    ").update_layout(showlegend=False).show()\n",
    "\n",
    "# Print the top 5 features, and inspect their dashboards\n",
    "for act, ind in zip(*sae_acts_post.topk(3)):\n",
    "    print(f\"Feature {ind} had activation {act:.2f}\")\n",
    "    display_dashboard(feature_idx=ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Feature Dashboards and Neuronpedia (see slides)\n",
    "\n",
    "(Might leave this challenges for after the session)\n",
    "\n",
    "Latent Search Challenges:\n",
    "1. Can you find a pokemon related latent using search via explanations? How about via inference?\n",
    "2. Can you find a latent fires between open brackets via explanations? How about via inference?\n",
    "3. Can you find a latent that fires when the next token is likely to start with the letter P? \n",
    "4. Extra hard: In GemmaScope Res 16k layer 20, can you find the \"cringe\" feature? \n",
    "\n",
    "Dashboard Quality Challenges:\n",
    "1. Can you find a dense feature? (firing > 2% of the time). What might it be tracking?\n",
    "2. Can you find an example of a bad latent explanation. Why is it bad? \n",
    "\n",
    "Redteaming Challenges:\n",
    "1. Using the feature absorption app, can you demonstrate examples of feature absorption? Replicate this result just on Neuronpedia.\n",
    "2. (hard)Using the [meta-SAEs app](https://metasae.streamlit.app/?page=Meta+Feature+Explorer&meta_feature=504) can you come up with any more features which might be subject to absorption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training SAEs (Almost exactly the ARENA Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SAELens` makes training your SAEs relatively straightforward (if you know what the config does!). The code for training is essentially:\n",
    "\n",
    "```python\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "runner_cfg = LanguageModelSAERunnerConfig(...)\n",
    "runner = SAETrainingRunner(runner_cfg)\n",
    "sae = runner.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LanguageModelSAERunnerConfig` class contains all the parameters necessary to specify how your SAE gets trained. This includes many of the parameters that went into the SAE config (in fact, this class contains a method `get_base_sae_cfg_dict()` which returns a dictionary that can be used to create the associated `SAEConfig` object). However, it also includes many other arguments which are specific to the training process itself. You can see the full set of config parameters in the source code for `LanguageModelSAERunnerConfig`, however for our purposes we can group them into ~7 main categories:\n",
    "\n",
    "1. **Data generation** - everything to do with how the data we use to train the SAE is generated & batched. Recall that your SAEs are trained on the activations of a base TransformerLens model, so this includes things like the model name (which should point to a model [supported by TransformerLens](https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)), hook point in the model, dataset path (which should reference a HuggingFace dataset), etc. We've also included `d_in` in this category since it's uniquely determined by the hook point you're training on.\n",
    "2. **SAE architecture** - everything to do with the SAE architecture, which isn't implied by the data generation parameters. This includes things like `d_sae` (which you are allowed to use, although in practice we often specify `expansion_factor` and then `d_sae` is defined as `d_in * expansion_factor`), which activation function to use, how our weights are initialized, whether we subtract the decoder bias `b_dec` from the input, etc.\n",
    "3. **Activations store** - everything to do with how the activations are stored and processed during training. Recall we used the `ActivationsStore` class earlier when we were generating the feature dashboards for our model. During training we also use an instance of this class to store and process batches of activations to feed into our SAE. We need to specify things like how many batches we want to store, how many prompts we want to process at once, etc.\n",
    "4. **Training hyperparameters (standard)** - these are the standard kinds of parameters you'd expect to see in an other ML training loop: learning rate & learning rate scheduling, betas for Adam optimizer, etc.\n",
    "5. **Training hyperparameters (SAE-specific)** - these are all the parameters which are specific to your SAE training. This means various coefficients like the $L_1$ penalty (as well as warmups for these coefficients), as well as things like the resampling protocol - more on this later. Certain other architectures (e.g. gated) might also come with additional parameters that need specifying.\n",
    "6. **Logging / evals** - these control how frequently we log data to Weights & Biases, as well as how often we perform evaluations on our SAE (more on evals in the second half of this exercise set!). Remember that when we talk about evals during training, we're often talking about different kinds of evals than we perform post-training to compare different SAEs to each other (although there's certainly a lot of overlap).\n",
    "7. **Misc** - this is a catchall for anything else you might want to specify, e.g. how often you save your model checkpoints, random seeds, device & dtype, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-GPU Training Runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inherit multi-GPU support from transformer_lens, so we pass arguments to the `model_from_pretrained_kwargs` to set `n_devices > 1`. \n",
    "\n",
    "```python\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "runner_cfg = LanguageModelSAERunnerConfig(\n",
    "    ..., \n",
    "    model_from_pretrained_kwargs = {\"n_devices\": torch.cuda.device_count() - 1}\n",
    "    )\n",
    "runner = SAETrainingRunner(runner_cfg)\n",
    "sae = runner.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-generating your activations (enable reuse and possibly better shuffling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll be able to reuse activations (eg: when hyperparameter tuning). It's worth\n",
    "\n",
    "```python\n",
    "from sae_lens.cache_activations_runner import CacheActivationsRunner\n",
    "from sae_lens.config import CacheActivationsRunnerConfig\n",
    "\n",
    "path =\"whatever/you/want\"\n",
    "cfg = CacheActivationsRunnerConfig(new_cached_activations_path=path, ...)\n",
    "CacheActivationsRunner(cfg).run()\n",
    "\n",
    "runner_cfg = LanguageModelSAERunnerConfig(\n",
    "    ..., \n",
    "    use_cached_activations=True,\n",
    "    cached_activations_path=path\n",
    ")\n",
    "runner = SAETrainingRunner(runner_cfg)\n",
    "sae = runner.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging, checkpointing & saving SAEs\n",
    "\n",
    "For any real training run, you should be **logging to Weights and Biases (WandB)**. This will allow you to track your training progress and compare different runs. To enable WandB, set `log_to_wandb=True`. The `wandb_project` parameter in the config controls the project name in WandB. You can also control the logging frequency with `wandb_log_frequency` and `eval_every_n_wandb_logs`. A number of helpful metrics are logged to WandB, including the sparsity of the SAE, the mean squared error (MSE) of the SAE, dead features, and explained variance. These metrics can be used to monitor the training progress and adjust the training parameters. We'll discuss these metrics more in later sections.\n",
    "\n",
    "**Checkpoints** allow you to save a snapshot of the SAE and sparsitity statistics during training. To enable checkpointing, set `n_checkpoints` to a value larger than 0. If WandB logging is enabled, checkpoints will be uploaded as WandB artifacts. To save checkpoints locally, the `checkpoint_path` parameter can be set to a local directory.\n",
    "\n",
    "Once you have a set of SAEs that you're happy with, your next step is to share them with the world! SAELens has a `upload_saes_to_huggingface()` function which makes this easy to do. You'll need to upload a dictionary where the keys are SAE ids, and the values are either `SAE` objects or paths to an SAE that's been saved using the `sae.save_model()` method (you can use a combination of both in your dictionary). Note that you'll need to be logged in to your huggingface account either by running `huggingface-cli login` in the terminal or by setting the `HF_TOKEN` environment variable to your API token (which should have write access to your repo).\n",
    "\n",
    "```python\n",
    "from sae_lens import SAE, upload_saes_to_huggingface\n",
    "\n",
    "# Create a dictionary of SAEs: this will be a single release; keys are the SAE ids\n",
    "saes_dict = {\n",
    "    \"blocks.0.hook_resid_pre\": layer_0_sae,\n",
    "    \"blocks.1.hook_resid_pre\": layer_1_sae,\n",
    "}\n",
    "\n",
    "# Upload the SAEs to HuggingFace, in your chosen repository\n",
    "upload_saes_to_huggingface(\n",
    "    saes_dict,\n",
    "    hf_repo_id=\"your-username/your-sae-repo\",\n",
    ")\n",
    "\n",
    "# Load them back in, using `from_pretrained()`\n",
    "uploaded_saes = {\n",
    "    layer: SAE.from_pretrained(\n",
    "        release=\"your-username/your-sae-repo\",\n",
    "        sae_id=f\"blocks.{layer}.hook_resid_pre\",\n",
    "        device=str(device)\n",
    "    )[0]\n",
    "    for layer in [0, 1]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "\n",
    "#### Reconstruction vs sparsity metrics\n",
    "\n",
    "As we've discussed, most metrics you log to WandB are attempts to measure either reconstruction loss or sparsity, in various different ways. The goal is to monitor how these two different objectives are being balanced, and hopefully find pareto-improvements for them! For reconstruction loss, you want to pay particular attention to **MSE loss**, **CE loss recovered** and **explained variance**. For sparsity, you want to look at the L0 and L1 statistics, as well as the activations histogram (more on that below, since it's more nuanced than \"brr line go up/down\"!).\n",
    "\n",
    "The L1 coefficient is the primary lever you have for managing the tradeoff between accurate reconstruction and sparsity. Too high and you get lots of dead features (although this is mediated by L1 warmup if using a scheduler - see below), too low and your features will be dense and polysemantic rather than sparse and interpretable.\n",
    "\n",
    "Another really important point when considering the tradeoff between these two metrics - it might be tempting to use a very high L1 coefficient to get nice sparse interpretable features, but if this comes at a cost of high reconstruction loss, then there's a real risk that **you're not actually learning the model's true behaviour.** SAEs are only valuable when they give us a true insight into what the model's representations actually are, and doing interpretability without this risks being just a waste of time. See discussion [here](https://www.lesswrong.com/posts/tEPHGZAb63dfq2v8n/how-useful-is-mechanistic-interpretability) between Neel Nanda, Ryan Greenblat & Buck Schlegris for more on this point (note that I don't agree with all the points made in this post, but it raises some highly valuable ideas that SAE researchers would do well to keep in mind).\n",
    "\n",
    "#### ...but metrics can sometimes be misleading\n",
    "\n",
    "Although metrics in one of these two groups will often tell a similar story (e.g. explained variance will usually be high when MSE loss is small), they can occasionally detach, and it's important to understand why this might happen. Some examples:\n",
    "\n",
    "- L0 and L1 both tell you about sparsity, but feature shrinkage makes them detach (it causes smaller L1, not L0)\n",
    "- MSE loss and KL div / downstream CE both tell you about reconstruction, but they detach because one is myopic and the other is not\n",
    "\n",
    "As well as being useful to understand these specific examples, it's valuable to put yourself in a skeptical mindset, and understand why these kinds of problems can arise. New metrics are being developed all the time, and some of them might be improvements over current ones while others might carry entirely new and unforseen pitfalls!\n",
    "\n",
    "#### Dead features, resampling & ghost gradients\n",
    "\n",
    "**Dead features** are ones that never fire on any input. These can be a big problem during training, because they don't receive any gradients and so represent permanently lost capacity in your SAE. Two ways of dealing with dead features, which Anthropic have described in various papers and update posts:\n",
    "\n",
    "1. **Ghost gradients** - this describes the method of adding an additional term to the loss, which essentially gives dead latents a gradient signal that pushes them in the direction of explaining more of the autoencoder's residual. Full technical details [here](https://transformer-circuits.pub/2024/jan-update/index.html#dict-learning-resampling).\n",
    "2. **Resampling** - at various timesteps we take all our dead features and randomly re-initialize them to values which help explain more of the residual (specifically, we will randomly select inputs which the SAE fails to reconstruct, and then set dead featurs to be the SAE hidden states corresponding to those inputs).\n",
    "\n",
    "These techniques are both useful, however they've been reassessed in the time after they were initially introduced, and are now not seen to be as critical as they once were (especially [ghost grads](https://transformer-circuits.pub/2024/march-update/index.html#dl-update)). Instead, we use more standard techniques to avoid dead features, specifically a combination of an appropriately small learning rate and an L1 warmup. We generally recommend people use resampling and not ghost gradients, but to take enough care with your LR and L1 warmup to avoid having to rely on resampling.\n",
    "\n",
    "Assuming we're not using ghost gradients, resampling is controlled by the following 3 parameters:\n",
    "\n",
    "- `feature_sampling_window`, which is how often we resample neurons\n",
    "- `dead_feature_window`, which is the size of the window over which we count dead features each time we resample. This should be smaller than `feature_sampling_window`\n",
    "- `dead_feature_threshold`, which is the threshold below which we consider a feature to be dead & resample it\n",
    "\n",
    "#### Dense latents & learning rates\n",
    "\n",
    "Dense latents are the opposite problem to dead latents: if your learning rate is too small or L1 penalty too small, you'll fail to train latents to the point of being sparse. A dense latent is one that fires too frequently (e.g. on >1/100 or even >1/10 tokens). These features seem generally uninterpretable, but can help with youre reconstruction loss immensely - essentially it's a way for the SAEs to smuggle not-particularly-sparse possibly-nonlinear computation into your SAE.\n",
    "\n",
    "It can be difficult to balance dense and dead latents during training. Generally you want to drop your learning rate as far down as it will go, without causing your features to be dense and your training to be super slow.\n",
    "\n",
    "Note - what (if any) the right number of dense or dead latents should be in any situation is very much an open question, and depends on your beliefs about the underlying feature distribution in question. One way we can investigate this question is to try and train SAEs in a simpler domain, where the underlying features are easier to guess about (e.g. OthelloGPT, or TinyStories - both of which we'll discuss later in this chapter).\n",
    "\n",
    "#### Interpreting the feature density histogram\n",
    "\n",
    "This section is quoted directly from Joseph Bloom's [excellent post](https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream#Why_can_training_Sparse_AutoEncoders_be_difficult__) on training SAEs:\n",
    "\n",
    "> Feature density histograms are a good measure of SAE quality. We plot the log10 feature sparsity (how often it fires) for all features. In order to make this easier to operationalize, Iâ€™ve drawn a diagram that captures my sense of the issues these histograms help you diagnose. Feature density histograms can be broken down into:\n",
    "> - **Too Dense**:  dense features will occur at a frequency > 1 / 100. Some dense-ish features are likely fine (such as a feature representing that a token begins with a space) but too many is likely an issue.\n",
    "> - **Too Sparse**: Dead features wonâ€™t be sampled so will turn up at log10(epsilon), for epsilon added to avoid logging 0 numbers. Too many of these mean youâ€™re over penalizing with L1.\n",
    "> - **Just-Right**: Without too many dead or dense features, we see a distribution that has most mass between -5 or -4 and -3 log10 feature sparsity. The exact range can vary depending on the model / SAE size but the dense or dead features tend to stick out.\n",
    ">\n",
    "> <img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/f9EgfLSurAiqRJySD/loz4zhrj3ps0cue7uike\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Case Study: TinyStories-1L, MLP-out\n",
    "\n",
    "In our first training case study, we'll train an SAE on the output of the final (only) MLP layer of a TinyStories model. [TinyStories](https://arxiv.org/abs/2305.07759) is a synthetic dataset consisting of short stories, which contains a vocabulary of ~1500 words (mostly just common words that typical 3-4 year old children can understand). Each story is also relatively short, self-contained, and contains a basic sequence of events which can often be causally inferred from the previous context. Example sequences look like:\n",
    "\n",
    "> Once upon a time, there was a little girl named Lily. Lily liked to pretend she was a popular princess. She lived in a big castle with her best friends, a cat and a dog. One day, while playing in the castle, Lily found a big cobweb. The cobweb was in the way of her fun game. She wanted to get rid of it, but she was scared of the spider that lived there. Lily asked her friends, the cat and the dog, to help her. They all worked together to clean the cobweb. The spider was sad, but it found a new home outside. Lily, the cat, and the dog were happy they could play without the cobweb in the way. And they all lived happily ever after.\n",
    "\n",
    "This dataset gives us a useful playground for interpretability analysis, because the kinds of features which it is useful for models to learn in order to minimize predictive loss on this dataset are far narrower and simpler than they would be for models trained on more complex natural language datasets.\n",
    "\n",
    "Let's load in the model we'll be training our SAE on, and get a sense for how models trained on this dataset behave by generating text from it. This is a useful first step when it comes to thinking about what features the model is likely to have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinystories_model = HookedSAETransformer.from_pretrained(\"tiny-stories-1L-21M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = [\n",
    "    (\n",
    "        i,\n",
    "        tinystories_model.generate(\"Once upon a time\", temperature=1, max_new_tokens=50),\n",
    "    )\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "print(tabulate(completions, tablefmt=\"simple_grid\", maxcolwidths=[None, 100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to train out SAE. We'll make a runner config, instantiate the runner and the rest is taken care of for us!\n",
    "\n",
    "During training, you use weights and biases to check key metrics which indicate how well we are able to optimize the variables we care about. You can reorganize your WandB dashboard to put important metrics like L0, CE loss score, explained variance etc in one section at the top. We also recommend you make a [run comparer](https://docs.wandb.ai/guides/app/features/panels/run-comparer/) for your different runs, whenever performing multiple training runs (e.g. hyperparameter sweeps).\n",
    "\n",
    "If you've disabled gradients, remember to re-enable them using `t.set_grad_enabled(True)` before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE, LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "total_training_steps = 30_000  # probably we should do more\n",
    "batch_size = 4096\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = l1_warm_up_steps = total_training_steps // 10  # 10% of training\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    #\n",
    "    # Data generation\n",
    "    model_name=\"tiny-stories-1L-21M\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "    hook_name=\"blocks.0.hook_mlp_out\",\n",
    "    hook_layer=0,\n",
    "    d_in=tinystories_model.cfg.d_model,\n",
    "    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",  # tokenized language dataset on HF for the Tiny Stories corpus.\n",
    "    is_dataset_tokenized=True,\n",
    "    prepend_bos=True,  # you should use whatever the base model was trained with\n",
    "    streaming=True,  # we could pre-download the token dataset if it was small.\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    context_size=512,  # larger is better but takes longer (for tutorial we'll use a short one)\n",
    "    #\n",
    "    # SAE architecture\n",
    "    architecture=\"gated\",\n",
    "    expansion_factor=16,\n",
    "    b_dec_init_method=\"zeros\",\n",
    "    apply_b_dec_to_input=True,\n",
    "    normalize_sae_decoder=False,\n",
    "    scale_sparsity_penalty_by_decoder_norm=True,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    #\n",
    "    # Activations store\n",
    "    n_batches_in_buffer=64,\n",
    "    training_tokens=total_training_tokens,\n",
    "    store_batch_size_prompts=16,\n",
    "    #\n",
    "    # Training hyperparameters (standard)\n",
    "    lr=5e-5,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",  # controls how the LR warmup / decay works\n",
    "    lr_warm_up_steps=lr_warm_up_steps,  # avoids large number of initial dead features\n",
    "    lr_decay_steps=lr_decay_steps,  # helps avoid overfitting\n",
    "    #\n",
    "    # Training hyperparameters (SAE-specific)\n",
    "    l1_coefficient=4,\n",
    "    l1_warm_up_steps=l1_warm_up_steps,\n",
    "    use_ghost_grads=False,  # we don't use ghost grads anymore\n",
    "    feature_sampling_window=2000,  # how often we resample dead features\n",
    "    dead_feature_window=1000,  # size of window to assess whether a feature is dead\n",
    "    dead_feature_threshold=1e-4,  # threshold for classifying feature as dead, over window\n",
    "    #\n",
    "    # Logging / evals\n",
    "    log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"arena-demos-tinystories\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "    #\n",
    "    # Misc.\n",
    "    device=str(device),\n",
    "    seed=42,\n",
    "    n_checkpoints=5,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "# # Comment this code out to train! Otherwise, it will load in the already trained model.\n",
    "# torch.set_grad_enabled(True)\n",
    "# runner = SAETrainingRunner(cfg)\n",
    "# sae = runner.run()\n",
    "\n",
    "hf_repo_id = \"callummcdougall/arena-demos-tinystories\"\n",
    "sae_id = cfg.hook_name\n",
    "\n",
    "# upload_saes_to_huggingface({sae_id: sae}, hf_repo_id=hf_repo_id)\n",
    "\n",
    "tinystories_sae = SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've finished training your SAE, you can try using the following code from the `sae_vis` library to visualize your SAE's latents.\n",
    "\n",
    "(Note - this code comes from a branch of the `sae_vis` library, which soon will be merged into main, and will also be more closely integrated with the rest of `SAELens`. For example, this method currently works by directly taking a batch of tokens, but in the future it will probably take an `ActivationsStore` object to make things easier.)\n",
    "\n",
    "First, we get a batch of tokens from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\"\n",
    "total_batch_size = 1024\n",
    "\n",
    "dataset = load_dataset(dataset_path, streaming=True)\n",
    "\n",
    "tokens = torch.tensor(\n",
    "    [x[\"input_ids\"] for i, x in zip(range(total_batch_size), dataset[\"train\"])],\n",
    "    device=str(device),\n",
    ")\n",
    "print(tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the visualization and save it (you'll need to download the file and open it in a browser to view). Note that if you get OOM errors then you can reduce the number of features visualized, or decrease either the batch size or context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_vis import SaeVisConfig, SaeVisData\n",
    "\n",
    "sae_vis_data = SaeVisData.create(\n",
    "    sae=tinystories_sae,\n",
    "    model=tinystories_model,\n",
    "    tokens=tokens,\n",
    "    cfg=SaeVisConfig(hook_point=tinystories_sae.cfg.hook_name, features=range(16)),\n",
    "    verbose=True,\n",
    ")\n",
    "sae_vis_data.save_feature_centric_vis(filename=\"feature_vis.html\")\n",
    "\n",
    "# display_vis_inline(str(section_dir / \"feature_vis.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - identify good and bad training curves\n",
    "\n",
    "```c\n",
    "Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª\n",
    "Importance: ðŸ”µðŸ”µðŸ”µðŸ”µðŸ”µ\n",
    "\n",
    "You should spend up to 25-40 minutes on this exercise.\n",
    "```\n",
    "\n",
    "[Here](https://wandb.ai/callum-mcdougall/arena-demos-tinystories-v3/workspace?nw=nwusercallummcdougall) is a link to a WandB project page with seven training runs. The first one (Run #0) is a \"good\" training run (at least compared to the others), and can be thought of as a baseline. Each of the other 6 runs (labelled Run #1 - Run #6) has some particular issue with it. Your task will be to identify the issue (i.e. from one or more of the metrics plots), and find the root cause of the issue from looking at the configs (you can compare the configs to each other in the \"runs\" tab). Note that we recommend trying to identify the issue from the plots first, rather than immediately jumping to the config and looking for a diff between it and the good run. You'll get most value out of the exercises by using the following pattern when assessing each run:\n",
    "\n",
    "1. Looking at the metrics, and finding some which seem like indications of poor SAE quality\n",
    "2. Based on these metrics, try and guess what might be going wrong in the config\n",
    "3. Look at the config, and test whether your guess was correct.\n",
    "\n",
    "Also, a reminder - you can look at a run's density histogram plot, although you can only see this when you're looking at the page for a single run (as opposed to the project page).\n",
    "\n",
    "Use the dropdowns below to see the answer for each of the runs. Note, the first few are more important to get right, as the last few are more difficult and don't always have obvious root causes.\n",
    "\n",
    "<details>\n",
    "<summary>Run #1</summary>\n",
    "\n",
    "This run had too small an L1 coefficient, meaning it wasn't learning a sparse solution. The tipoff here should have been the feature sparsity statistics, e.g. L0 being extremely high. Exactly what L0 is ideal varies between different models and hook points (and also depends on things like the SAE width - see the section on feature splitting earlier for more on this), but as an idea, the canonical GemmaScope SAEs were chosen to be those with L0 closest to 100. Much larger than this (e.g. 200+) is almost definitely bad, especially if we're talking about what should fundamentally be a pretty simple dataset (TinyStories, with a 1L model).\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Run #2</summary>\n",
    "\n",
    "This run had far too many dead latents. This was as a result of choosing an unnecessarily large expansion factor: 32, rather than an expansion factor of 16 as is used in the baseline run. Note that having a larger expansion factor & dead features isn't necessarily bad, but it does imply a lot of wasted capacity. The fact that resampling was seemingly unable to reduce the number of dead latents is a sign that our `d_sae` was larger than it needed to be.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Run #3</summary>\n",
    "\n",
    "This run had a very low learning rate: `1e-5` vs the baseline value of `5e-5`. Having a learning rate this low isn't inherently bad if you train for longer (and in fact a smaller learning rate and longer training duration is generally better if you have the time for it), but given the same number of training tokens a smaller learning rate can result in poorer end-of-training performance. In this case, we can see that most loss curves are still dropping when the training finishes, suggesting that this model was undertrained.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Run #4</summary>\n",
    "\n",
    "This run had an expansion factor of 1, meaning that the number of learned features couldn't be larger than the dimensionality of the MLP output. This is obviously bad, and will lead to the poor performance seen in the loss curves (both in terms of sparsity and reconstruction loss).\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Run #5</summary>\n",
    "\n",
    "This run had a large number of dead features. Unlike run #2, the cause wasn't an unnecessarily large expansion factor, instead it was a combination of:\n",
    "\n",
    "- High learning rate\n",
    "- No warmup steps\n",
    "- No feature resampling\n",
    "\n",
    "So unlike run #2, there wasn't also a large number of live features, meaning performance was much poorer.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Run #6 (hint)</summary>\n",
    "\n",
    "The failure mode here is of a different kind than the other five. Try looking at the plot `metrics/ce_loss_without_sae` - what does this tell you? (You can look at the SAELens source code to figure out what this metric means).\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Run #6</summary>\n",
    "\n",
    "The failure mode for this run is different from the other runs in this section. The SAE was trained perfectly fine, but it was trained on activations generated from the wrong input distribution! The dataset from which we generated our model activations was `apollo-research/monology-pile-uncopyrighted-tokenizer-gpt2` - this dataset was designed for a model like GPT2, and not for the tinystories model we're using.\n",
    "\n",
    "The tipoff here could have come from a few different plots, but in particular the `\"metrics/ce_loss_without_sae\"` plot - we can see that the model performs much worse (without the SAE even being involved) than it did for any of the other runs in the project. This metrics plot is a useful sanity check to make sure your model is being fed appropriate data!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Evaluating SAEs (interactive session, unfinished but we can slow down here and do it together)\n",
    "\n",
    "Together let's:\n",
    "- Start with a code that loops over model activations and generates feature activations.\n",
    "- Add code to count feature activations / distinct prompts in which features activate.\n",
    "- Plot a histogram of feature density.\n",
    "- Plot Feature Density vs Consistent Activation Heuristic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-use the code I wrote to reproduce Josh's non-linear features results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sae_lens import HookedSAETransformer, SAE, ActivationsStore\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# gpt2\n",
    "# model_name = \"gpt2-small\"\n",
    "# release = \"gpt2-small-res-jb\"\n",
    "# sae_id = \"blocks.7.hook_resid_pre\"\n",
    "\n",
    "\n",
    "# tinystories\n",
    "model_name = \"tiny-stories-1L-21M\"\n",
    "release = \"callummcdougall/arena-demos-tinystories\"\n",
    "sae_id = \"blocks.0.hook_mlp_out\"\n",
    "\n",
    "\n",
    "\n",
    "model: HookedSAETransformer = HookedSAETransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=release,\n",
    "    sae_id=sae_id,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Example of how you can use this:\n",
    "tokens = activation_store.get_batch_tokens()\n",
    "assert tokens.shape == (activation_store.store_batch_size_prompts, activation_store.context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "day_of_the_week_features = [2592, 4445, 4663, 4733, 6531, 8179, 9566, 20927, 24185]\n",
    "# months_of_the_year = [3977, 4140, 5993, 7299, 9104, 9401, 10449, 11196, 12661, 14715, 17068, 17528, 19589, 21033, 22043, 23304]\n",
    "# years_of_20th_century = [1052, 2753, 4427, 6382, 8314, 9576, 9606, 13551, 19734, 20349]\n",
    "\n",
    "days_of_the_week = [\n",
    "    \"Monday\",\n",
    "    \"Tuesday\",\n",
    "    \"Wednesday\",\n",
    "    \"Thursday\",\n",
    "    \"Friday\",\n",
    "    \"Saturday\",\n",
    "    \"Sunday\",\n",
    "]\n",
    "buffer = 5\n",
    "seq_len = activation_store.context_size\n",
    "sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "\n",
    "all_data = {\"recons\": [], \"context\": [], \"token\": [], \"token_group\": []}\n",
    "total_batches = 500\n",
    "\n",
    "for i in tqdm(range(total_batches)):\n",
    "    _, cache = model.run_with_cache_with_saes(\n",
    "        tokens := activation_store.get_batch_tokens(),\n",
    "        saes=[sae],\n",
    "        stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "        names_filter=[sae_acts_post_hook_name],\n",
    "    )\n",
    "    acts = cache[sae_acts_post_hook_name][..., day_of_the_week_features].flatten(0, 1)\n",
    "\n",
    "    any_feature_fired = (acts > 0).any(dim=1)\n",
    "    acts = acts[any_feature_fired]\n",
    "    reconstructions = acts @ gpt2_sae.W_dec[day_of_the_week_features]\n",
    "\n",
    "    all_data[\"recons\"].append(reconstructions)\n",
    "\n",
    "    for batch_seq_flat_idx in torch.nonzero(any_feature_fired).squeeze(-1).tolist():\n",
    "        batch, seq = divmod(batch_seq_flat_idx, seq_len)  # type: ignore\n",
    "\n",
    "        token = gpt2.tokenizer.decode(tokens[batch, seq])  # type: ignore\n",
    "        token_group = token.strip() if token.strip() in days_of_the_week else \"Other\"\n",
    "\n",
    "        context = gpt2.tokenizer.decode(  # type: ignore\n",
    "            tokens[batch, max(seq - buffer, 0) : min(seq + buffer + 1, seq_len)]\n",
    "        )\n",
    "\n",
    "        all_data[\"context\"].append(context)\n",
    "        all_data[\"token\"].append(token)\n",
    "        all_data[\"token_group\"].append(token_group)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_embedding = pca.fit_transform(torch.concat(all_data.pop(\"recons\")).detach().cpu().numpy())\n",
    "\n",
    "all_data |= {\"PC2\": pca_embedding[:, 1], \"PC3\": pca_embedding[:, 2]}\n",
    "pca_df = pd.DataFrame(all_data)\n",
    "\n",
    "px.scatter(\n",
    "    pca_df,\n",
    "    x=\"PC2\",\n",
    "    y=\"PC3\",\n",
    "    hover_data=[\"context\"],\n",
    "    hover_name=\"token\",\n",
    "    height=700,\n",
    "    width=1000,\n",
    "    color=\"token_group\",\n",
    "    color_discrete_sequence=px.colors.sample_colorscale(\"Viridis\", 7) + [\"#aaa\"],\n",
    "    title=\"PCA Subspace Reconstructions\",\n",
    "    labels={\"token_group\": \"Activating token\"},\n",
    "    category_orders={\"token_group\": days_of_the_week + [\"Other\"]},\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Analysing SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to use Callum's TinyStories SAE (maybe we'll generate CAH scores and use it to find good steering features)\n",
    "model = HookedSAETransformer.from_pretrained(\"tiny-stories-1L-21M\")\n",
    "hf_repo_id = \"callummcdougall/arena-demos-tinystories\"\n",
    "sae_id = cfg.hook_name\n",
    "sae = SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=str(device),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, if we want to use my GPT2 SAE (maybe we'll generate CAH scores and use it to find good steering features)\n",
    "# Link to Neuronpedia page with CAH stats: https://www.neuronpedia.org/gpt2-small/7-res-jb\n",
    "model = HookedSAETransformer.from_pretrained(\"gpt2-small\")\n",
    "hf_repo_id = \"gpt2-small-res-jb\"\n",
    "sae_id = \"blocks.7.hook_resid_pre\"\n",
    "sae = SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "\n",
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=str(device),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steering / Clamping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from functools import partial \n",
    "\n",
    "def find_max_activation(model, sae, activation_store, feature_idx, num_batches=100):\n",
    "    '''\n",
    "    Find the maximum activation for a given feature index. This is useful for \n",
    "    calibrating the right amount of the feature to add.\n",
    "    '''\n",
    "    max_activation = 0.0\n",
    "\n",
    "    pbar = tqdm(range(num_batches))\n",
    "    for _ in pbar:\n",
    "        tokens = activation_store.get_batch_tokens()\n",
    "        \n",
    "        _, cache = model.run_with_cache(\n",
    "            tokens, \n",
    "            stop_at_layer=sae.cfg.hook_layer + 1, \n",
    "            names_filter=[sae.cfg.hook_name]\n",
    "        )\n",
    "        sae_in = cache[sae.cfg.hook_name]\n",
    "        feature_acts = sae.encode(sae_in).squeeze()\n",
    "\n",
    "        feature_acts = feature_acts.flatten(0, 1)\n",
    "        batch_max_activation = feature_acts[:, feature_idx].max().item()\n",
    "        max_activation = max(max_activation, batch_max_activation)\n",
    "        \n",
    "        pbar.set_description(f\"Max activation: {max_activation:.4f}\")\n",
    "\n",
    "    return max_activation\n",
    "\n",
    "def steering(activations, hook, steering_strength=1.0, steering_vector=None, max_act=1.0):\n",
    "    # Note if the feature fires anyway, we'd be adding to that here.\n",
    "    return activations + max_act * steering_strength * steering_vector\n",
    "\n",
    "def generate_with_steering(model, sae, prompt, steering_feature, max_act, steering_strength=1.0, max_new_tokens=95):\n",
    "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "    \n",
    "    steering_vector = sae.W_dec[steering_feature].to(model.cfg.device)\n",
    "    \n",
    "    steering_hook = partial(\n",
    "        steering,\n",
    "        steering_vector=steering_vector,\n",
    "        steering_strength=steering_strength,\n",
    "        max_act=max_act\n",
    "    )\n",
    "    \n",
    "    # standard transformerlens syntax for a hook context for generation\n",
    "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, steering_hook)]):\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos = False if device == \"mps\" else True,\n",
    "            prepend_bos = sae.cfg.prepend_bos,\n",
    "        )\n",
    "    \n",
    "    return model.tokenizer.decode(output[0])\n",
    "\n",
    "# Choose a feature to steer\n",
    "steering_feature = 100  # Choose a feature to steer towards\n",
    "\n",
    "# Find the maximum activation for this feature\n",
    "max_act = find_max_activation(model, sae, activation_store, steering_feature)\n",
    "print(f\"Maximum activation for feature {steering_feature}: {max_act:.4f}\")\n",
    "\n",
    "# note we could also get the max activation from Neuronpedia (https://www.neuronpedia.org/api-doc#tag/lookup/GET/api/feature/{modelId}/{layer}/{index})\n",
    "\n",
    "# Generate text without steering for comparison\n",
    "prompt = \"Once upon a time\"\n",
    "normal_text = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=95, \n",
    "    stop_at_eos = False if device == \"mps\" else True,\n",
    "    prepend_bos = sae.cfg.prepend_bos,\n",
    ")\n",
    "\n",
    "print(\"\\nNormal text (without steering):\")\n",
    "print(normal_text)\n",
    "\n",
    "# Generate text with steering\n",
    "steered_text = generate_with_steering(model, sae, prompt, steering_feature, max_act, steering_strength=2.0)\n",
    "print(\"Steered text:\")\n",
    "print(steered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do steering via the Neuronpedia interface / API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "url = \"https://www.neuronpedia.org/api/steer\"\n",
    "\n",
    "payload = {\n",
    "    # \"prompt\": \"A knight in shining\",\n",
    "    # \"prompt\": \"He had to fight back in self-\", \n",
    "    \"prompt\": \"In the middle of the universe is the galactic\",\n",
    "    # \"prompt\": \"Oh no. We're running on empty. Its time to fill up the car with\",\n",
    "    # \"prompt\": \"Sure, I'm happy to pay. I don't have any cash on me but let me write you a\",\n",
    "    \"modelId\": \"gpt2-small\",\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"modelId\": \"gpt2-small\",\n",
    "            \"layer\": \"7-res-jb\",\n",
    "            \"index\": 6770,\n",
    "            \"strength\": 8\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0.2,\n",
    "    \"n_tokens\": 2,\n",
    "    \"freq_penalty\": 1,\n",
    "    \"seed\": np.random.randint(100),\n",
    "    \"strength_multiplier\": 4\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "url = \"https://www.neuronpedia.org/api/steer\"\n",
    "\n",
    "payload = {\n",
    "    \"prompt\": \"I wrote a letter to my girlfiend. It said \\\"\",\n",
    "    \"modelId\": \"gpt2-small\",\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"modelId\": \"gpt2-small\",\n",
    "            \"layer\": \"7-res-jb\",\n",
    "            \"index\": 20115,\n",
    "            \"strength\": 4\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "    \"n_tokens\": 120,\n",
    "    \"freq_penalty\": 1,\n",
    "    \"seed\": np.random.randint(100),\n",
    "    \"strength_multiplier\": 4\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Ablation\n",
    "\n",
    "Feature ablation is also worth looking at. In a way, it's a special case of steering where the value of the feature is always zeroed out.\n",
    "\n",
    "Here we do the following:\n",
    "1. Use test prompt rather than generate to get more nuance. \n",
    "2. attach a hook to the SAE feature activations.\n",
    "3. 0 out a feature at all positions (we know that the default feature fires at the final position.)\n",
    "4. Check whether this ablation is more / less effective if we include the error term (info our SAE isn't capturing).\n",
    "\n",
    "Note that the existence of [The Hydra Effect](https://arxiv.org/abs/2307.15771) can make reasoning about ablation experiments difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "from functools import partial\n",
    "\n",
    "def test_prompt_with_ablation(model, sae, prompt, answer, ablation_features):\n",
    "    \n",
    "    def ablate_feature_hook(feature_activations, hook, feature_ids, position = None):\n",
    "    \n",
    "        if position is None:\n",
    "            feature_activations[:,:,feature_ids] = 0\n",
    "        else:\n",
    "            feature_activations[:,position,feature_ids] = 0\n",
    "        \n",
    "        return feature_activations\n",
    "        \n",
    "    ablation_hook = partial(ablate_feature_hook, feature_ids = ablation_features)\n",
    "    \n",
    "    model.add_sae(sae)\n",
    "    hook_point = sae.cfg.hook_name + '.hook_sae_acts_post'\n",
    "    model.add_hook(hook_point, ablation_hook, \"fwd\")\n",
    "    \n",
    "    test_prompt(prompt, answer, model)\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    model.reset_saes()\n",
    "\n",
    "# Example usage in a notebook:\n",
    "\n",
    "# Assume model and sae are already defined\n",
    "\n",
    "# Choose a feature to ablate\n",
    "\n",
    "model.reset_hooks(including_permanent=True)\n",
    "prompt = \"In the beginning, God created the heavens and the\"\n",
    "answer = \"earth\"\n",
    "test_prompt(prompt, answer, model)\n",
    "\n",
    "\n",
    "# Generate text with feature ablation\n",
    "print(\"Test Prompt with feature ablation and no error term\")\n",
    "ablation_feature = 16873  # Replace with any feature index you're interested in. We use the religion feature\n",
    "sae.use_error_term = False\n",
    "test_prompt_with_ablation(model, sae, prompt, answer, ablation_feature)\n",
    "\n",
    "print(\"Test Prompt with feature ablation and error term\")\n",
    "ablation_feature = 16873  # Replace with any feature index you're interested in. We use the religion feature\n",
    "sae.use_error_term = True\n",
    "test_prompt_with_ablation(model, sae, prompt, answer, ablation_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Any, Literal, NamedTuple, Callable\n",
    "\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "\n",
    "class SaeReconstructionCache(NamedTuple):\n",
    "    sae_in: torch.Tensor\n",
    "    feature_acts: torch.Tensor\n",
    "    sae_out: torch.Tensor\n",
    "    sae_error: torch.Tensor\n",
    "\n",
    "\n",
    "def track_grad(tensor: torch.Tensor) -> None:\n",
    "    \"\"\"wrapper around requires_grad and retain_grad\"\"\"\n",
    "    tensor.requires_grad_(True)\n",
    "    tensor.retain_grad()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ApplySaesAndRunOutput:\n",
    "    model_output: torch.Tensor\n",
    "    model_activations: dict[str, torch.Tensor]\n",
    "    sae_activations: dict[str, SaeReconstructionCache]\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Helper to zero grad all tensors in this object.\"\"\"\n",
    "        self.model_output.grad = None\n",
    "        for act in self.model_activations.values():\n",
    "            act.grad = None\n",
    "        for cache in self.sae_activations.values():\n",
    "            cache.sae_in.grad = None\n",
    "            cache.feature_acts.grad = None\n",
    "            cache.sae_out.grad = None\n",
    "            cache.sae_error.grad = None\n",
    "\n",
    "\n",
    "def apply_saes_and_run(\n",
    "    model: HookedTransformer,\n",
    "    saes: dict[str, SAE],\n",
    "    input: Any,\n",
    "    include_error_term: bool = True,\n",
    "    track_model_hooks: list[str] | None = None,\n",
    "    return_type: Literal[\"logits\", \"loss\"] = \"logits\",\n",
    "    track_grads: bool = False,\n",
    ") -> ApplySaesAndRunOutput:\n",
    "    \"\"\"\n",
    "    Apply the SAEs to the model at the specific hook points, and run the model.\n",
    "    By default, this will include a SAE error term which guarantees that the SAE\n",
    "    will not affect model output. This function is designed to work correctly with\n",
    "    backprop as well, so it can be used for gradient-based feature attribution.\n",
    "\n",
    "    Args:\n",
    "        model: the model to run\n",
    "        saes: the SAEs to apply\n",
    "        input: the input to the model\n",
    "        include_error_term: whether to include the SAE error term to ensure the SAE doesn't affect model output. Default True\n",
    "        track_model_hooks: a list of hook points to record the activations and gradients. Default None\n",
    "        return_type: this is passed to the model.run_with_hooks function. Default \"logits\"\n",
    "        track_grads: whether to track gradients. Default False\n",
    "    \"\"\"\n",
    "\n",
    "    fwd_hooks = []\n",
    "    bwd_hooks = []\n",
    "\n",
    "    sae_activations: dict[str, SaeReconstructionCache] = {}\n",
    "    model_activations: dict[str, torch.Tensor] = {}\n",
    "\n",
    "    # this hook just track the SAE input, output, features, and error. If `track_grads=True`, it also ensures\n",
    "    # that requires_grad is set to True and retain_grad is called for intermediate values.\n",
    "    def reconstruction_hook(sae_in: torch.Tensor, hook: HookPoint, hook_point: str):  # noqa: ARG001\n",
    "        sae = saes[hook_point]\n",
    "        feature_acts = sae.encode(sae_in)\n",
    "        sae_out = sae.decode(feature_acts)\n",
    "        sae_error = (sae_in - sae_out).detach().clone()\n",
    "        if track_grads:\n",
    "            track_grad(sae_error)\n",
    "            track_grad(sae_out)\n",
    "            track_grad(feature_acts)\n",
    "            track_grad(sae_in)\n",
    "        sae_activations[hook_point] = SaeReconstructionCache(\n",
    "            sae_in=sae_in,\n",
    "            feature_acts=feature_acts,\n",
    "            sae_out=sae_out,\n",
    "            sae_error=sae_error,\n",
    "        )\n",
    "\n",
    "        if include_error_term:\n",
    "            return sae_out + sae_error\n",
    "        return sae_out\n",
    "\n",
    "    def sae_bwd_hook(output_grads: torch.Tensor, hook: HookPoint):  # noqa: ARG001\n",
    "        # this just passes the output grads to the input, so the SAE gets the same grads despite the error term hackery\n",
    "        return (output_grads,)\n",
    "\n",
    "    # this hook just records model activations, and ensures that intermediate activations have gradient tracking turned on if needed\n",
    "    def tracking_hook(hook_input: torch.Tensor, hook: HookPoint, hook_point: str):  # noqa: ARG001\n",
    "        model_activations[hook_point] = hook_input\n",
    "        if track_grads:\n",
    "            track_grad(hook_input)\n",
    "        return hook_input\n",
    "\n",
    "    for hook_point in saes.keys():\n",
    "        fwd_hooks.append(\n",
    "            (hook_point, partial(reconstruction_hook, hook_point=hook_point))\n",
    "        )\n",
    "        bwd_hooks.append((hook_point, sae_bwd_hook))\n",
    "    for hook_point in track_model_hooks or []:\n",
    "        fwd_hooks.append((hook_point, partial(tracking_hook, hook_point=hook_point)))\n",
    "\n",
    "    # now, just run the model while applying the hooks\n",
    "    with model.hooks(fwd_hooks=fwd_hooks, bwd_hooks=bwd_hooks):\n",
    "        model_output = model(input, return_type=return_type)\n",
    "\n",
    "    return ApplySaesAndRunOutput(\n",
    "        model_output=model_output,\n",
    "        model_activations=model_activations,\n",
    "        sae_activations=sae_activations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Any, Literal, NamedTuple\n",
    "\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "@dataclass\n",
    "class AttributionGrads:\n",
    "    metric: torch.Tensor\n",
    "    model_output: torch.Tensor\n",
    "    model_activations: dict[str, torch.Tensor]\n",
    "    sae_activations: dict[str, SaeReconstructionCache]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Attribution:\n",
    "    model_attributions: dict[str, torch.Tensor]\n",
    "    model_activations: dict[str, torch.Tensor]\n",
    "    model_grads: dict[str, torch.Tensor]\n",
    "    sae_feature_attributions: dict[str, torch.Tensor]\n",
    "    sae_feature_activations: dict[str, torch.Tensor]\n",
    "    sae_feature_grads: dict[str, torch.Tensor]\n",
    "    sae_errors_attribution_proportion: dict[str, float]\n",
    "\n",
    "\n",
    "def calculate_attribution_grads(\n",
    "    model: HookedSAETransformer,\n",
    "    prompt: str,\n",
    "    metric_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    track_hook_points: list[str] | None = None,\n",
    "    include_saes: dict[str, SAE] | None = None,\n",
    "    return_logits: bool = True,\n",
    "    include_error_term: bool = True,\n",
    ") -> AttributionGrads:\n",
    "    \"\"\"\n",
    "    Wrapper around apply_saes_and_run that calculates gradients wrt to the metric_fn.\n",
    "    Tracks grads for both SAE feature and model neurons, and returns them in a structured format.\n",
    "    \"\"\"\n",
    "    output = apply_saes_and_run(\n",
    "        model,\n",
    "        saes=include_saes or {},\n",
    "        input=prompt,\n",
    "        return_type=\"logits\" if return_logits else \"loss\",\n",
    "        track_model_hooks=track_hook_points,\n",
    "        include_error_term=include_error_term,\n",
    "        track_grads=True,\n",
    "    )\n",
    "    metric = metric_fn(output.model_output)\n",
    "    output.zero_grad()\n",
    "    metric.backward()\n",
    "    return AttributionGrads(\n",
    "        metric=metric,\n",
    "        model_output=output.model_output,\n",
    "        model_activations=output.model_activations,\n",
    "        sae_activations=output.sae_activations,\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_feature_attribution(\n",
    "    model: HookedSAETransformer,\n",
    "    input: Any,\n",
    "    metric_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    track_hook_points: list[str] | None = None,\n",
    "    include_saes: dict[str, SAE] | None = None,\n",
    "    return_logits: bool = True,\n",
    "    include_error_term: bool = True,\n",
    ") -> Attribution:\n",
    "    \"\"\"\n",
    "    Calculate feature attribution for SAE features and model neurons following\n",
    "    the procedure in https://transformer-circuits.pub/2024/march-update/index.html#feature-heads.\n",
    "    This include the SAE error term by default, so inserting the SAE into the calculation is\n",
    "    guaranteed to not affect the model output. This can be disabled by setting `include_error_term=False`.\n",
    "\n",
    "    Args:\n",
    "        model: The model to calculate feature attribution for.\n",
    "        input: The input to the model.\n",
    "        metric_fn: A function that takes the model output and returns a scalar metric.\n",
    "        track_hook_points: A list of model hook points to track activations for, if desired\n",
    "        include_saes: A dictionary of SAEs to include in the calculation. The key is the hook point to apply the SAE to.\n",
    "        return_logits: Whether to return the model logits or loss. This is passed to TLens, so should match whatever the metric_fn expects (probably logits)\n",
    "        include_error_term: Whether to include the SAE error term in the calculation. This is recommended, as it ensures that the SAE will not affecting the model output.\n",
    "    \"\"\"\n",
    "    # first, calculate gradients wrt to the metric_fn.\n",
    "    # these will be multiplied with the activation values to get the attributions\n",
    "    outputs_with_grads = calculate_attribution_grads(\n",
    "        model,\n",
    "        input,\n",
    "        metric_fn,\n",
    "        track_hook_points,\n",
    "        include_saes=include_saes,\n",
    "        return_logits=return_logits,\n",
    "        include_error_term=include_error_term,\n",
    "    )\n",
    "    model_attributions = {}\n",
    "    model_activations = {}\n",
    "    model_grads = {}\n",
    "    sae_feature_attributions = {}\n",
    "    sae_feature_activations = {}\n",
    "    sae_feature_grads = {}\n",
    "    sae_error_proportions = {}\n",
    "    # this code is long, but all it's doing is multiplying the grads by the activations\n",
    "    # and recording grads, acts, and attributions in dictionaries to return to the user\n",
    "    with torch.no_grad():\n",
    "        for name, act in outputs_with_grads.model_activations.items():\n",
    "            assert act.grad is not None\n",
    "            raw_activation = act.detach().clone()\n",
    "            model_attributions[name] = (act.grad * raw_activation).detach().clone()\n",
    "            model_activations[name] = raw_activation\n",
    "            model_grads[name] = act.grad.detach().clone()\n",
    "        for name, act in outputs_with_grads.sae_activations.items():\n",
    "            assert act.feature_acts.grad is not None\n",
    "            assert act.sae_out.grad is not None\n",
    "            raw_activation = act.feature_acts.detach().clone()\n",
    "            sae_feature_attributions[name] = (\n",
    "                (act.feature_acts.grad * raw_activation).detach().clone()\n",
    "            )\n",
    "            sae_feature_activations[name] = raw_activation\n",
    "            sae_feature_grads[name] = act.feature_acts.grad.detach().clone()\n",
    "            if include_error_term:\n",
    "                assert act.sae_error.grad is not None\n",
    "                error_grad_norm = act.sae_error.grad.norm().item()\n",
    "            else:\n",
    "                error_grad_norm = 0\n",
    "            sae_out_norm = act.sae_out.grad.norm().item()\n",
    "            sae_error_proportions[name] = error_grad_norm / (\n",
    "                sae_out_norm + error_grad_norm + EPS\n",
    "            )\n",
    "        return Attribution(\n",
    "            model_attributions=model_attributions,\n",
    "            model_activations=model_activations,\n",
    "            model_grads=model_grads,\n",
    "            sae_feature_attributions=sae_feature_attributions,\n",
    "            sae_feature_activations=sae_feature_activations,\n",
    "            sae_feature_grads=sae_feature_grads,\n",
    "            sae_errors_attribution_proportion=sae_error_proportions,\n",
    "        )\n",
    "        \n",
    "        \n",
    "# prompt = \" Tiger Woods plays the sport of\"\n",
    "# pos_token = model.tokenizer.encode(\" golf\")[0]\n",
    "prompt = \"In the beginning, God created the heavens and the\"\n",
    "pos_token = model.tokenizer.encode(\" earth\")\n",
    "neg_token = model.tokenizer.encode(\" sky\")\n",
    "def metric_fn(logits: torch.tensor, pos_token: torch.tensor =pos_token, neg_token: torch.Tensor=neg_token) -> torch.Tensor:\n",
    "    return logits[0,-1,pos_token] - logits[0,-1,neg_token]\n",
    "\n",
    "feature_attribution_df = calculate_feature_attribution(\n",
    "    input = prompt,\n",
    "    model = model,\n",
    "    metric_fn = metric_fn,\n",
    "    include_saes={sae.cfg.hook_name: sae},\n",
    "    include_error_term=True,\n",
    "    return_logits=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_str_tokens(prompt)\n",
    "unique_tokens = [f\"{i}/{t}\" for i, t in enumerate(tokens)]\n",
    "\n",
    "px.bar(x = unique_tokens,\n",
    "       y = feature_attribution_df.sae_feature_attributions[sae.cfg.hook_name][0].sum(-1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_feature_to_long_df(sparse_tensor: torch.Tensor) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a sparse tensor to a long format pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(sparse_tensor.detach().cpu().numpy())\n",
    "    df_long = df.melt(ignore_index=False, var_name='column', value_name='value')\n",
    "    df_long.columns = [\"feature\", \"attribution\"]\n",
    "    df_long_nonzero = df_long[df_long['attribution'] != 0]\n",
    "    df_long_nonzero = df_long_nonzero.reset_index().rename(columns={'index': 'position'})\n",
    "    return df_long_nonzero\n",
    "\n",
    "df_long_nonzero = convert_sparse_feature_to_long_df(feature_attribution_df.sae_feature_attributions[sae.cfg.hook_name][0])\n",
    "df_long_nonzero.sort_values(\"attribution\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# get a random feature from the SAE\n",
    "feature_idx = torch.randint(0, sae.cfg.d_sae, (1,)).item()\n",
    "\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release = \"gpt2-small\", sae_id=\"7-res-jb\", feature_idx=0):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)\n",
    "\n",
    "# html = get_dashboard_html(sae_release = \"gpt2-small\", sae_id=\"7-res-jb\", feature_idx=feature_idx)\n",
    "# IFrame(html, width=1200, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_long_nonzero.query(\"position==8\").groupby(\"feature\").attribution.sum().sort_values(ascending=False).head(5).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# one by one\n",
    "# for i, v in df_long_nonzero.query(\"position==8\").groupby(\"feature\").attribution.sum().sort_values(ascending=False).head(5).items():\n",
    "#     print(f\"Feature {i} had a total attribution of {v:.2f}\")\n",
    "#     html = get_dashboard_html(sae_release = \"gpt2-small\", sae_id=f\"{sae.cfg.hook_layer}-res-jb\", feature_idx=int(i))\n",
    "#     display(IFrame(html, width=1200, height=300))\n",
    "\n",
    "# as a list \n",
    "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "get_neuronpedia_quick_list(sae, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in df_long_nonzero.groupby(\"feature\").attribution.sum().sort_values(ascending=False).head(5).items():\n",
    "    print(f\"Feature {i} had a total attribution of {v:.2f}\")\n",
    "    html = get_dashboard_html(sae_release = \"gpt2-small\", sae_id=f\"{sae.cfg.hook_layer}-res-jb\", feature_idx=int(i))\n",
    "    display(IFrame(html, width=1200, height=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "get_neuronpedia_quick_list(sae, feature_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
