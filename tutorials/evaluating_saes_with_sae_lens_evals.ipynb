{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating SAEs with SAE Lens Evals\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial we will cover the use of our evaluation tools on SAEs. As there is no single metric for SAE quality, we include a number of metrics from multiple categories to help you assess whether an SAE meets your usage requirements. In many cases, you will want to compare multiple SAEs for a given model layer in order to determine which performs best on specific metrics. Below, we explain how to run the evaluations, how each metric is calculated, and how to interpret the values shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    from google.colab import output\n",
    "\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens sae-dashboard\n",
    "except:\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython  # type: ignore\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Command Line Utility\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see which SAEs are available for a specified model, you can simply run the following code snippet. For running evals, the primary things to pay attention to are the release name and the format used in `saes_map` to specify the target layer. These will be used as input for the evals runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "\n",
    "# TODO: Make this nicer.\n",
    "df = pd.DataFrame.from_records(\n",
    "    {k: v.__dict__ for k, v in get_pretrained_saes_directory().items()}\n",
    ").T\n",
    "df.drop(\n",
    "    columns=[\n",
    "        \"expected_var_explained\",\n",
    "        \"expected_l0\",\n",
    "        \"config_overrides\",\n",
    "        \"conversion_func\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "df[\n",
    "    df.release.str.contains(\"gpt2\")\n",
    "]  # Each row is a \"release\" which has multiple SAEs which may have different configs / match different hook points in a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run evals for 3 SAEs:\n",
    "- Apollo's End-to-End SAEs with downstream reconstruction loss (`gpt2-small-res_scefr-ajt`)\n",
    "- OpenAI's TopK SAEs (`gpt2-small-resid-mid-v5-32k`)\n",
    "- Joseph Bloom's Residual Stream SAEs (`gpt2-small-res-jb`)\n",
    "\n",
    "These SAEs are not perfectly comparable because they are not all the same width (32K for OpenAI vs. 24K for the others), and they are not all trained at the same part of the layer (resid-mid for OpenAI TopK, resid-pre for Joseph Bloom's and Apollo's), but they will suffice for our tutorial.\n",
    "\n",
    "Currently, the evals are designed to be run through the CLI. Simply copy and paste the command below. (Note: Omit the `poetry run` portion if you are not using a poetry environment.)\n",
    "\n",
    "```bash\n",
    "poetry run python sae_lens/evals.py \"gpt2-small-res_scefr-ajt|gpt2-small-resid-mid-v5-32k|gpt2-small-res-jb\" \"blocks.10.*\" \\\n",
    "    --batch_size_prompts 16 \\\n",
    "    --n_eval_sparsity_variance_batches 200 \\\n",
    "    --n_eval_reconstruction_batches 20 \\\n",
    "    --output_dir \"demo_eval_results\" \\ \n",
    "    --verbose\n",
    "```\n",
    "\n",
    "`evals.py` accepts a wide range of arguments, most of which are used to control whether to evaluate a particular metric. You can see the full list in the source code, but the most important arguments are the ones we've used here:\n",
    "- `\"gpt2-small-res_scefr-ajt|gpt2-small-resid-mid-v5-32k|gpt2-small-res-jb\"` This is a regex pattern used to match SAE release names. The script allows you to specify multiple or just one.\n",
    "- `\"blocks.10.*\"` This is another regex pattern for SAE block names. This particular pattern applies to each of the releases we're targeting, so layer 10 (resid-pre or resid-mid) will be evaluated for each of them.\n",
    "- `batch_size_prompts 16` Here, we're setting the batch size for the dataset of evaluation prompts to 16.\n",
    "- `n_eval_sparsity_variance_batches 200` The total number of  evaluation prompt batches to be used when evaluating sparsity and variance metrics.\n",
    "- `n_eval_reconstruction_batches 20` Number of prompt batches to use for reconstruction evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the evals are run, we can open them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../demo_eval_results\"\n",
    "\n",
    "# list files in the output directory\n",
    "print(os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the results is printed below. The evaluation script will produce a list of nested dictionaries, with each unique SAE (release and SAE ID combination) as an item in the list. Dictionaries for individual models are saved as files, and a file with all results combined is saved as well. Within the nested dictionary, we store metadata about the SAE, the `eval_cfg` that was used to generate the results, a dictionary containing SAE-level metrics (`metrics`), another dictionary containing feature-level metrics (`feature_metrics`), and finally the SAE config, which contains information about the SAE itself, its training, the dataset used for evaluation, and various other details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first file in the output directory\n",
    "first_file = os.path.join(output_dir, os.listdir(output_dir)[0])\n",
    "print(first_file)\n",
    "\n",
    "# load the results json pretty print it\n",
    "with open(first_file, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(json.dumps(results[\"sae_set\"], indent=4))\n",
    "print(json.dumps(results[\"eval_cfg\"], indent=4))\n",
    "print(json.dumps(results[\"metrics\"], indent=4))\n",
    "# print(json.dumps(results['feature_metrics'], indent=4)) # uncomment to see feature metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[\"metrics\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Configuration\n",
    "\n",
    "The `eval_cfg` dictionary shows what parameters were used to generate the evaluation. The Boolean items are used to set whether to run particular evaluations; by default, all are set to True. Some other parameters of note:\n",
    "- `context_size`: The length of the prompts used in the evaluation.\n",
    "- `dataset`: This is a path to the HuggingFace dataset used for evaluation. Often, this will be the same dataset the SAE was trained on, but this is not always the case--sometimes that dataset is not publicly available.\n",
    "- `library_version`: The version of SAE Lens used.\n",
    "- `git_hash`: The hash of the specific commit used.\n",
    "- `batch_size_prompts`: Number of prompts per batch, for each batch of the evaluation runs.\n",
    "- `n_eval_reconstruction_batches`: Number of batches used to evaluate reconstruction metrics. This number sometimes needs tuning to ensure that you are getting sufficient data and some level of metric stability.\n",
    "- `n_eval_sparsity_variance_batches`: Number of batches used to evaluate sparsity and variance metrics. This number needs tuning as well; e.g. if it is too low, the number of dead neurons will appear artificially high, and the feature density histogram will appear less smooth. Generally, this number should be much higher than the number of reconstruction batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Sense of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding SAE Metrics\n",
    "\n",
    "The SAE metrics we compute can be grouped into six distinct categories. Let's explore each category and its associated metrics in detail. We'll then compare our three SAEs on each metric.\n",
    "\n",
    "\n",
    "#### Model Performance Preservation\n",
    "\n",
    "These metrics indicate how much the SAE affects the underlying model's performance and output:\n",
    "\n",
    "- **ce_loss_with_sae**: Cross-entropy loss of the model's output after applying the SAE.\n",
    "- **ce_loss_without_sae**: Baseline cross-entropy loss of the original model without the SAE.\n",
    "- **ce_loss_with_ablation**: Cross-entropy loss when the relevant activations are set to zero.\n",
    "- **ce_loss_score**: A derived metric comparing the cross-entropy losses with and without the SAE.\n",
    "\n",
    "#### Model Behavior Preservation\n",
    "\n",
    "These metrics indicate differences between the distributions of logit predictions with and without the SAE.\n",
    "\n",
    "- **kl_div_score**: A derived metric comparing the KL divergence with SAE to the KL divergence with ablation.\n",
    "- **kl_div_with_sae**: Kullback-Leibler divergence between the original model's output distribution and the distribution after applying the SAE. Lower values indicate better preservation of the model's behavior.\n",
    "- **kl_div_with_ablation**: KL divergence between the original model's output and the output when the relevant activations are set to zero. This serves as a baseline for comparison.\n",
    "\n",
    "#### Reconstruction Quality\n",
    "\n",
    "These metrics assess how well the SAE can reconstruct the original input activations at the target layer:\n",
    "\n",
    "- **mse**: Mean Squared Error between the input and the reconstruction. Lower values indicate better reconstruction accuracy.\n",
    "- **explained_variance**: The proportion of variance in the input that is explained by the SAE's reconstruction. Higher values indicate better reconstruction quality.\n",
    "- **cossim**: Cosine similarity between the input and the reconstruction. Values closer to 1 indicate better preservation of the input's direction.\n",
    "\n",
    "#### Shrinkage\n",
    "\n",
    "These metrics show how well the SAE preserves the overall magnitude of the input:\n",
    "\n",
    "- **l2_norm_in**: The L2 norm (Euclidean norm) of the input activations.\n",
    "- **l2_norm_out**: The L2 norm of the output activations after passing through the SAE.\n",
    "- **l2_ratio**: The ratio of the L2 norm of the SAE output to the L2 norm of the SAE input. A value close to 1 indicates good preservation of the input's magnitude.\n",
    "- **relative_reconstruction_bias**: Measures the bias in reconstruction. Values closer to 1 indicate less bias.\n",
    "\n",
    "#### Sparsity\n",
    "\n",
    "These metrics measure how sparse the SAE's activations are:\n",
    "\n",
    "- **l0**: The L0 \"norm\" of the SAE's activations, which is the number of non-zero elements. It measures how many features are active.\n",
    "- **l1**: The L1 norm of the SAE's activations, which is the sum of the absolute values. It's another measure of sparsity.\n",
    "\n",
    "#### Token Statistics\n",
    "\n",
    "These metrics provide context about the amount of data used in the evaluation:\n",
    "\n",
    "- **total_tokens_eval_reconstruction**: The total number of tokens used in evaluating reconstruction metrics.\n",
    "- **total_tokens_eval_sparsity_variance**: The total number of tokens used in evaluating sparsity and variance metrics.\n",
    "\n",
    "Now let's take a look at the metrics we've collected and see how our SAEs fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the metric groups\n",
    "metric_groups = {\n",
    "    \"Reconstruction Quality\": [\n",
    "        \"l2_ratio\",\n",
    "        \"relative_reconstruction_bias\",\n",
    "        \"explained_variance\",\n",
    "        \"mse\",\n",
    "        \"cossim\",\n",
    "    ],\n",
    "    \"Magnitude Preservation\": [\"l2_norm_in\", \"l2_norm_out\"],\n",
    "    \"Model Behavior Preservation\": [\n",
    "        \"kl_div_with_sae\",\n",
    "        \"kl_div_with_ablation\",\n",
    "        \"kl_div_score\",\n",
    "        \"ce_loss_with_sae\",\n",
    "        \"ce_loss_without_sae\",\n",
    "        \"ce_loss_with_ablation\",\n",
    "        \"ce_loss_score\",\n",
    "    ],\n",
    "    \"Sparsity\": [\"l0\", \"l1\"],\n",
    "    \"Evaluation Scale\": [\n",
    "        \"total_tokens_eval_reconstruction\",\n",
    "        \"total_tokens_eval_sparsity_variance\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Open all_eval_results.json\n",
    "all_eval_results_file = os.path.join(output_dir, \"all_eval_results.json\")\n",
    "with open(all_eval_results_file, \"r\") as f:\n",
    "    all_eval_results = json.load(f)\n",
    "\n",
    "# Sort all_eval_results by sae_set and sae_id\n",
    "all_eval_results.sort(key=lambda x: (x[\"sae_set\"], x[\"sae_id\"]))\n",
    "\n",
    "# Print all unique SAE sets in order\n",
    "print(\"SAE Sets evaluated:\")\n",
    "unique_sae_sets = []\n",
    "for result in all_eval_results:\n",
    "    if result[\"sae_set\"] not in unique_sae_sets:\n",
    "        unique_sae_sets.append(result[\"sae_set\"])\n",
    "print(\", \".join(unique_sae_sets))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Function to print metrics for all SAEs\n",
    "def print_all_sae_metrics(all_results):\n",
    "    # Get unique combinations of dataset and context size\n",
    "    configs = set(\n",
    "        (r[\"eval_cfg\"][\"dataset\"], r[\"eval_cfg\"][\"context_size\"]) for r in all_results\n",
    "    )\n",
    "\n",
    "    for dataset, context_size in configs:\n",
    "        print(f\"Dataset: {dataset}, Context Size: {context_size}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Filter results for this dataset and context size\n",
    "        filtered_results = [\n",
    "            r\n",
    "            for r in all_results\n",
    "            if r[\"eval_cfg\"][\"dataset\"] == dataset\n",
    "            and r[\"eval_cfg\"][\"context_size\"] == context_size\n",
    "        ]\n",
    "\n",
    "        # Print SAE identifiers\n",
    "        sae_ids = [f\"{r['sae_set']} - {r['sae_id']}\" for r in filtered_results]\n",
    "        print(\"SAEs:\", \", \".join(sae_ids))\n",
    "        print()\n",
    "\n",
    "        for group, metrics in metric_groups.items():\n",
    "            print(f\"\\n{group}:\")\n",
    "            for metric in metrics:\n",
    "                values = [r[\"metrics\"].get(metric, \"N/A\") for r in filtered_results]\n",
    "                formatted_values = [\n",
    "                    f\"{v:.4f}\" if isinstance(v, float) else str(v) for v in values\n",
    "                ]\n",
    "                print(f\"  {metric}: {formatted_values}\")\n",
    "        print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "\n",
    "# Print metrics for all SAEs\n",
    "print_all_sae_metrics(all_eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some interesting differences here among the SAEs. For example, the Apollo end-to-end SAE shows a much higher MSE between the SAE input and reconstruction, and also shows a significant change in L2 norm of the output vs. input. It also has anomalous explained variance and much lower input-output cosine sim. Yet, it performs the best in terms of KL divergence. These differences are probably due to the very different loss function that these SAEs were trained on.\n",
    "\n",
    "We can also see that the OpenAI TopK SAE has much more sparsity than the other two SAEs (which entails lower L0 and L1 values), yet still performs similarly to the JB SAEs in terms of fidelity (CE loss score) and other model behavior preservation metrics. These SAEs were trained with a TopK activation function, which enforces the desired level of sparsity.\n",
    "\n",
    "Ideally, we would like to have perfect model behavior preservation, a high degree of sparsity, perfect reconstruction quality, and so on. Unfortunately, in reality there are often tradeoffs between these desiderata. Nevertheless, sometimes new SAE methods advance the Pareto frontier, allowing us to find objectively somewhat better SAEs for our use case.\n",
    "\n",
    "In this case, since the OpenAI TopK SAE performed quite well, we might choose this one for a future project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Featurewise Metrics\n",
    "\n",
    "In our evaluation process, we also collect metrics for individual SAE features. Let's go through these category by category, starting with feature density metrics. We collect two of these:\n",
    "- Feature density itself, which we generally plot as log10 feature density\n",
    "- Consistent activation heuristic, which helps identify multi-token features along with dense features in general.\n",
    "\n",
    "Let's plot these first, and we'll then explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Density Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = os.path.join(\n",
    "    output_dir,\n",
    "    \"gpt2-small-resid-mid-v5-32k-blocks.10.hook_resid_mid_128_Skylion007_openwebtext.json\",\n",
    ")\n",
    "with open(results_file, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "feature_stats = results[\"feature_metrics\"]\n",
    "feature_df = pd.DataFrame.from_dict(feature_stats)\n",
    "\n",
    "feature_df[\"feature_name\"] = [i for i in range(len(feature_df))]\n",
    "\n",
    "log10_feature_density = np.log10(feature_df[\"feature_density\"] + 1e-10)\n",
    "log10_consistent_activation_heuristic = np.log10(\n",
    "    feature_df[\"consistent_activation_heuristic\"] + 1e-10\n",
    ")\n",
    "\n",
    "px.histogram(\n",
    "    feature_df,\n",
    "    x=log10_feature_density,\n",
    "    nbins=100,\n",
    "    title=\"Log10 Feature Density\",\n",
    "    labels={\"x\": \"log10_feature_density\"},  # Add this line\n",
    ").show()\n",
    "\n",
    "px.scatter(\n",
    "    feature_df,\n",
    "    x=log10_feature_density,\n",
    "    y=log10_consistent_activation_heuristic,\n",
    "    marginal_y=\"histogram\",\n",
    "    marginal_x=\"histogram\",\n",
    "    hover_name=\"feature_name\",\n",
    "    title=\"Log10 Feature Density vs Consistent Activation Heuristic\",\n",
    "    labels={\"x\": \"log10_feature_density\", \"y\": \"log10_consistent_activation_heuristic\"},\n",
    "    height=800,\n",
    "    width=1200,\n",
    "    opacity=0.2,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now discuss how these are calculated and how to interpret them.\n",
    "\n",
    "#### Feature Density\n",
    "\n",
    "According to [Anthropic](https://transformer-circuits.pub/2023/monosemantic-features#appendix-feature-density), an important proxy for autoencoder performance is *feature density*. Feature Density is defined as the fraction of tokens on which the feature fires / activates. In practice, we plot a histogram of `-log10(feature_density)` in order to see the distribution of feature densities. \n",
    "\n",
    "##### Calculation\n",
    "\n",
    "To calculate feature density, we count how many times a feature fires over a very large dataset.\n",
    "\n",
    "```python\n",
    "feature_density = total_feature_acts / total_tokens\n",
    "log10_feature_density = np.log10(feature_density + 1e-10)\n",
    "```\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "- **dense latents**: Latents that fire every on in one hundred tokens or fewer can be considered dense and show up between -2 and 0 on the histogram. Dense features are less likely to be interpretable and too many dense features might enable summary statistics such as MSE to appear deceptively good. \n",
    "- **dead latents**: Latents that fail to fire on any examples in the dataset may be \"dead\" meaning that they may never fire and are essentially wasted capacity of the SAE. We see these as a column at -10 on the histogram.\n",
    "- **bimodality**: Sometimes the distribution appears bimodal (having two peaks). We're not entirely sure what causes this (if you know, let us know!)\n",
    "\n",
    "#### Consistent Activation Heuristic (CAH)\n",
    "\n",
    "The Consistent Activation Heuristic is a measure of how \"contextiness\". High scores on CAH correspond to latents that will fire on almost every token in a context that they fire on (eg: tracking that a piece of text is a about a topic). Anecdotally, these latents may be more useful for steering than others. By contrast, low CAH latents tend to fire once per prompt (maybe on a specific token). \n",
    "\n",
    "##### Calculation\n",
    "\n",
    "```python\n",
    "consistent_activation_heuristic = total_feature_acts / total_feature_prompts\n",
    "log10_consistent_activation_heuristic = np.log10(consistent_activation_heuristic + 1e-10)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `total_feature_acts`: The total number of times each feature was activated across all tokens.\n",
    "- `total_feature_prompts`: The number of prompts in which each feature was activated at least once.\n",
    "\n",
    "#### Combining CAH and Log10 Feature Density\n",
    "\n",
    "When visualizing these metrics together (e.g., in a scatter plot), you can identify:\n",
    "\n",
    "1. Dense, contexty latents (high log10 density, high CAH)\n",
    "2. Sparse, contexty latents (low log10 density, high CAH)\n",
    "3. Dense, few token or single token latents (high log10 density, low CAH)\n",
    "4. Sparse, few token or single token latents (low log10 density, low CAH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurewise Weight Based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature_id\": [\n",
    "            f\"feature_{i}\"\n",
    "            for i in range(\n",
    "                len(results[\"feature_metrics\"][\"encoder_decoder_cosine_sim\"])\n",
    "            )\n",
    "        ],\n",
    "        \"encoder_bias\": results[\"feature_metrics\"][\"encoder_bias\"],\n",
    "        \"encoder_norm\": results[\"feature_metrics\"][\"encoder_norm\"],\n",
    "        \"encoder_decoder_cosine_sim\": results[\"feature_metrics\"][\n",
    "            \"encoder_decoder_cosine_sim\"\n",
    "        ],\n",
    "        \"feature_density\": results[\"feature_metrics\"][\"feature_density\"],\n",
    "        \"consistent_activation_heuristic\": results[\"feature_metrics\"][\n",
    "            \"consistent_activation_heuristic\"\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "df[\"log10_feature_density\"] = np.log10(df[\"feature_density\"] + 1e-10)\n",
    "df[\"log10_consistent_activation_heuristic\"] = np.log10(\n",
    "    df[\"consistent_activation_heuristic\"] + 1e-10\n",
    ")\n",
    "\n",
    "px.scatter(\n",
    "    df,\n",
    "    x=log10_feature_density,\n",
    "    y=log10_consistent_activation_heuristic,\n",
    "    marginal_y=\"histogram\",\n",
    "    marginal_x=\"histogram\",\n",
    "    hover_name=\"feature_id\",\n",
    "    title=\"Log10 Feature Density vs Consistent Activation Heuristic\",\n",
    "    labels={\"x\": \"log10_feature_density\", \"y\": \"log10_consistent_activation_heuristic\"},\n",
    "    height=800,\n",
    "    width=1200,\n",
    "    opacity=0.2,\n",
    ").show()\n",
    "\n",
    "\n",
    "def create_cosine_sim_plot(feature_df, title):\n",
    "    fig = px.histogram(feature_df, x=\"encoder_decoder_cosine_sim\", title=title)\n",
    "    fig.update_layout(height=400, width=600)\n",
    "    return fig\n",
    "\n",
    "\n",
    "# # Create and display the encoder-decoder cosine similarity histogram\n",
    "cosine_sim_hist = create_cosine_sim_plot(df, \"Encoder-Decoder Cosine Similarity\")\n",
    "cosine_sim_hist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Based Metrics\n",
    "\n",
    "Further insights into SAE quality can be derived from weight based analysis of the SAE. Specifically:\n",
    "- `Encoder Bias`: The encoder bias term tends to roughly correspond to feature density. \n",
    "- `Encoder Norm`: A larger encoder norm tends to be present in features that activate on a smaller number of vocabulary items ([according to the mechanistic interpretability team OpenAI](https://arxiv.org/pdf/2406.04093))\n",
    "- `Decoder - Encoder Cosine Similarity`: Early SAEs were trained with tied encoder / decoder weights, but performance improvements were found by untieing these weights. Checking the similarity between encoder and decoder weights may be informative (for example, differences between encoder and decoder weights may be related to [feature absorption](https://arxiv.org/abs/2409.14507))\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "- `Encoder Bias` and `Encoder Norm` likely track something important but it's not yet clear from the literature whether there's a standard way to use these to evaluate SAEs.\n",
    "- On the other hand, it's likely better `Decoder - Encoder Cosine Similarity` is generally higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
