{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "# from transformer_lens.train import HookedTransformerTrainConfig, train\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import os\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create huggingface data with othello games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_last_missing_move(data):\n",
    "    rows = data[\"tokens\"]\n",
    "    for i, row in enumerate(rows):\n",
    "        # Add number between 1 and 60 that is not present in the tokens as last move\n",
    "        all_numbers = set(range(1, 61))\n",
    "        # Take the number that is not in the intersection of all_numbers and tokens\n",
    "        last_number = list(all_numbers - set(row))[0]\n",
    "        # Update the row in place\n",
    "        rows[i].append(last_number)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 27.66ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 736.23ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': tensor([20, 21, 28, 23, 13,  5, 34, 19, 16, 43, 14, 30, 22, 40, 47, 48, 33, 54,\n",
      "        52, 42, 41, 49, 50, 39, 29, 58, 38, 36, 18, 12, 57, 56, 55, 24, 44, 10,\n",
      "        59, 26, 11, 37, 25, 27, 31,  2, 17, 32,  9, 35, 45, 60,  3, 46,  6,  4,\n",
      "        53,  1, 15,  7, 51,  8])}\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = t.tensor(np.load(\"data/board_seqs_int_small.npy\"), dtype=t.long)\n",
    "tokenized_data = tokenized_data[:, :59] # remove XX at the end\n",
    "\n",
    "data_dict = {\"tokens\": tokenized_data.tolist()}\n",
    "\n",
    "# shrink to only first 100\n",
    "data_dict[\"tokens\"] = data_dict[\"tokens\"][:100]\n",
    "add_last_missing_move(data_dict)\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "dataset.set_format(type=\"torch\", columns=[\"tokens\"])\n",
    "\n",
    "\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# upload to Hugging Face\n",
    "access_token = os.environ[\"HF_TOKEN\"]\n",
    "dataset_dict.push_to_hub(\"Thijmen/othello_dataset\", \"main\", token=access_token)\n",
    "\n",
    "# print a row of data\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model_cfg = HookedTransformerConfig(\n",
    "    n_layers = 6,\n",
    "    d_model = 128,\n",
    "    d_head = 64,\n",
    "    n_heads = 8,\n",
    "    d_mlp = 128*4,\n",
    "    d_vocab = 61,\n",
    "    n_ctx = 59,\n",
    "    act_fn=\"gelu\",\n",
    "    normalization_type=\"LN\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model = HookedTransformer(model_cfg).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model and save locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model, train_cfg, dataset)\n",
    "# t.save(model.state_dict(), f\"othello_gpt.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model and verify that model can be loaded using TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "odict_keys(['embed.W_E', 'pos_embed.W_pos', 'blocks.0.ln1.w', 'blocks.0.ln1.b', 'blocks.0.ln2.w', 'blocks.0.ln2.b', 'blocks.0.attn.W_Q', 'blocks.0.attn.W_O', 'blocks.0.attn.b_Q', 'blocks.0.attn.b_O', 'blocks.0.attn.W_K', 'blocks.0.attn.W_V', 'blocks.0.attn.b_K', 'blocks.0.attn.b_V', 'blocks.0.attn.mask', 'blocks.0.attn.IGNORE', 'blocks.0.mlp.W_in', 'blocks.0.mlp.b_in', 'blocks.0.mlp.W_out', 'blocks.0.mlp.b_out', 'blocks.1.ln1.w', 'blocks.1.ln1.b', 'blocks.1.ln2.w', 'blocks.1.ln2.b', 'blocks.1.attn.W_Q', 'blocks.1.attn.W_O', 'blocks.1.attn.b_Q', 'blocks.1.attn.b_O', 'blocks.1.attn.W_K', 'blocks.1.attn.W_V', 'blocks.1.attn.b_K', 'blocks.1.attn.b_V', 'blocks.1.attn.mask', 'blocks.1.attn.IGNORE', 'blocks.1.mlp.W_in', 'blocks.1.mlp.b_in', 'blocks.1.mlp.W_out', 'blocks.1.mlp.b_out', 'blocks.2.ln1.w', 'blocks.2.ln1.b', 'blocks.2.ln2.w', 'blocks.2.ln2.b', 'blocks.2.attn.W_Q', 'blocks.2.attn.W_O', 'blocks.2.attn.b_Q', 'blocks.2.attn.b_O', 'blocks.2.attn.W_K', 'blocks.2.attn.W_V', 'blocks.2.attn.b_K', 'blocks.2.attn.b_V', 'blocks.2.attn.mask', 'blocks.2.attn.IGNORE', 'blocks.2.mlp.W_in', 'blocks.2.mlp.b_in', 'blocks.2.mlp.W_out', 'blocks.2.mlp.b_out', 'blocks.3.ln1.w', 'blocks.3.ln1.b', 'blocks.3.ln2.w', 'blocks.3.ln2.b', 'blocks.3.attn.W_Q', 'blocks.3.attn.W_O', 'blocks.3.attn.b_Q', 'blocks.3.attn.b_O', 'blocks.3.attn.W_K', 'blocks.3.attn.W_V', 'blocks.3.attn.b_K', 'blocks.3.attn.b_V', 'blocks.3.attn.mask', 'blocks.3.attn.IGNORE', 'blocks.3.mlp.W_in', 'blocks.3.mlp.b_in', 'blocks.3.mlp.W_out', 'blocks.3.mlp.b_out', 'blocks.4.ln1.w', 'blocks.4.ln1.b', 'blocks.4.ln2.w', 'blocks.4.ln2.b', 'blocks.4.attn.W_Q', 'blocks.4.attn.W_O', 'blocks.4.attn.b_Q', 'blocks.4.attn.b_O', 'blocks.4.attn.W_K', 'blocks.4.attn.W_V', 'blocks.4.attn.b_K', 'blocks.4.attn.b_V', 'blocks.4.attn.mask', 'blocks.4.attn.IGNORE', 'blocks.4.mlp.W_in', 'blocks.4.mlp.b_in', 'blocks.4.mlp.W_out', 'blocks.4.mlp.b_out', 'blocks.5.ln1.w', 'blocks.5.ln1.b', 'blocks.5.ln2.w', 'blocks.5.ln2.b', 'blocks.5.attn.W_Q', 'blocks.5.attn.W_O', 'blocks.5.attn.b_Q', 'blocks.5.attn.b_O', 'blocks.5.attn.W_K', 'blocks.5.attn.W_V', 'blocks.5.attn.b_K', 'blocks.5.attn.b_V', 'blocks.5.attn.mask', 'blocks.5.attn.IGNORE', 'blocks.5.mlp.W_in', 'blocks.5.mlp.b_in', 'blocks.5.mlp.W_out', 'blocks.5.mlp.b_out', 'ln_final.w', 'ln_final.b', 'unembed.W_U', 'unembed.b_U'])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer(model_cfg).to(device)\n",
    "model.load_and_process_state_dict(t.load(f\"othello_gpt.pth\"))\n",
    "print(model.state_dict().keys())\n",
    "print(model.tokenizer) # model has no tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload model to huggingface\n",
    "\n",
    "I upload the othello_pth.pth to huggingface manually and created a config.json for it\n",
    "\n",
    "See: https://huggingface.co/Thijmen/othello-GPT-model\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "  \"n_layers\": 6,\n",
    "  \"d_model\": 128,\n",
    "  \"d_head\": 64,\n",
    "  \"n_heads\": 8,\n",
    "  \"d_mlp\": 512,\n",
    "  \"d_vocab\": 61,\n",
    "  \"n_ctx\": 59,\n",
    "  \"act_fn\": \"gelu\",\n",
    "  \"normalization_type\": \"LN\",\n",
    "  \"attn_only\": false,\n",
    "  \"architecture\": \"mingpt\",\n",
    "  \"model_type\": \"gpt2\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Added my model to offical list in the TransformerLens library\n",
    "\n",
    "In the file transformerlens/loading_from_pretrained.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SAE\n",
    "\n",
    "Getting error as model does not seem to be in the right format. I should probably train it with huggingface library? Not sure what I need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "\n",
    "# train_cfg = HookedTransformerTrainConfig(\n",
    "#     lr=1e-4,\n",
    "#     batch_size=512,\n",
    "#     num_epochs=1,\n",
    "#     device=device,\n",
    "#     wandb=True,\n",
    "#     wandb_project_name=\"OthelloTraining\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 128-L1-5-LR-5e-05-Tokens-5.120e+04\n",
      "n_tokens_per_buffer (millions): 0.003072\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 100\n",
      "Total wandb updates: 3\n",
      "n_tokens_per_feature_sampling_window (millions): 1.536\n",
      "n_tokens_per_dead_feature_window (millions): 1.536\n",
      "We will reset the sparsity calculation 0 times.\n",
      "Number tokens in sparsity calculation window: 5.12e+05\n",
      "This happend\n",
      "{'n_layers': 6, 'd_model': 128, 'd_mlp': 512, 'd_head': 64, 'n_heads': 8, 'n_ctx': 59, 'd_vocab': 61, 'act_fn': 'gelu', 'attn_only': False, 'normalization_type': 'LNPre'}\n",
      "Loaded pretrained model my-own-othello-model into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthijmen-nijdam\u001b[0m (\u001b[33mfact_ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/wandb/run-20240617_170345-8m5p4f5v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fact_ai/sae-othello/runs/8m5p4f5v' target=\"_blank\">128-L1-5-LR-5e-05-Tokens-5.120e+04</a></strong> to <a href='https://wandb.ai/fact_ai/sae-othello' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fact_ai/sae-othello' target=\"_blank\">https://wandb.ai/fact_ai/sae-othello</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fact_ai/sae-othello/runs/8m5p4f5v' target=\"_blank\">https://wandb.ai/fact_ai/sae-othello/runs/8m5p4f5v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100| MSE Loss 115.223 | L1 0.101: 100%|██████████| 51200/51200 [00:12<00:00, 3984.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>▁▁▁</td></tr><tr><td>details/current_learning_rate</td><td>██▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▅█</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▁█</td></tr><tr><td>losses/mse_loss</td><td>▁▂█</td></tr><tr><td>losses/overall_loss</td><td>▁▂█</td></tr><tr><td>metrics/explained_variance</td><td>█▁▄</td></tr><tr><td>metrics/explained_variance_std</td><td>▁█▅</td></tr><tr><td>metrics/l0</td><td>█▁▁</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>5</td></tr><tr><td>details/current_learning_rate</td><td>3e-05</td></tr><tr><td>details/n_training_tokens</td><td>46080</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>0.01867</td></tr><tr><td>losses/mse_loss</td><td>115.9123</td></tr><tr><td>losses/overall_loss</td><td>116.00567</td></tr><tr><td>metrics/explained_variance</td><td>-1.28853</td></tr><tr><td>metrics/explained_variance_std</td><td>0.50842</td></tr><tr><td>metrics/l0</td><td>1.88086</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>15.92969</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">128-L1-5-LR-5e-05-Tokens-5.120e+04</strong> at: <a href='https://wandb.ai/fact_ai/sae-othello/runs/8m5p4f5v' target=\"_blank\">https://wandb.ai/fact_ai/sae-othello/runs/8m5p4f5v</a><br/> View project at: <a href='https://wandb.ai/fact_ai/sae-othello' target=\"_blank\">https://wandb.ai/fact_ai/sae-othello</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240617_170345-8m5p4f5v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_training_steps = 100  # probably we should do more\n",
    "batch_size = 512\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "l1_warm_up_steps = total_training_steps // 20  # 5% of training\n",
    "\n",
    "thijmen_data = \"Thijmen/othello_dataset\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"my-own-othello-model\",  # added this to offical list in TransformerLens library\n",
    "    hook_name=\"blocks.0.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n",
    "    hook_layer=0,  # Only one layer in the model.\n",
    "    d_in=128,  # the width of the mlp output.\n",
    "    dataset_path=\"taufeeque/othellogpt\",  # my own dataset which i created in this file and uploaded to HF.\n",
    "    is_dataset_tokenized=True, # dataset is tokenized, although i saw this flag is not in use anymore\n",
    "    streaming=False,  # we could pre-download the token dataset if it was small.\n",
    "    prepend_bos=False,\n",
    "    # SAE Parameters\n",
    "    mse_loss_normalization=None,  # We won't normalize the mse loss,\n",
    "    expansion_factor=1,  # the width of the SAE. Larger will result in better stats but slower training.\n",
    "    b_dec_init_method=\"zeros\",  # The geometric median can be used to initialize the decoder weights.\n",
    "    apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.\n",
    "    normalize_sae_decoder=False,\n",
    "    scale_sparsity_penalty_by_decoder_norm=True,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    # normalize_activations=True,\n",
    "    # Training Parameters\n",
    "    lr=5e-5,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
    "    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.\n",
    "    l1_coefficient=5,  # will control how sparse the feature activations are\n",
    "    l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    context_size=3,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n",
    "    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    store_batch_size_prompts=16,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=False,  # we don't use ghost grads anymore.\n",
    "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
    "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
    "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
    "    # WANDB\n",
    "    log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"sae-othello\",  # the project name in wandb.\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "    # Misc\n",
    "    # device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "# look at the next cell to see some instruction for what to do while this is running.\n",
    "sparse_autoencoder = SAETrainingRunner(cfg).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_autoencoder.save_model(\"othello_sae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'my-own-othello-model', 'model_class_name': 'HookedTransformer', 'hook_name': 'blocks.0.hook_mlp_out', 'hook_eval': 'NOT_IN_USE', 'hook_layer': 0, 'hook_head_index': None, 'dataset_path': 'taufeeque/othellogpt', 'streaming': False, 'is_dataset_tokenized': True, 'context_size': 3, 'use_cached_activations': False, 'cached_activations_path': None, 'd_in': 128, 'd_sae': 128, 'b_dec_init_method': 'zeros', 'expansion_factor': 1, 'activation_fn': 'relu', 'normalize_sae_decoder': False, 'noise_scale': 0.0, 'from_pretrained_path': None, 'apply_b_dec_to_input': False, 'decoder_orthogonal_init': False, 'decoder_heuristic_init': True, 'init_encoder_as_decoder_transpose': True, 'n_batches_in_buffer': 64, 'training_tokens': 4096000, 'finetuning_tokens': 0, 'store_batch_size_prompts': 16, 'train_batch_size_tokens': 4096, 'normalize_activations': 'none', 'device': 'cpu', 'act_store_device': 'cpu', 'seed': 42, 'dtype': 'float32', 'prepend_bos': False, 'autocast': False, 'autocast_lm': False, 'compile_llm': False, 'llm_compilation_mode': None, 'compile_sae': False, 'sae_compilation_mode': None, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'mse_loss_normalization': None, 'l1_coefficient': 5, 'lp_norm': 1.0, 'scale_sparsity_penalty_by_decoder_norm': True, 'l1_warm_up_steps': 50, 'lr': 5e-05, 'lr_scheduler_name': 'constant', 'lr_warm_up_steps': 0, 'lr_end': 5e-06, 'lr_decay_steps': 200, 'n_restart_cycles': 1, 'finetuning_method': None, 'use_ghost_grads': False, 'feature_sampling_window': 1000, 'dead_feature_window': 1000, 'dead_feature_threshold': 0.0001, 'n_eval_batches': 10, 'eval_batch_size_prompts': None, 'log_to_wandb': False, 'log_activations_store_to_wandb': False, 'log_optimizer_state_to_wandb': False, 'wandb_project': 'sae_lens_tutorial', 'wandb_id': None, 'run_name': '128-L1-5-LR-5e-05-Tokens-4.096e+06', 'wandb_entity': None, 'wandb_log_frequency': 30, 'eval_every_n_wandb_logs': 20, 'resume': False, 'n_checkpoints': 0, 'checkpoint_path': 'checkpoints/t1bxleff', 'verbose': True, 'model_kwargs': {}, 'model_from_pretrained_kwargs': {}, 'sae_lens_version': '3.4.0', 'sae_lens_training_version': '3.4.0', 'tokens_per_buffer': 786432}\n"
     ]
    }
   ],
   "source": [
    "dict = cfg.to_dict()\n",
    "print(dict)\n",
    "sae = sparse_autoencoder\n",
    "# write to json\n",
    "import json\n",
    "with open(\"cfgs/sae_config.json\", \"w\") as f:\n",
    "    json.dump(dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from datasets import load_dataset\n",
    "token_dataset = load_dataset(\n",
    "    path = \"taufeeque/othellogpt\",\n",
    "    split=\"train\",\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.to_json(\"cfgs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 21, 34, 19, 13, 40, 47, 28, 12, 41, 35, 5, 10, 43, 3, 23, 6, 26, 42, 54, 52, 51, 59, 36, 39, 44, 11, 49, 4, 27, 32, 38, 33, 1, 25, 60, 48, 17, 50, 57, 29, 58, 55, 14, 22, 46, 30, 24, 9, 31, 16, 8, 56, 7, 45, 18, 15, 2, 37, 53], [34, 42, 27, 19, 50, 51, 52, 35, 41, 48, 49, 56, 36, 58, 11, 60, 20, 40, 57, 30, 24, 28, 59, 2, 21, 44, 22, 12, 13, 26, 4, 14, 15, 23, 5, 43, 33, 32, 10, 8, 16, 47, 55, 54, 46, 45, 37, 7, 53, 31, 39, 9, 29, 38, 25, 18, 3, 6, 17, 1], [20, 21, 22, 13, 14, 19, 5, 15, 29, 11, 16, 8, 28, 6, 27, 33, 4, 3, 18, 7, 2, 12, 41, 10, 32, 23, 9, 37, 24, 40, 39, 50, 25, 17, 59, 47, 1, 42, 51, 36, 54, 55, 35, 30, 34, 43, 52, 26, 48, 60, 56, 46, 45, 57, 38, 58, 31, 49, 44, 53], [34, 28, 21, 12, 23, 42, 13, 22, 33, 39, 41, 14, 43, 50, 29, 27, 40, 49, 4, 24, 16, 20, 57, 48, 19, 30, 18, 51, 32, 52, 5, 26, 47, 9, 60, 37, 31, 35, 17, 38, 7, 6, 58, 3, 45, 56, 2, 59, 44, 8, 36, 15, 11, 25, 46, 10, 55, 53, 1, 54], [20, 19, 41, 21, 11, 28, 10, 40, 39, 49, 34, 38, 14, 13, 29, 27, 48, 3, 2, 50, 46, 15, 4, 35, 59, 23, 32, 37, 24, 43, 25, 51, 6, 1, 52, 30, 36, 12, 16, 56, 5, 22, 18, 8, 55, 9, 57, 42, 26, 44, 47, 53, 45, 7, 33, 31, 17, 60, 54, 58], [20, 19, 34, 42, 18, 12, 4, 28, 29, 10, 35, 5, 1, 30, 50, 3, 43, 49, 48, 26, 41, 17, 44, 56, 57, 21, 23, 15, 24, 16, 36, 33, 8, 22, 51, 39, 11, 60, 6, 40, 59, 7, 25, 58, 55, 54, 46, 53, 13, 31, 32, 27, 9, 52, 14, 2, 37, 38, 45, 47], [34, 28, 23, 41, 48, 49, 22, 47, 57, 29, 20, 14, 21, 50, 30, 11, 59, 15, 46, 35, 42, 43, 19, 40, 36, 52, 8, 44, 51, 56, 3, 27, 18, 39, 32, 54, 60, 33, 5, 45, 53, 55, 37, 38, 26, 6, 7, 10, 17, 2, 13, 4, 25, 58, 9, 12, 1, 24, 16, 31], [27, 33, 39, 21, 42, 41, 40, 38, 26, 46, 34, 51, 13, 25, 37, 47, 32, 48, 20, 14, 43, 50, 60, 52, 17, 9, 22, 35, 44, 36, 49, 12, 3, 4, 57, 31, 30, 45, 7, 2, 53, 29, 24, 28, 19, 23, 6, 15, 5, 55, 8, 11, 56, 18, 10, 59, 58, 16, 1, 54], [20, 33, 42, 21, 19, 34, 13, 50, 41, 48, 38, 10, 39, 32, 18, 45, 2, 5, 51, 52, 43, 1, 58, 3, 46, 11, 37, 54, 40, 25, 36, 47, 12, 27, 26, 31, 29, 35, 28, 23, 30, 49, 60, 59, 15, 7, 14, 16, 56, 57, 17, 55, 4, 9, 44, 22, 53, 6, 8, 24], [20, 19, 41, 40, 39, 42, 27, 49, 10, 38, 34, 22, 51, 11, 57, 28, 12, 2, 33, 13, 3, 48, 14, 15, 29, 35, 46, 30, 37, 1, 55, 58, 21, 26, 56, 45, 9, 18, 23, 5, 8, 53, 16, 43, 59, 4, 36, 60, 44, 54, 6, 50, 17, 52, 32, 24, 25, 47, 7, 31], [20, 21, 34, 41, 14, 27, 49, 7, 22, 35, 32, 50, 51, 11, 6, 5, 19, 26, 42, 33, 2, 23, 13, 57, 40, 28, 39, 37, 24, 12, 56, 55, 31, 10, 4, 47, 38, 30, 9, 3, 45, 48, 18, 46, 36, 15, 54, 60, 43, 44, 29, 25, 8, 16, 52, 17, 58, 59, 1, 53], [27, 19, 11, 21, 20, 10, 34, 41, 9, 3, 2, 17, 48, 39, 22, 57, 4, 29, 33, 23, 1, 18, 47, 46, 14, 6, 43, 32, 42, 28, 25, 35, 38, 13, 31, 44, 54, 45, 24, 30, 40, 56, 49, 26, 7, 15, 36, 50, 53, 8, 37, 12, 55, 16, 52, 51, 59, 60, 58, 5], [34, 28, 20, 42, 23, 33, 43, 22, 51, 11, 2, 50, 39, 24, 35, 19, 14, 47, 46, 6, 41, 27, 18, 9, 55, 4, 32, 38, 15, 56, 17, 7, 21, 25, 59, 58, 3, 52, 49, 48, 29, 57, 37, 13, 5, 10, 1, 36, 45, 26, 31, 12, 44, 54, 53, 40, 30, 16, 8, 60], [27, 33, 40, 47, 42, 19, 54, 51, 28, 29, 60, 34, 20, 21, 32, 37, 22, 43, 18, 23, 52, 17, 15, 35, 9, 14, 24, 49, 5, 38, 45, 46, 50, 53, 30, 13, 56, 1, 4, 6, 41, 55, 11, 58, 59, 44, 26, 25, 57, 2, 31, 7, 16, 10, 3, 8, 12, 48, 36, 39], [27, 21, 28, 33, 40, 18, 13, 48, 38, 32, 56, 49, 31, 20, 26, 46, 39, 23, 41, 6, 16, 14, 57, 50, 43, 59, 51, 55, 53, 58, 12, 34, 42, 35, 52, 11, 10, 9, 19, 25, 47, 22, 7, 54, 37, 44, 36, 15, 1, 8, 2, 30, 5, 4, 24, 29, 17, 45, 60, 3], [27, 21, 22, 33, 41, 49, 39, 40, 50, 38, 28, 20, 48, 57, 11, 29, 34, 23, 47, 43, 37, 58, 14, 56, 55, 15, 51, 31, 45, 46, 54, 19, 26, 6, 59, 2, 25, 35, 30, 32, 10, 53, 12, 24, 18, 3, 52, 60, 13, 4, 16, 44, 42, 8, 36, 17, 7, 5, 9, 1], [27, 19, 20, 33, 10, 21, 39, 26, 22, 47, 34, 42, 31, 11, 12, 25, 50, 51, 46, 49, 41, 5, 4, 13, 28, 2, 3, 14, 17, 53, 35, 9, 15, 8, 23, 38, 7, 18, 55, 44, 29, 30, 57, 56, 16, 24, 36, 58, 60, 43, 1, 40, 59, 32, 48, 52, 45, 6, 37, 54], [20, 33, 42, 12, 27, 18, 4, 34, 26, 50, 9, 21, 39, 46, 41, 40, 48, 55, 14, 47, 43, 17, 59, 29, 38, 10, 24, 22, 54, 53, 13, 49, 57, 37, 25, 28, 2, 35, 44, 51, 52, 36, 30, 58, 56, 7, 19, 60, 31, 5, 15, 8, 45, 32, 6, 3, 23, 16, 11, 1], [27, 33, 42, 19, 10, 20, 11, 51, 40, 28, 39, 12, 5, 4, 60, 6, 34, 9, 2, 26, 17, 48, 29, 25, 47, 18, 57, 21, 14, 55, 46, 35, 38, 23, 43, 31, 32, 52, 16, 30, 22, 41, 36, 56, 15, 24, 50, 45, 37, 58, 13, 7, 1, 44, 49, 3, 59, 8, 54, 53], [34, 28, 21, 40, 35, 23, 27, 18, 26, 12, 42, 32, 39, 41, 22, 36, 13, 48, 44, 43, 51, 52, 47, 46, 53, 33, 54, 60, 59, 29, 24, 14, 3, 49, 6, 30, 19, 16, 56, 58, 9, 38, 5, 57, 15, 11, 7, 8, 31, 50, 10, 37, 20, 25, 17, 55, 45, 4, 1, 2], [27, 33, 41, 34, 38, 48, 49, 18, 19, 20, 21, 50, 11, 4, 43, 32, 31, 52, 22, 37, 9, 46, 12, 25, 17, 14, 13, 5, 35, 10, 57, 40, 36, 15, 28, 23, 7, 29, 24, 3, 26, 6, 54, 45, 42, 44, 53, 47, 1, 30, 55, 2, 16, 59, 58, 51, 60, 8, 39, 56], [20, 33, 41, 12, 27, 40, 19, 50, 21, 13, 5, 32, 28, 4, 3, 18, 10, 29, 48, 1, 38, 46, 22, 55, 9, 14, 59, 11, 6, 26, 36, 30, 35, 23, 39, 44, 17, 49, 37, 51, 53, 25, 42, 45, 43, 52, 57, 54, 24, 2, 31, 15, 7, 34, 47, 56, 60, 16, 8, 58], [20, 21, 42, 27, 18, 19, 11, 39, 28, 34, 22, 29, 30, 9, 46, 50, 40, 13, 35, 43, 51, 2, 4, 33, 49, 47, 32, 52, 3, 24, 54, 5, 36, 48, 56, 57, 14, 26, 58, 7, 44, 12, 15, 38, 31, 60, 41, 10, 17, 37, 45, 53, 16, 6, 1, 59, 8, 55, 23, 25], [34, 42, 41, 28, 43, 52, 44, 50, 29, 40, 58, 24, 27, 33, 60, 49, 26, 32, 39, 20, 30, 38, 11, 18, 46, 22, 19, 53, 9, 2, 13, 51, 21, 59, 23, 15, 31, 57, 16, 12, 47, 55, 3, 45, 35, 4, 6, 25, 14, 5, 48, 37, 56, 8, 1, 7, 10, 36, 54, 17], [20, 19, 27, 33, 10, 22, 34, 11, 40, 13, 15, 1, 28, 43, 26, 42, 35, 48, 39, 23, 49, 32, 21, 50, 52, 17, 2, 3, 57, 7, 38, 46, 37, 31, 25, 14, 54, 53, 12, 36, 24, 30, 5, 55, 56, 47, 29, 44, 4, 41, 8, 60, 45, 59, 58, 6, 16, 18, 9, 51], [41, 28, 19, 39, 29, 50, 46, 27, 33, 21, 22, 32, 25, 10, 13, 23, 15, 47, 38, 5, 1, 8, 55, 30, 35, 18, 7, 42, 4, 53, 51, 40, 17, 24, 16, 48, 56, 57, 34, 36, 45, 49, 58, 3, 44, 43, 26, 37, 20, 31, 52, 54, 6, 11, 9, 12, 2, 14, 60, 59], [20, 21, 42, 39, 34, 19, 33, 43, 12, 5, 46, 50, 22, 32, 27, 11, 35, 36, 4, 48, 10, 3, 41, 14, 28, 23, 15, 7, 52, 1, 58, 40, 26, 57, 9, 17, 6, 49, 25, 2, 55, 18, 24, 59, 13, 47, 29, 45, 44, 60, 38, 37, 54, 30, 16, 56, 8, 53, 31, 51], [34, 42, 20, 33, 32, 19, 18, 10, 41, 17, 1, 28, 29, 2, 51, 60, 43, 40, 3, 50, 26, 38, 47, 56, 46, 23, 15, 44, 21, 22, 52, 16, 58, 30, 11, 27, 49, 9, 35, 54, 31, 7, 14, 37, 39, 48, 59, 4, 53, 25, 24, 5, 55, 13, 57, 36, 6, 8, 12, 45], [27, 21, 14, 26, 20, 33, 42, 34, 41, 11, 40, 7, 32, 48, 47, 50, 13, 22, 28, 46, 43, 23, 18, 9, 58, 4, 2, 49, 57, 29, 19, 59, 24, 37, 54, 39, 60, 17, 38, 30, 6, 15, 35, 44, 25, 53, 31, 3, 12, 16, 8, 56, 55, 1, 5, 36, 52, 10, 45, 51], [41, 28, 22, 40, 27, 26, 39, 33, 17, 50, 20, 32, 37, 21, 59, 48, 56, 46, 29, 57, 13, 18, 34, 30, 23, 11, 55, 42, 9, 53, 36, 38, 47, 24, 15, 25, 12, 14, 49, 1, 51, 52, 10, 2, 54, 5, 31, 45, 58, 44, 43, 3, 19, 16, 4, 60, 8, 35, 7, 6], [20, 21, 28, 33, 13, 14, 22, 29, 41, 42, 6, 7, 34, 12, 8, 23, 24, 36, 50, 35, 5, 58, 38, 32, 43, 48, 49, 45, 55, 27, 30, 4, 26, 17, 11, 16, 39, 47, 44, 18, 10, 19, 25, 2, 15, 40, 59, 60, 1, 51, 37, 3, 56, 57, 31, 46, 53, 52, 9, 54], [27, 19, 20, 33, 18, 12, 41, 40, 13, 26, 32, 31, 4, 28, 11, 39, 47, 5, 17, 21, 14, 46, 34, 22, 45, 3, 2, 25, 10, 7, 35, 42, 51, 6, 49, 1, 23, 60, 37, 55, 29, 16, 50, 48, 38, 53, 58, 59, 24, 30, 15, 43, 54, 9, 36, 56, 8, 44, 52, 57]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Must provide a tokenizer if passing a string to the model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_tokens \u001b[38;5;241m=\u001b[39m token_dataset[:\u001b[38;5;241m32\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_tokens)\n\u001b[0;32m----> 8\u001b[0m _, cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Use the SAE\u001b[39;00m\n\u001b[1;32m     11\u001b[0m feature_acts \u001b[38;5;241m=\u001b[39m sae\u001b[38;5;241m.\u001b[39mencode(cache[sae\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mhook_name])\n",
      "File \u001b[0;32m/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/.venv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:627\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    612\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    620\u001b[0m ]:\n\u001b[1;32m    621\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    631\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
      "File \u001b[0;32m/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/.venv/lib/python3.11/site-packages/transformer_lens/hook_points.py:513\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    500\u001b[0m     names_filter,\n\u001b[1;32m    501\u001b[0m     incl_bwd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m     pos_slice\u001b[38;5;241m=\u001b[39mpos_slice,\n\u001b[1;32m    505\u001b[0m )\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    508\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    509\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    510\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    511\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    512\u001b[0m ):\n\u001b[0;32m--> 513\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    515\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/.venv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:522\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mLocallyOverridenDefaults(\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28mself\u001b[39m, prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos, padding_side\u001b[38;5;241m=\u001b[39mpadding_side\n\u001b[1;32m    515\u001b[0m ):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m         (\n\u001b[1;32m    518\u001b[0m             residual,\n\u001b[1;32m    519\u001b[0m             tokens,\n\u001b[1;32m    520\u001b[0m             shortformer_pos_embed,\n\u001b[1;32m    521\u001b[0m             attention_mask,\n\u001b[0;32m--> 522\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_to_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\n",
      "File \u001b[0;32m/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/.venv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:274\u001b[0m, in \u001b[0;36mHookedTransformer.input_to_embed\u001b[0;34m(self, input, prepend_bos, padding_side, past_kv_cache)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert input to first residual stream.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m        and attention_mask will be stored in the cache.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# If text, convert to tokens (batch_size=1)\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust provide a tokenizer if passing a string to the model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# This is only intended to support passing in a single string\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tokens(\u001b[38;5;28minput\u001b[39m, prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos, padding_side\u001b[38;5;241m=\u001b[39mpadding_side)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Must provide a tokenizer if passing a string to the model"
     ]
    }
   ],
   "source": [
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:32][\"tokens\"]\n",
    "    print(batch_tokens)\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=False)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
