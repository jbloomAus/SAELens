{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import transformer_lens\n",
    "importlib.reload(transformer_lens)\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.train import HookedTransformerTrainConfig, train\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import os\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create huggingface data with othello games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 100/100 [00:00<00:00, 716.01ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': tensor([20, 21, 28, 23, 13,  5, 34, 19, 16, 43, 14, 30, 22, 40, 47, 48, 33, 54,\n",
      "        52, 42, 41, 49, 50, 39, 29, 58, 38, 36, 18, 12, 57, 56, 55, 24, 44, 10,\n",
      "        59, 26, 11, 37, 25, 27, 31,  2, 17, 32,  9, 35, 45, 60,  3, 46,  6,  4,\n",
      "        53,  1, 15,  7, 51])}\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = t.tensor(np.load(\"data/board_seqs_int_small.npy\"), dtype=t.long)\n",
    "tokenized_data = tokenized_data[:, :59] # remove XX at the end\n",
    "\n",
    "data_dict = {\"tokens\": tokenized_data.tolist()}\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "dataset.set_format(type=\"torch\", columns=[\"tokens\"])\n",
    "\n",
    "# upload to Hugging Face\n",
    "access_token = os.environ[\"HF_TOKEN\"]\n",
    "dataset.push_to_hub(\"Thijmen/othello_dataset\", \"main\", token=access_token)\n",
    "\n",
    "# print a row of data\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model_cfg = HookedTransformerConfig(\n",
    "    n_layers = 6,\n",
    "    d_model = 128,\n",
    "    d_head = 64,\n",
    "    n_heads = 8,\n",
    "    d_mlp = 128*4,\n",
    "    d_vocab = 61,\n",
    "    n_ctx = 59,\n",
    "    act_fn=\"gelu\",\n",
    "    normalization_type=\"LN\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model = HookedTransformer(model_cfg).to(device)\n",
    "\n",
    "\n",
    "train_cfg = HookedTransformerTrainConfig(\n",
    "    lr=1e-4,\n",
    "    batch_size=512,\n",
    "    num_epochs=1,\n",
    "    device=device,\n",
    "    wandb=True,\n",
    "    wandb_project_name=\"OthelloTraining\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model and save locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthijmen-nijdam\u001b[0m (\u001b[33mjkbkaiser1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/wandb/run-20240610_163641-vqp88w4i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jkbkaiser1/OthelloTraining/runs/vqp88w4i' target=\"_blank\">smooth-deluge-17</a></strong> to <a href='https://wandb.ai/jkbkaiser1/OthelloTraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jkbkaiser1/OthelloTraining' target=\"_blank\">https://wandb.ai/jkbkaiser1/OthelloTraining</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jkbkaiser1/OthelloTraining/runs/vqp88w4i' target=\"_blank\">https://wandb.ai/jkbkaiser1/OthelloTraining/runs/vqp88w4i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Samples 512 Step 0 Loss 4.42979097366333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Samples 26112 Step 50 Loss 3.8277058601379395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Samples 51712 Step 100 Loss 3.5579421520233154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Samples 77312 Step 150 Loss 3.3707311153411865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [01:01,  3.18it/s]\n",
      "100%|██████████| 1/1 [01:01<00:00, 61.72s/it]\n"
     ]
    }
   ],
   "source": [
    "train(model, train_cfg, dataset)\n",
    "t.save(model.state_dict(), f\"othello_gpt.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model and verify that model can be loaded using TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer(model_cfg).to(device)\n",
    "model.load_and_process_state_dict(t.load(f\"othello_gpt.pth\"))\n",
    "print(model.tokenizer) # model has no tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload model to huggingface\n",
    "\n",
    "I upload the othello_pth.pth to huggingface manually and created a config.json for it\n",
    "\n",
    "See: https://huggingface.co/Thijmen/othello-GPT-model\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "  \"n_layers\": 6,\n",
    "  \"d_model\": 128,\n",
    "  \"d_head\": 64,\n",
    "  \"n_heads\": 8,\n",
    "  \"d_mlp\": 512,\n",
    "  \"d_vocab\": 61,\n",
    "  \"n_ctx\": 59,\n",
    "  \"act_fn\": \"gelu\",\n",
    "  \"normalization_type\": \"LN\",\n",
    "  \"attn_only\": false,\n",
    "  \"architecture\": \"mingpt\",\n",
    "  \"model_type\": \"gpt2\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Added my model to offical list in the TransformerLens library\n",
    "\n",
    "In the file transformerlens/loading_from_pretrained.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SAE\n",
    "\n",
    "Getting error as model does not seem to be in the right format. I should probably train it with huggingface library? Not sure what I need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 512-L1-5-LR-5e-05-Tokens-1.229e+08\n",
      "n_tokens_per_buffer (millions): 0.003072\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 30000\n",
      "Total wandb updates: 1000\n",
      "n_tokens_per_feature_sampling_window (millions): 12.288\n",
      "n_tokens_per_dead_feature_window (millions): 12.288\n",
      "We will reset the sparsity calculation 30 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 63\u001b[0m\n\u001b[1;32m      9\u001b[0m cfg \u001b[38;5;241m=\u001b[39m LanguageModelSAERunnerConfig(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Data Generating Function (Model + Training Distibuion)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThijmen/othello-GPT-model\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# added this to offical list in TransformerLens library\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# look at the next cell to see some instruction for what to do while this is running.\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m sparse_autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mSAETrainingRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/sae_lens/sae_training_runner.py:41\u001b[0m, in \u001b[0;36mSAETrainingRunner.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: LanguageModelSAERunnerConfig):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_class_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_from_pretrained_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_from_pretrained_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations_store \u001b[38;5;241m=\u001b[39m ActivationsStore\u001b[38;5;241m.\u001b[39mfrom_config(\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg,\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mfrom_pretrained_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/sae_lens/load_model.py:17\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_class_name, model_name, device, model_from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m model_from_pretrained_kwargs \u001b[38;5;241m=\u001b[39m model_from_pretrained_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_class_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHookedTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHookedTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_from_pretrained_kwargs\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_class_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHookedMamba\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/.venv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:1238\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1233\u001b[0m official_model_name \u001b[38;5;241m=\u001b[39m loading\u001b[38;5;241m.\u001b[39mget_official_model_name(model_name)\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;66;03m# Load the config into an HookedTransformerConfig object. If loading from a\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# checkpoint, the config object will contain the information about the\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;66;03m# checkpoint\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mloading\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pretrained_model_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mofficial_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_devices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_prepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_prepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfrom_pretrained_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mpositional_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshortformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fold_ln:\n",
      "File \u001b[0;32m/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/.venv/lib/python3.11/site-packages/transformer_lens/loading_from_pretrained.py:1261\u001b[0m, in \u001b[0;36mget_pretrained_model_config\u001b[0;34m(model_name, hf_cfg, checkpoint_index, checkpoint_value, fold_ln, device, n_devices, default_prepend_bos, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1258\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mofficial_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires setting trust_remote_code=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1259\u001b[0m         )\n\u001b[1;32m   1260\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1261\u001b[0m     cfg_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_hf_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mofficial_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;66;03m# Processing common to both model types\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;66;03m# Remove any prefix, saying the organization who made a model.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m cfg_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m official_model_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/mnt/c/Users/thijm/GitHub/Master_related/ProjectAI_related/OthelloGPT-SAELens/.venv/lib/python3.11/site-packages/transformer_lens/loading_from_pretrained.py:674\u001b[0m, in \u001b[0;36mconvert_hf_model_config\u001b[0;34m(model_name, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     huggingface_token \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    669\u001b[0m     hf_config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    670\u001b[0m         official_model_name,\n\u001b[1;32m    671\u001b[0m         token\u001b[38;5;241m=\u001b[39mhuggingface_token,\n\u001b[1;32m    672\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    673\u001b[0m     )\n\u001b[0;32m--> 674\u001b[0m     architecture \u001b[38;5;241m=\u001b[39m \u001b[43mhf_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marchitectures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m official_model_name\u001b[38;5;241m.\u001b[39mstartswith(\n\u001b[1;32m    677\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    678\u001b[0m ):  \u001b[38;5;66;03m# same architecture for LLaMA and Llama-2\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     cfg_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    680\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_head\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4096\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgated_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    695\u001b[0m     }\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "total_training_steps = 30_000  # probably we should do more\n",
    "batch_size = 4096\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "l1_warm_up_steps = total_training_steps // 20  # 5% of training\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"Thijmen/othello-GPT-model\",  # added this to offical list in TransformerLens library\n",
    "    hook_name=\"blocks.0.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n",
    "    hook_layer=0,  # Only one layer in the model.\n",
    "    d_in=128*4,  # the width of the mlp output.\n",
    "    dataset_path=\"Thijmen/othello_dataset\",  # my own dataset which i created in this file and uploaded to HF.\n",
    "    is_dataset_tokenized=True, # dataset is tokenized, although i saw this flag is not in use anymore\n",
    "    streaming=False,  # we could pre-download the token dataset if it was small.\n",
    "    prepend_bos=False,\n",
    "    # SAE Parameters\n",
    "    mse_loss_normalization=None,  # We won't normalize the mse loss,\n",
    "    expansion_factor=1,  # the width of the SAE. Larger will result in better stats but slower training.\n",
    "    b_dec_init_method=\"zeros\",  # The geometric median can be used to initialize the decoder weights.\n",
    "    apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.\n",
    "    normalize_sae_decoder=False,\n",
    "    scale_sparsity_penalty_by_decoder_norm=True,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    normalize_activations=True,\n",
    "    # Training Parameters\n",
    "    lr=5e-5,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
    "    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.\n",
    "    l1_coefficient=5,  # will control how sparse the feature activations are\n",
    "    l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    context_size=3,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n",
    "    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    store_batch_size_prompts=16,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=False,  # we don't use ghost grads anymore.\n",
    "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
    "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
    "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
    "    # WANDB\n",
    "    log_to_wandb=False,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"sae_lens_tutorial\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "# look at the next cell to see some instruction for what to do while this is running.\n",
    "sparse_autoencoder = SAETrainingRunner(cfg).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
