{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>SAELens v6</p> <p>SAELens 6.0.0 is live with changes to SAE training and loading. Check out the migration guide \u2192</p> <p></p>"},{"location":"#saelens","title":"SAELens","text":"<p>The SAELens training codebase exists to help researchers:</p> <ul> <li>Train sparse autoencoders.</li> <li>Analyse sparse autoencoders and neural network internals.</li> <li>Generate insights which make it easier to create safe and aligned AI systems.</li> </ul> <p>Please note these docs are in beta. We intend to make them cleaner and more comprehensive over time.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install sae-lens\n</code></pre>"},{"location":"#loading-sparse-autoencoders-from-huggingface","title":"Loading Sparse Autoencoders from Huggingface","text":"<p>To load a pretrained sparse autoencoder, you can use <code>SAE.from_pretrained()</code> as below:</p> <pre><code>from sae_lens import SAE\n\nsae = SAE.from_pretrained(\n    release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n    sae_id = \"blocks.8.hook_resid_pre\", # won't always be a hook point\n    device = \"cuda\"\n)\n</code></pre> <p>You can see other importable SAEs on this page.</p> <p>Any SAE on Huggingface that's trained using SAELens can also be loaded using <code>SAE.from_pretrained()</code>. In this case, <code>release</code> is the name of the Huggingface repo, and <code>sae_id</code> is the path to the SAE in the repo. You can see a list of SAEs listed on Huggingface with the saelens tag.</p>"},{"location":"#loading-sparse-autoencoders-from-disk","title":"Loading Sparse Autoencoders from Disk","text":"<p>To load a pretrained sparse autoencoder from disk that you've trained yourself, you can use <code>SAE.load_from_disk()</code> as below.</p> <pre><code>from sae_lens import SAE\n\nsae = SAE.load_from_disk(\"/path/to/your/sae\", device=\"cuda\")\n</code></pre>"},{"location":"#importing-saes-from-other-libraries","title":"Importing SAEs from other libraries","text":"<p>You can import an SAE created with another library by writing a custom <code>PretrainedSaeHuggingfaceLoader</code> or <code>PretrainedSaeDiskLoader</code> for use with <code>SAE.from_pretrained()</code> or <code>SAE.load_from_disk()</code>, respectively. See the pretrained_sae_loaders.py file for more details, or ask on the Open Source Mechanistic Interpretability Slack. If you write a good custom loader for another library, please consider contributing it back to SAELens!</p>"},{"location":"#background-and-further-readings","title":"Background and further Readings","text":"<p>We highly recommend this tutorial.</p> <p>For recent progress in SAEs, we recommend the LessWrong forum's Sparse Autoencoder tag</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>I wrote a tutorial to show users how to do some basic exploration of their SAE:</p> <ul> <li>Loading and Analysing Pre-Trained Sparse Autoencoders </li> <li>Understanding SAE Features with the Logit Lens </li> <li>Training a Sparse Autoencoder </li> </ul>"},{"location":"#example-wandb-dashboard","title":"Example WandB Dashboard","text":"<p>WandB Dashboards provide lots of useful insights while training SAEs. Here's a screenshot from one training run.</p> <p></p>"},{"location":"#citation","title":"Citation","text":"<pre><code>@misc{bloom2024saetrainingcodebase,\n   title = {SAELens},\n   author = {Bloom, Joseph and Tigges, Curt and Duong, Anthony and Chanin, David},\n   year = {2024},\n   howpublished = {\\url{https://github.com/jbloomAus/SAELens}},\n}}\n</code></pre>"},{"location":"api/","title":"API","text":""},{"location":"api/#sae_lens.ActivationsStore","title":"<code>ActivationsStore</code>","text":"<p>Class for streaming tokens and generating and storing activations while training SAEs.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>class ActivationsStore:\n    \"\"\"\n    Class for streaming tokens and generating and storing activations\n    while training SAEs.\n    \"\"\"\n\n    model: HookedRootModule\n    dataset: HfDataset\n    cached_activations_path: str | None\n    cached_activation_dataset: Dataset | None = None\n    tokens_column: Literal[\"tokens\", \"input_ids\", \"text\", \"problem\"]\n    hook_name: str\n    hook_head_index: int | None\n    _dataloader: Iterator[Any] | None = None\n    exclude_special_tokens: torch.Tensor | None = None\n    device: torch.device\n\n    @classmethod\n    def from_cache_activations(\n        cls,\n        model: HookedRootModule,\n        cfg: CacheActivationsRunnerConfig,\n    ) -&gt; ActivationsStore:\n        \"\"\"\n        Public api to create an ActivationsStore from a cached activations dataset.\n        \"\"\"\n        return cls(\n            cached_activations_path=cfg.new_cached_activations_path,\n            dtype=cfg.dtype,\n            hook_name=cfg.hook_name,\n            context_size=cfg.context_size,\n            d_in=cfg.d_in,\n            n_batches_in_buffer=cfg.n_batches_in_buffer,\n            total_training_tokens=cfg.training_tokens,\n            store_batch_size_prompts=cfg.model_batch_size,  # get_buffer\n            train_batch_size_tokens=cfg.model_batch_size,  # dataloader\n            seqpos_slice=(None,),\n            device=torch.device(cfg.device),  # since we're sending these to SAE\n            # NOOP\n            prepend_bos=False,\n            hook_head_index=None,\n            dataset=cfg.dataset_path,\n            streaming=False,\n            model=model,\n            normalize_activations=\"none\",\n            model_kwargs=None,\n            autocast_lm=False,\n            dataset_trust_remote_code=None,\n            exclude_special_tokens=None,\n        )\n\n    @classmethod\n    def from_config(\n        cls,\n        model: HookedRootModule,\n        cfg: LanguageModelSAERunnerConfig[T_TRAINING_SAE_CONFIG]\n        | CacheActivationsRunnerConfig,\n        override_dataset: HfDataset | None = None,\n    ) -&gt; ActivationsStore:\n        if isinstance(cfg, CacheActivationsRunnerConfig):\n            return cls.from_cache_activations(model, cfg)\n\n        cached_activations_path = cfg.cached_activations_path\n        # set cached_activations_path to None if we're not using cached activations\n        if (\n            isinstance(cfg, LanguageModelSAERunnerConfig)\n            and not cfg.use_cached_activations\n        ):\n            cached_activations_path = None\n\n        if override_dataset is None and cfg.dataset_path == \"\":\n            raise ValueError(\n                \"You must either pass in a dataset or specify a dataset_path in your configutation.\"\n            )\n\n        device = torch.device(cfg.act_store_device)\n        exclude_special_tokens = cfg.exclude_special_tokens\n        if exclude_special_tokens is False:\n            exclude_special_tokens = None\n        if exclude_special_tokens is True:\n            exclude_special_tokens = _get_special_token_ids(model.tokenizer)  # type: ignore\n        if exclude_special_tokens is not None:\n            exclude_special_tokens = torch.tensor(\n                exclude_special_tokens, dtype=torch.long, device=device\n            )\n        return cls(\n            model=model,\n            dataset=override_dataset or cfg.dataset_path,\n            streaming=cfg.streaming,\n            hook_name=cfg.hook_name,\n            hook_head_index=cfg.hook_head_index,\n            context_size=cfg.context_size,\n            d_in=cfg.d_in\n            if isinstance(cfg, CacheActivationsRunnerConfig)\n            else cfg.sae.d_in,\n            n_batches_in_buffer=cfg.n_batches_in_buffer,\n            total_training_tokens=cfg.training_tokens,\n            store_batch_size_prompts=cfg.store_batch_size_prompts,\n            train_batch_size_tokens=cfg.train_batch_size_tokens,\n            prepend_bos=cfg.prepend_bos,\n            normalize_activations=cfg.sae.normalize_activations,\n            device=device,\n            dtype=cfg.dtype,\n            cached_activations_path=cached_activations_path,\n            model_kwargs=cfg.model_kwargs,\n            autocast_lm=cfg.autocast_lm,\n            dataset_trust_remote_code=cfg.dataset_trust_remote_code,\n            seqpos_slice=cfg.seqpos_slice,\n            exclude_special_tokens=exclude_special_tokens,\n            disable_concat_sequences=cfg.disable_concat_sequences,\n            sequence_separator_token=cfg.sequence_separator_token,\n        )\n\n    @classmethod\n    def from_sae(\n        cls,\n        model: HookedRootModule,\n        sae: SAE[T_SAE_CONFIG],\n        dataset: HfDataset | str,\n        dataset_trust_remote_code: bool = False,\n        context_size: int | None = None,\n        streaming: bool = True,\n        store_batch_size_prompts: int = 8,\n        n_batches_in_buffer: int = 8,\n        train_batch_size_tokens: int = 4096,\n        total_tokens: int = 10**9,\n        device: str = \"cpu\",\n        disable_concat_sequences: bool = False,\n        sequence_separator_token: int | Literal[\"bos\", \"eos\", \"sep\"] | None = \"bos\",\n    ) -&gt; ActivationsStore:\n        if sae.cfg.metadata.hook_name is None:\n            raise ValueError(\"hook_name is required\")\n        if sae.cfg.metadata.context_size is None:\n            raise ValueError(\"context_size is required\")\n        if sae.cfg.metadata.prepend_bos is None:\n            raise ValueError(\"prepend_bos is required\")\n        return cls(\n            model=model,\n            dataset=dataset,\n            d_in=sae.cfg.d_in,\n            hook_name=sae.cfg.metadata.hook_name,\n            hook_head_index=sae.cfg.metadata.hook_head_index,\n            context_size=sae.cfg.metadata.context_size\n            if context_size is None\n            else context_size,\n            prepend_bos=sae.cfg.metadata.prepend_bos,\n            streaming=streaming,\n            store_batch_size_prompts=store_batch_size_prompts,\n            train_batch_size_tokens=train_batch_size_tokens,\n            n_batches_in_buffer=n_batches_in_buffer,\n            total_training_tokens=total_tokens,\n            normalize_activations=sae.cfg.normalize_activations,\n            dataset_trust_remote_code=dataset_trust_remote_code,\n            dtype=sae.cfg.dtype,\n            device=torch.device(device),\n            seqpos_slice=sae.cfg.metadata.seqpos_slice or (None,),\n            disable_concat_sequences=disable_concat_sequences,\n            sequence_separator_token=sequence_separator_token,\n        )\n\n    def __init__(\n        self,\n        model: HookedRootModule,\n        dataset: HfDataset | str,\n        streaming: bool,\n        hook_name: str,\n        hook_head_index: int | None,\n        context_size: int,\n        d_in: int,\n        n_batches_in_buffer: int,\n        total_training_tokens: int,\n        store_batch_size_prompts: int,\n        train_batch_size_tokens: int,\n        prepend_bos: bool,\n        normalize_activations: str,\n        device: torch.device,\n        dtype: str,\n        cached_activations_path: str | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n        autocast_lm: bool = False,\n        dataset_trust_remote_code: bool | None = None,\n        seqpos_slice: tuple[int | None, ...] = (None,),\n        exclude_special_tokens: torch.Tensor | None = None,\n        disable_concat_sequences: bool = False,\n        sequence_separator_token: int | Literal[\"bos\", \"eos\", \"sep\"] | None = \"bos\",\n    ):\n        self.model = model\n        if model_kwargs is None:\n            model_kwargs = {}\n        self.model_kwargs = model_kwargs\n        self.dataset = (\n            load_dataset(\n                dataset,\n                split=\"train\",\n                streaming=streaming,\n                trust_remote_code=dataset_trust_remote_code,  # type: ignore\n            )\n            if isinstance(dataset, str)\n            else dataset\n        )\n\n        if isinstance(dataset, (Dataset, DatasetDict)):\n            self.dataset = cast(Dataset | DatasetDict, self.dataset)\n            n_samples = len(self.dataset)\n\n            if n_samples &lt; total_training_tokens:\n                warnings.warn(\n                    f\"The training dataset contains fewer samples ({n_samples}) than the number of samples required by your training configuration ({total_training_tokens}). This will result in multiple training epochs and some samples being used more than once.\"\n                )\n\n        self.hook_name = hook_name\n        self.hook_head_index = hook_head_index\n        self.context_size = context_size\n        self.d_in = d_in\n        self.n_batches_in_buffer = n_batches_in_buffer\n        self.half_buffer_size = n_batches_in_buffer // 2\n        self.total_training_tokens = total_training_tokens\n        self.store_batch_size_prompts = store_batch_size_prompts\n        self.train_batch_size_tokens = train_batch_size_tokens\n        self.prepend_bos = prepend_bos\n        self.normalize_activations = normalize_activations\n        self.device = torch.device(device)\n        self.dtype = DTYPE_MAP[dtype]\n        self.cached_activations_path = cached_activations_path\n        self.autocast_lm = autocast_lm\n        self.seqpos_slice = seqpos_slice\n        self.training_context_size = len(range(context_size)[slice(*seqpos_slice)])\n        self.exclude_special_tokens = exclude_special_tokens\n        self.disable_concat_sequences = disable_concat_sequences\n        self.sequence_separator_token: int | Literal[\"bos\", \"eos\", \"sep\"] | None = (\n            sequence_separator_token\n        )\n\n        self.n_dataset_processed = 0\n\n        # Check if dataset is tokenized\n        dataset_sample = next(iter(self.dataset))\n\n        # check if it's tokenized\n        if \"tokens\" in dataset_sample:\n            self.is_dataset_tokenized = True\n            self.tokens_column = \"tokens\"\n        elif \"input_ids\" in dataset_sample:\n            self.is_dataset_tokenized = True\n            self.tokens_column = \"input_ids\"\n        elif \"text\" in dataset_sample:\n            self.is_dataset_tokenized = False\n            self.tokens_column = \"text\"\n        elif \"problem\" in dataset_sample:\n            self.is_dataset_tokenized = False\n            self.tokens_column = \"problem\"\n        else:\n            raise ValueError(\n                \"Dataset must have a 'tokens', 'input_ids', 'text', or 'problem' column.\"\n            )\n        if self.is_dataset_tokenized:\n            ds_context_size = len(dataset_sample[self.tokens_column])  # type: ignore\n            if ds_context_size &lt; self.context_size:\n                raise ValueError(\n                    f\"\"\"pretokenized dataset has context_size {ds_context_size}, but the provided context_size is {self.context_size}.\n                    The context_size {ds_context_size} is expected to be larger than or equal to the provided context size {self.context_size}.\"\"\"\n                )\n            if self.context_size != ds_context_size:\n                warnings.warn(\n                    f\"\"\"pretokenized dataset has context_size {ds_context_size}, but the provided context_size is {self.context_size}. Some data will be discarded in this case.\"\"\",\n                    RuntimeWarning,\n                )\n            # TODO: investigate if this can work for iterable datasets, or if this is even worthwhile as a perf improvement\n            if hasattr(self.dataset, \"set_format\"):\n                self.dataset.set_format(type=\"torch\", columns=[self.tokens_column])  # type: ignore\n\n            if (\n                isinstance(dataset, str)\n                and hasattr(model, \"tokenizer\")\n                and model.tokenizer is not None\n            ):\n                validate_pretokenized_dataset_tokenizer(\n                    dataset_path=dataset,\n                    model_tokenizer=model.tokenizer,  # type: ignore\n                )\n        else:\n            warnings.warn(\n                \"Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\"\n            )\n\n        self.iterable_sequences = self._iterate_tokenized_sequences()\n\n        self.cached_activation_dataset = self.load_cached_activation_dataset()\n\n        # TODO add support for \"mixed loading\" (ie use cache until you run out, then switch over to streaming from HF)\n\n    def _iterate_raw_dataset(\n        self,\n    ) -&gt; Generator[torch.Tensor | list[int] | str, None, None]:\n        \"\"\"\n        Helper to iterate over the dataset while incrementing n_dataset_processed\n        \"\"\"\n        for row in self.dataset:\n            # typing datasets is difficult\n            yield row[self.tokens_column]  # type: ignore\n            self.n_dataset_processed += 1\n\n    def _iterate_raw_dataset_tokens(self) -&gt; Generator[torch.Tensor, None, None]:\n        \"\"\"\n        Helper to create an iterator which tokenizes raw text from the dataset on the fly\n        \"\"\"\n        for row in self._iterate_raw_dataset():\n            tokens = (\n                self.model.to_tokens(\n                    row,\n                    truncate=False,\n                    move_to_device=False,  # we move to device below\n                    prepend_bos=False,\n                )  # type: ignore\n                .squeeze(0)\n                .to(self.device)\n            )\n            if len(tokens.shape) != 1:\n                raise ValueError(f\"tokens.shape should be 1D but was {tokens.shape}\")\n            yield tokens\n\n    def _iterate_tokenized_sequences(self) -&gt; Generator[torch.Tensor, None, None]:\n        \"\"\"\n        Generator which iterates over full sequence of context_size tokens\n        \"\"\"\n        # If the datset is pretokenized, we will slice the dataset to the length of the context window if needed. Otherwise, no further processing is needed.\n        # We assume that all necessary BOS/EOS/SEP tokens have been added during pretokenization.\n        if self.is_dataset_tokenized:\n            for row in self._iterate_raw_dataset():\n                yield torch.tensor(\n                    row[\n                        : self.context_size\n                    ],  # If self.context_size = None, this line simply returns the whole row\n                    dtype=torch.long,\n                    device=self.device,\n                    requires_grad=False,\n                )\n        # If the dataset isn't tokenized, we'll tokenize, concat, and batch on the fly\n        else:\n            tokenizer = getattr(self.model, \"tokenizer\", None)\n            bos_token_id = None if tokenizer is None else tokenizer.bos_token_id\n\n            yield from concat_and_batch_sequences(\n                tokens_iterator=self._iterate_raw_dataset_tokens(),\n                context_size=self.context_size,\n                begin_batch_token_id=(bos_token_id if self.prepend_bos else None),\n                begin_sequence_token_id=None,\n                sequence_separator_token_id=get_special_token_from_cfg(\n                    self.sequence_separator_token, tokenizer\n                )\n                if tokenizer is not None\n                else None,\n                disable_concat_sequences=self.disable_concat_sequences,\n            )\n\n    def load_cached_activation_dataset(self) -&gt; Dataset | None:\n        \"\"\"\n        Load the cached activation dataset from disk.\n\n        - If cached_activations_path is set, returns Huggingface Dataset else None\n        - Checks that the loaded dataset has current has activations for hooks in config and that shapes match.\n        \"\"\"\n        if self.cached_activations_path is None:\n            return None\n\n        assert self.cached_activations_path is not None  # keep pyright happy\n        # Sanity check: does the cache directory exist?\n        if not os.path.exists(self.cached_activations_path):\n            raise FileNotFoundError(\n                f\"Cache directory {self.cached_activations_path} does not exist. \"\n                \"Consider double-checking your dataset, model, and hook names.\"\n            )\n\n        # ---\n        # Actual code\n        activations_dataset = datasets.load_from_disk(self.cached_activations_path)\n        columns = [self.hook_name]\n        if \"token_ids\" in activations_dataset.column_names:\n            columns.append(\"token_ids\")\n        activations_dataset.set_format(\n            type=\"torch\", columns=columns, device=self.device, dtype=self.dtype\n        )\n        self.current_row_idx = 0  # idx to load next batch from\n        # ---\n\n        assert isinstance(activations_dataset, Dataset)\n\n        # multiple in hooks future\n        if not set([self.hook_name]).issubset(activations_dataset.column_names):\n            raise ValueError(\n                f\"loaded dataset does not include hook activations, got {activations_dataset.column_names}\"\n            )\n\n        if activations_dataset.features[self.hook_name].shape != (\n            self.context_size,\n            self.d_in,\n        ):\n            raise ValueError(\n                f\"Given dataset of shape {activations_dataset.features[self.hook_name].shape} does not match context_size ({self.context_size}) and d_in ({self.d_in})\"\n            )\n\n        return activations_dataset\n\n    def shuffle_input_dataset(self, seed: int, buffer_size: int = 1):\n        \"\"\"\n        This applies a shuffle to the huggingface dataset that is the input to the activations store. This\n        also shuffles the shards of the dataset, which is especially useful for evaluating on different\n        sections of very large streaming datasets. Buffer size is only relevant for streaming datasets.\n        The default buffer_size of 1 means that only the shard will be shuffled; larger buffer sizes will\n        additionally shuffle individual elements within the shard.\n        \"\"\"\n        if isinstance(self.dataset, IterableDataset):\n            self.dataset = self.dataset.shuffle(seed=seed, buffer_size=buffer_size)\n        else:\n            self.dataset = self.dataset.shuffle(seed=seed)\n        self.iterable_dataset = iter(self.dataset)\n\n    def reset_input_dataset(self):\n        \"\"\"\n        Resets the input dataset iterator to the beginning.\n        \"\"\"\n        self.iterable_dataset = iter(self.dataset)\n\n    def get_batch_tokens(\n        self, batch_size: int | None = None, raise_at_epoch_end: bool = False\n    ):\n        \"\"\"\n        Streams a batch of tokens from a dataset.\n\n        If raise_at_epoch_end is true we will reset the dataset at the end of each epoch and raise a StopIteration. Otherwise we will reset silently.\n        \"\"\"\n        if not batch_size:\n            batch_size = self.store_batch_size_prompts\n        sequences = []\n        # the sequences iterator yields fully formed tokens of size context_size, so we just need to cat these into a batch\n        for _ in range(batch_size):\n            try:\n                sequences.append(next(self.iterable_sequences))\n            except StopIteration:\n                self.iterable_sequences = self._iterate_tokenized_sequences()\n                if raise_at_epoch_end:\n                    raise StopIteration(\n                        f\"Ran out of tokens in dataset after {self.n_dataset_processed} samples, beginning the next epoch.\"\n                    )\n                sequences.append(next(self.iterable_sequences))\n\n        return torch.stack(sequences, dim=0).to(_get_model_device(self.model))\n\n    @torch.no_grad()\n    def get_activations(self, batch_tokens: torch.Tensor):\n        \"\"\"\n        Returns activations of shape (batches, context, num_layers, d_in)\n\n        d_in may result from a concatenated head dimension.\n        \"\"\"\n        with torch.autocast(\n            device_type=\"cuda\",\n            dtype=torch.bfloat16,\n            enabled=self.autocast_lm,\n        ):\n            layerwise_activations_cache = self.model.run_with_cache(\n                batch_tokens,\n                names_filter=[self.hook_name],\n                stop_at_layer=extract_stop_at_layer_from_tlens_hook_name(\n                    self.hook_name\n                ),\n                prepend_bos=False,\n                **self.model_kwargs,\n            )[1]\n\n        layerwise_activations = layerwise_activations_cache[self.hook_name][\n            :, slice(*self.seqpos_slice)\n        ]\n\n        n_batches, n_context = layerwise_activations.shape[:2]\n\n        stacked_activations = torch.zeros((n_batches, n_context, self.d_in))\n\n        if self.hook_head_index is not None:\n            stacked_activations[:, :] = layerwise_activations[\n                :, :, self.hook_head_index\n            ]\n        elif layerwise_activations.ndim &gt; 3:  # if we have a head dimension\n            try:\n                stacked_activations[:, :] = layerwise_activations.view(\n                    n_batches, n_context, -1\n                )\n            except RuntimeError as e:\n                logger.error(f\"Error during view operation: {e}\")\n                logger.info(\"Attempting to use reshape instead...\")\n                stacked_activations[:, :] = layerwise_activations.reshape(\n                    n_batches, n_context, -1\n                )\n        else:\n            stacked_activations[:, :] = layerwise_activations\n\n        return stacked_activations\n\n    def _load_buffer_from_cached(\n        self,\n        total_size: int,\n        context_size: int,\n        d_in: int,\n        raise_on_epoch_end: bool,\n    ) -&gt; tuple[\n        Float[torch.Tensor, \"(total_size context_size) num_layers d_in\"],\n        Int[torch.Tensor, \"(total_size context_size)\"] | None,\n    ]:\n        \"\"\"\n        Loads `total_size` activations from `cached_activation_dataset`\n\n        The dataset has columns for each hook_name,\n        each containing activations of shape (context_size, d_in).\n\n        raises StopIteration\n        \"\"\"\n        assert self.cached_activation_dataset is not None\n        # In future, could be a list of multiple hook names\n        if self.hook_name not in self.cached_activation_dataset.column_names:\n            raise ValueError(\n                f\"Missing columns in dataset. Expected {self.hook_name}, \"\n                f\"got {self.cached_activation_dataset.column_names}.\"\n            )\n\n        if self.current_row_idx &gt; len(self.cached_activation_dataset) - total_size:\n            self.current_row_idx = 0\n            if raise_on_epoch_end:\n                raise StopIteration\n\n        new_buffer = []\n        ds_slice = self.cached_activation_dataset[\n            self.current_row_idx : self.current_row_idx + total_size\n        ]\n        # Load activations for each hook.\n        # Usually faster to first slice dataset then pick column\n        new_buffer = ds_slice[self.hook_name]\n        if new_buffer.shape != (total_size, context_size, d_in):\n            raise ValueError(\n                f\"new_buffer has shape {new_buffer.shape}, \"\n                f\"but expected ({total_size}, {context_size}, {d_in}).\"\n            )\n\n        self.current_row_idx += total_size\n        acts_buffer = new_buffer.reshape(total_size * context_size, d_in)\n\n        if \"token_ids\" not in self.cached_activation_dataset.column_names:\n            return acts_buffer, None\n\n        token_ids_buffer = ds_slice[\"token_ids\"]\n        if token_ids_buffer.shape != (total_size, context_size):\n            raise ValueError(\n                f\"token_ids_buffer has shape {token_ids_buffer.shape}, \"\n                f\"but expected ({total_size}, {context_size}).\"\n            )\n        token_ids_buffer = token_ids_buffer.reshape(total_size * context_size)\n        return acts_buffer, token_ids_buffer\n\n    @torch.no_grad()\n    def get_raw_buffer(\n        self,\n        n_batches_in_buffer: int,\n        raise_on_epoch_end: bool = False,\n        shuffle: bool = True,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n        \"\"\"\n        Loads the next n_batches_in_buffer batches of activations into a tensor and returns it.\n\n        The primary purpose here is maintaining a shuffling buffer.\n\n        If raise_on_epoch_end is True, when the dataset it exhausted it will automatically refill the dataset and then raise a StopIteration so that the caller has a chance to react.\n        \"\"\"\n        context_size = self.context_size\n        batch_size = self.store_batch_size_prompts\n        d_in = self.d_in\n        total_size = batch_size * n_batches_in_buffer\n\n        if self.cached_activation_dataset is not None:\n            return self._load_buffer_from_cached(\n                total_size, context_size, d_in, raise_on_epoch_end\n            )\n\n        refill_iterator = range(0, total_size, batch_size)\n        # Initialize empty tensor buffer of the maximum required size with an additional dimension for layers\n        new_buffer_activations = torch.zeros(\n            (total_size, self.training_context_size, d_in),\n            dtype=self.dtype,  # type: ignore\n            device=self.device,\n        )\n        new_buffer_token_ids = torch.zeros(\n            (total_size, self.training_context_size),\n            dtype=torch.long,\n            device=self.device,\n        )\n\n        for refill_batch_idx_start in tqdm(\n            refill_iterator, leave=False, desc=\"Refilling buffer\"\n        ):\n            # move batch toks to gpu for model\n            refill_batch_tokens = self.get_batch_tokens(\n                raise_at_epoch_end=raise_on_epoch_end\n            ).to(_get_model_device(self.model))\n            refill_activations = self.get_activations(refill_batch_tokens)\n            # move acts back to cpu\n            refill_activations.to(self.device)\n            new_buffer_activations[\n                refill_batch_idx_start : refill_batch_idx_start + batch_size, ...\n            ] = refill_activations\n\n            # handle seqpos_slice, this is done for activations in get_activations\n            refill_batch_tokens = refill_batch_tokens[:, slice(*self.seqpos_slice)]\n            new_buffer_token_ids[\n                refill_batch_idx_start : refill_batch_idx_start + batch_size, ...\n            ] = refill_batch_tokens\n\n        new_buffer_activations = new_buffer_activations.reshape(-1, d_in)\n        new_buffer_token_ids = new_buffer_token_ids.reshape(-1)\n        if shuffle:\n            new_buffer_activations, new_buffer_token_ids = permute_together(\n                [new_buffer_activations, new_buffer_token_ids]\n            )\n\n        return (\n            new_buffer_activations,\n            new_buffer_token_ids,\n        )\n\n    def get_filtered_buffer(\n        self,\n        n_batches_in_buffer: int,\n        raise_on_epoch_end: bool = False,\n        shuffle: bool = True,\n    ) -&gt; torch.Tensor:\n        return _filter_buffer_acts(\n            self.get_raw_buffer(\n                n_batches_in_buffer=n_batches_in_buffer,\n                raise_on_epoch_end=raise_on_epoch_end,\n                shuffle=shuffle,\n            ),\n            self.exclude_special_tokens,\n        )\n\n    def _iterate_filtered_activations(self) -&gt; Generator[torch.Tensor, None, None]:\n        \"\"\"\n        Iterate over the filtered tokens in the buffer.\n        \"\"\"\n        while True:\n            try:\n                yield self.get_filtered_buffer(\n                    self.half_buffer_size, raise_on_epoch_end=True\n                )\n            except StopIteration:\n                warnings.warn(\n                    \"All samples in the training dataset have been exhausted, beginning new epoch.\"\n                )\n                try:\n                    yield self.get_filtered_buffer(self.half_buffer_size)\n                except StopIteration:\n                    raise ValueError(\n                        \"Unable to fill buffer after starting new epoch. Dataset may be too small.\"\n                    )\n\n    def get_data_loader(\n        self,\n    ) -&gt; Iterator[Any]:\n        \"\"\"\n        Return an auto-refilling stream of filtered and mixed activations.\n        \"\"\"\n        return mixing_buffer(\n            buffer_size=self.n_batches_in_buffer * self.training_context_size,\n            batch_size=self.train_batch_size_tokens,\n            activations_loader=self._iterate_filtered_activations(),\n        )\n\n    def next_batch(self) -&gt; torch.Tensor:\n        \"\"\"Get next batch, updating buffer if needed.\"\"\"\n        return self.__next__()\n\n    # ActivationsStore should be an iterator\n    def __next__(self) -&gt; torch.Tensor:\n        if self._dataloader is None:\n            self._dataloader = self.get_data_loader()\n        return next(self._dataloader)\n\n    def __iter__(self) -&gt; Iterator[torch.Tensor]:\n        return self\n\n    def state_dict(self) -&gt; dict[str, torch.Tensor]:\n        return {\"n_dataset_processed\": torch.tensor(self.n_dataset_processed)}\n\n    def save(self, file_path: str):\n        \"\"\"save the state dict to a file in safetensors format\"\"\"\n        save_file(self.state_dict(), file_path)\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.from_cache_activations","title":"<code>from_cache_activations(model, cfg)</code>  <code>classmethod</code>","text":"<p>Public api to create an ActivationsStore from a cached activations dataset.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>@classmethod\ndef from_cache_activations(\n    cls,\n    model: HookedRootModule,\n    cfg: CacheActivationsRunnerConfig,\n) -&gt; ActivationsStore:\n    \"\"\"\n    Public api to create an ActivationsStore from a cached activations dataset.\n    \"\"\"\n    return cls(\n        cached_activations_path=cfg.new_cached_activations_path,\n        dtype=cfg.dtype,\n        hook_name=cfg.hook_name,\n        context_size=cfg.context_size,\n        d_in=cfg.d_in,\n        n_batches_in_buffer=cfg.n_batches_in_buffer,\n        total_training_tokens=cfg.training_tokens,\n        store_batch_size_prompts=cfg.model_batch_size,  # get_buffer\n        train_batch_size_tokens=cfg.model_batch_size,  # dataloader\n        seqpos_slice=(None,),\n        device=torch.device(cfg.device),  # since we're sending these to SAE\n        # NOOP\n        prepend_bos=False,\n        hook_head_index=None,\n        dataset=cfg.dataset_path,\n        streaming=False,\n        model=model,\n        normalize_activations=\"none\",\n        model_kwargs=None,\n        autocast_lm=False,\n        dataset_trust_remote_code=None,\n        exclude_special_tokens=None,\n    )\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.get_activations","title":"<code>get_activations(batch_tokens)</code>","text":"<p>Returns activations of shape (batches, context, num_layers, d_in)</p> <p>d_in may result from a concatenated head dimension.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>@torch.no_grad()\ndef get_activations(self, batch_tokens: torch.Tensor):\n    \"\"\"\n    Returns activations of shape (batches, context, num_layers, d_in)\n\n    d_in may result from a concatenated head dimension.\n    \"\"\"\n    with torch.autocast(\n        device_type=\"cuda\",\n        dtype=torch.bfloat16,\n        enabled=self.autocast_lm,\n    ):\n        layerwise_activations_cache = self.model.run_with_cache(\n            batch_tokens,\n            names_filter=[self.hook_name],\n            stop_at_layer=extract_stop_at_layer_from_tlens_hook_name(\n                self.hook_name\n            ),\n            prepend_bos=False,\n            **self.model_kwargs,\n        )[1]\n\n    layerwise_activations = layerwise_activations_cache[self.hook_name][\n        :, slice(*self.seqpos_slice)\n    ]\n\n    n_batches, n_context = layerwise_activations.shape[:2]\n\n    stacked_activations = torch.zeros((n_batches, n_context, self.d_in))\n\n    if self.hook_head_index is not None:\n        stacked_activations[:, :] = layerwise_activations[\n            :, :, self.hook_head_index\n        ]\n    elif layerwise_activations.ndim &gt; 3:  # if we have a head dimension\n        try:\n            stacked_activations[:, :] = layerwise_activations.view(\n                n_batches, n_context, -1\n            )\n        except RuntimeError as e:\n            logger.error(f\"Error during view operation: {e}\")\n            logger.info(\"Attempting to use reshape instead...\")\n            stacked_activations[:, :] = layerwise_activations.reshape(\n                n_batches, n_context, -1\n            )\n    else:\n        stacked_activations[:, :] = layerwise_activations\n\n    return stacked_activations\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.get_batch_tokens","title":"<code>get_batch_tokens(batch_size=None, raise_at_epoch_end=False)</code>","text":"<p>Streams a batch of tokens from a dataset.</p> <p>If raise_at_epoch_end is true we will reset the dataset at the end of each epoch and raise a StopIteration. Otherwise we will reset silently.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def get_batch_tokens(\n    self, batch_size: int | None = None, raise_at_epoch_end: bool = False\n):\n    \"\"\"\n    Streams a batch of tokens from a dataset.\n\n    If raise_at_epoch_end is true we will reset the dataset at the end of each epoch and raise a StopIteration. Otherwise we will reset silently.\n    \"\"\"\n    if not batch_size:\n        batch_size = self.store_batch_size_prompts\n    sequences = []\n    # the sequences iterator yields fully formed tokens of size context_size, so we just need to cat these into a batch\n    for _ in range(batch_size):\n        try:\n            sequences.append(next(self.iterable_sequences))\n        except StopIteration:\n            self.iterable_sequences = self._iterate_tokenized_sequences()\n            if raise_at_epoch_end:\n                raise StopIteration(\n                    f\"Ran out of tokens in dataset after {self.n_dataset_processed} samples, beginning the next epoch.\"\n                )\n            sequences.append(next(self.iterable_sequences))\n\n    return torch.stack(sequences, dim=0).to(_get_model_device(self.model))\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.get_data_loader","title":"<code>get_data_loader()</code>","text":"<p>Return an auto-refilling stream of filtered and mixed activations.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def get_data_loader(\n    self,\n) -&gt; Iterator[Any]:\n    \"\"\"\n    Return an auto-refilling stream of filtered and mixed activations.\n    \"\"\"\n    return mixing_buffer(\n        buffer_size=self.n_batches_in_buffer * self.training_context_size,\n        batch_size=self.train_batch_size_tokens,\n        activations_loader=self._iterate_filtered_activations(),\n    )\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.get_raw_buffer","title":"<code>get_raw_buffer(n_batches_in_buffer, raise_on_epoch_end=False, shuffle=True)</code>","text":"<p>Loads the next n_batches_in_buffer batches of activations into a tensor and returns it.</p> <p>The primary purpose here is maintaining a shuffling buffer.</p> <p>If raise_on_epoch_end is True, when the dataset it exhausted it will automatically refill the dataset and then raise a StopIteration so that the caller has a chance to react.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>@torch.no_grad()\ndef get_raw_buffer(\n    self,\n    n_batches_in_buffer: int,\n    raise_on_epoch_end: bool = False,\n    shuffle: bool = True,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n    \"\"\"\n    Loads the next n_batches_in_buffer batches of activations into a tensor and returns it.\n\n    The primary purpose here is maintaining a shuffling buffer.\n\n    If raise_on_epoch_end is True, when the dataset it exhausted it will automatically refill the dataset and then raise a StopIteration so that the caller has a chance to react.\n    \"\"\"\n    context_size = self.context_size\n    batch_size = self.store_batch_size_prompts\n    d_in = self.d_in\n    total_size = batch_size * n_batches_in_buffer\n\n    if self.cached_activation_dataset is not None:\n        return self._load_buffer_from_cached(\n            total_size, context_size, d_in, raise_on_epoch_end\n        )\n\n    refill_iterator = range(0, total_size, batch_size)\n    # Initialize empty tensor buffer of the maximum required size with an additional dimension for layers\n    new_buffer_activations = torch.zeros(\n        (total_size, self.training_context_size, d_in),\n        dtype=self.dtype,  # type: ignore\n        device=self.device,\n    )\n    new_buffer_token_ids = torch.zeros(\n        (total_size, self.training_context_size),\n        dtype=torch.long,\n        device=self.device,\n    )\n\n    for refill_batch_idx_start in tqdm(\n        refill_iterator, leave=False, desc=\"Refilling buffer\"\n    ):\n        # move batch toks to gpu for model\n        refill_batch_tokens = self.get_batch_tokens(\n            raise_at_epoch_end=raise_on_epoch_end\n        ).to(_get_model_device(self.model))\n        refill_activations = self.get_activations(refill_batch_tokens)\n        # move acts back to cpu\n        refill_activations.to(self.device)\n        new_buffer_activations[\n            refill_batch_idx_start : refill_batch_idx_start + batch_size, ...\n        ] = refill_activations\n\n        # handle seqpos_slice, this is done for activations in get_activations\n        refill_batch_tokens = refill_batch_tokens[:, slice(*self.seqpos_slice)]\n        new_buffer_token_ids[\n            refill_batch_idx_start : refill_batch_idx_start + batch_size, ...\n        ] = refill_batch_tokens\n\n    new_buffer_activations = new_buffer_activations.reshape(-1, d_in)\n    new_buffer_token_ids = new_buffer_token_ids.reshape(-1)\n    if shuffle:\n        new_buffer_activations, new_buffer_token_ids = permute_together(\n            [new_buffer_activations, new_buffer_token_ids]\n        )\n\n    return (\n        new_buffer_activations,\n        new_buffer_token_ids,\n    )\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.load_cached_activation_dataset","title":"<code>load_cached_activation_dataset()</code>","text":"<p>Load the cached activation dataset from disk.</p> <ul> <li>If cached_activations_path is set, returns Huggingface Dataset else None</li> <li>Checks that the loaded dataset has current has activations for hooks in config and that shapes match.</li> </ul> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def load_cached_activation_dataset(self) -&gt; Dataset | None:\n    \"\"\"\n    Load the cached activation dataset from disk.\n\n    - If cached_activations_path is set, returns Huggingface Dataset else None\n    - Checks that the loaded dataset has current has activations for hooks in config and that shapes match.\n    \"\"\"\n    if self.cached_activations_path is None:\n        return None\n\n    assert self.cached_activations_path is not None  # keep pyright happy\n    # Sanity check: does the cache directory exist?\n    if not os.path.exists(self.cached_activations_path):\n        raise FileNotFoundError(\n            f\"Cache directory {self.cached_activations_path} does not exist. \"\n            \"Consider double-checking your dataset, model, and hook names.\"\n        )\n\n    # ---\n    # Actual code\n    activations_dataset = datasets.load_from_disk(self.cached_activations_path)\n    columns = [self.hook_name]\n    if \"token_ids\" in activations_dataset.column_names:\n        columns.append(\"token_ids\")\n    activations_dataset.set_format(\n        type=\"torch\", columns=columns, device=self.device, dtype=self.dtype\n    )\n    self.current_row_idx = 0  # idx to load next batch from\n    # ---\n\n    assert isinstance(activations_dataset, Dataset)\n\n    # multiple in hooks future\n    if not set([self.hook_name]).issubset(activations_dataset.column_names):\n        raise ValueError(\n            f\"loaded dataset does not include hook activations, got {activations_dataset.column_names}\"\n        )\n\n    if activations_dataset.features[self.hook_name].shape != (\n        self.context_size,\n        self.d_in,\n    ):\n        raise ValueError(\n            f\"Given dataset of shape {activations_dataset.features[self.hook_name].shape} does not match context_size ({self.context_size}) and d_in ({self.d_in})\"\n        )\n\n    return activations_dataset\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.next_batch","title":"<code>next_batch()</code>","text":"<p>Get next batch, updating buffer if needed.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def next_batch(self) -&gt; torch.Tensor:\n    \"\"\"Get next batch, updating buffer if needed.\"\"\"\n    return self.__next__()\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.reset_input_dataset","title":"<code>reset_input_dataset()</code>","text":"<p>Resets the input dataset iterator to the beginning.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def reset_input_dataset(self):\n    \"\"\"\n    Resets the input dataset iterator to the beginning.\n    \"\"\"\n    self.iterable_dataset = iter(self.dataset)\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.save","title":"<code>save(file_path)</code>","text":"<p>save the state dict to a file in safetensors format</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def save(self, file_path: str):\n    \"\"\"save the state dict to a file in safetensors format\"\"\"\n    save_file(self.state_dict(), file_path)\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.shuffle_input_dataset","title":"<code>shuffle_input_dataset(seed, buffer_size=1)</code>","text":"<p>This applies a shuffle to the huggingface dataset that is the input to the activations store. This also shuffles the shards of the dataset, which is especially useful for evaluating on different sections of very large streaming datasets. Buffer size is only relevant for streaming datasets. The default buffer_size of 1 means that only the shard will be shuffled; larger buffer sizes will additionally shuffle individual elements within the shard.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def shuffle_input_dataset(self, seed: int, buffer_size: int = 1):\n    \"\"\"\n    This applies a shuffle to the huggingface dataset that is the input to the activations store. This\n    also shuffles the shards of the dataset, which is especially useful for evaluating on different\n    sections of very large streaming datasets. Buffer size is only relevant for streaming datasets.\n    The default buffer_size of 1 means that only the shard will be shuffled; larger buffer sizes will\n    additionally shuffle individual elements within the shard.\n    \"\"\"\n    if isinstance(self.dataset, IterableDataset):\n        self.dataset = self.dataset.shuffle(seed=seed, buffer_size=buffer_size)\n    else:\n        self.dataset = self.dataset.shuffle(seed=seed)\n    self.iterable_dataset = iter(self.dataset)\n</code></pre>"},{"location":"api/#sae_lens.BatchTopKTrainingSAE","title":"<code>BatchTopKTrainingSAE</code>","text":"<p>               Bases: <code>TopKTrainingSAE</code></p> <p>Global Batch TopK Training SAE</p> <p>This SAE will maintain the k on average across the batch, rather than enforcing the k per-sample as in standard TopK.</p> <p>BatchTopK SAEs are saved as JumpReLU SAEs after training.</p> Source code in <code>sae_lens/saes/batchtopk_sae.py</code> <pre><code>class BatchTopKTrainingSAE(TopKTrainingSAE):\n    \"\"\"\n    Global Batch TopK Training SAE\n\n    This SAE will maintain the k on average across the batch, rather than enforcing the k per-sample as in standard TopK.\n\n    BatchTopK SAEs are saved as JumpReLU SAEs after training.\n    \"\"\"\n\n    topk_threshold: torch.Tensor\n    cfg: BatchTopKTrainingSAEConfig  # type: ignore[assignment]\n\n    def __init__(self, cfg: BatchTopKTrainingSAEConfig, use_error_term: bool = False):\n        super().__init__(cfg, use_error_term)\n\n        self.register_buffer(\n            \"topk_threshold\",\n            # use double precision as otherwise we can run into numerical issues\n            torch.tensor(0.0, dtype=torch.double, device=self.W_dec.device),\n        )\n\n    def get_activation_fn(self) -&gt; Callable[[torch.Tensor], torch.Tensor]:\n        return BatchTopK(self.cfg.k)\n\n    @override\n    def training_forward_pass(self, step_input: TrainStepInput) -&gt; TrainStepOutput:\n        output = super().training_forward_pass(step_input)\n        self.update_topk_threshold(output.feature_acts)\n        output.metrics[\"topk_threshold\"] = self.topk_threshold\n        return output\n\n    @torch.no_grad()\n    def update_topk_threshold(self, acts_topk: torch.Tensor) -&gt; None:\n        positive_mask = acts_topk &gt; 0\n        lr = self.cfg.topk_threshold_lr\n        # autocast can cause numerical issues with the threshold update\n        with torch.autocast(self.topk_threshold.device.type, enabled=False):\n            if positive_mask.any():\n                min_positive = (\n                    acts_topk[positive_mask].min().to(self.topk_threshold.dtype)\n                )\n                self.topk_threshold = (1 - lr) * self.topk_threshold + lr * min_positive\n\n    @override\n    def process_state_dict_for_saving_inference(\n        self, state_dict: dict[str, Any]\n    ) -&gt; None:\n        super().process_state_dict_for_saving_inference(state_dict)\n        # turn the topk threshold into jumprelu threshold\n        topk_threshold = state_dict.pop(\"topk_threshold\").item()\n        state_dict[\"threshold\"] = torch.ones_like(self.b_enc) * topk_threshold\n</code></pre>"},{"location":"api/#sae_lens.BatchTopKTrainingSAEConfig","title":"<code>BatchTopKTrainingSAEConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TopKTrainingSAEConfig</code></p> <p>Configuration class for training a BatchTopKTrainingSAE.</p> Source code in <code>sae_lens/saes/batchtopk_sae.py</code> <pre><code>@dataclass\nclass BatchTopKTrainingSAEConfig(TopKTrainingSAEConfig):\n    \"\"\"\n    Configuration class for training a BatchTopKTrainingSAE.\n    \"\"\"\n\n    k: float = 100  # type: ignore[assignment]\n    topk_threshold_lr: float = 0.01\n\n    @override\n    @classmethod\n    def architecture(cls) -&gt; str:\n        return \"batchtopk\"\n\n    @override\n    def get_inference_config_class(self) -&gt; type[SAEConfig]:\n        return JumpReLUSAEConfig\n</code></pre>"},{"location":"api/#sae_lens.CacheActivationsRunner","title":"<code>CacheActivationsRunner</code>","text":"Source code in <code>sae_lens/cache_activations_runner.py</code> <pre><code>class CacheActivationsRunner:\n    def __init__(\n        self,\n        cfg: CacheActivationsRunnerConfig,\n        override_dataset: Dataset | None = None,\n    ):\n        self.cfg = cfg\n        self.model: HookedRootModule = load_model(\n            model_class_name=self.cfg.model_class_name,\n            model_name=self.cfg.model_name,\n            device=self.cfg.device,\n            model_from_pretrained_kwargs=self.cfg.model_from_pretrained_kwargs,\n        )\n        if self.cfg.compile_llm:\n            self.model = torch.compile(self.model, mode=self.cfg.llm_compilation_mode)  # type: ignore\n        self.activations_store = _mk_activations_store(\n            self.model,\n            self.cfg,\n            override_dataset=override_dataset,\n        )\n        self.context_size = self._get_sliced_context_size(\n            self.cfg.context_size, self.cfg.seqpos_slice\n        )\n        features_dict: dict[str, Array2D | Sequence] = {\n            hook_name: Array2D(\n                shape=(self.context_size, self.cfg.d_in), dtype=self.cfg.dtype\n            )\n            for hook_name in [self.cfg.hook_name]\n        }\n        features_dict[\"token_ids\"] = Sequence(  # type: ignore\n            Value(dtype=\"int32\"), length=self.context_size\n        )\n        self.features = Features(features_dict)\n\n    def __str__(self):\n        \"\"\"\n        Print the number of tokens to be cached.\n        Print the number of buffers, and the number of tokens per buffer.\n        Print the disk space required to store the activations.\n\n        \"\"\"\n\n        bytes_per_token = (\n            self.cfg.d_in * self.cfg.dtype.itemsize\n            if isinstance(self.cfg.dtype, torch.dtype)\n            else DTYPE_MAP[self.cfg.dtype].itemsize\n        )\n        total_training_tokens = self.cfg.n_seq_in_dataset * self.context_size\n        total_disk_space_gb = total_training_tokens * bytes_per_token / 10**9\n\n        return (\n            f\"Activation Cache Runner:\\n\"\n            f\"Total training tokens: {total_training_tokens}\\n\"\n            f\"Number of buffers: {self.cfg.n_buffers}\\n\"\n            f\"Tokens per buffer: {self.cfg.n_tokens_in_buffer}\\n\"\n            f\"Disk space required: {total_disk_space_gb:.2f} GB\\n\"\n            f\"Configuration:\\n\"\n            f\"{self.cfg}\"\n        )\n\n    @staticmethod\n    def _consolidate_shards(\n        source_dir: Path, output_dir: Path, copy_files: bool = True\n    ) -&gt; Dataset:\n        \"\"\"Consolidate sharded datasets into a single directory without rewriting data.\n\n        Each of the shards must be of the same format, aka the full dataset must be able to\n        be recreated like so:\n\n        ```\n        ds = concatenate_datasets(\n            [Dataset.load_from_disk(str(shard_dir)) for shard_dir in sorted(source_dir.iterdir())]\n        )\n\n        ```\n\n        Sharded dataset format:\n        ```\n        source_dir/\n            shard_00000/\n                dataset_info.json\n                state.json\n                data-00000-of-00002.arrow\n                data-00001-of-00002.arrow\n            shard_00001/\n                dataset_info.json\n                state.json\n                data-00000-of-00001.arrow\n        ```\n\n        And flattens them into the format:\n\n        ```\n        output_dir/\n            dataset_info.json\n            state.json\n            data-00000-of-00003.arrow\n            data-00001-of-00003.arrow\n            data-00002-of-00003.arrow\n        ```\n\n        allowing the dataset to be loaded like so:\n\n        ```\n        ds = datasets.load_from_disk(output_dir)\n        ```\n\n        Args:\n            source_dir: Directory containing the sharded datasets\n            output_dir: Directory to consolidate the shards into\n            copy_files: If True, copy files; if False, move them and delete source_dir\n        \"\"\"\n        first_shard_dir_name = \"shard_00000\"  # shard_{i:05d}\n\n        if not source_dir.exists() or not source_dir.is_dir():\n            raise NotADirectoryError(\n                f\"source_dir is not an existing directory: {source_dir}\"\n            )\n\n        if not output_dir.exists() or not output_dir.is_dir():\n            raise NotADirectoryError(\n                f\"output_dir is not an existing directory: {output_dir}\"\n            )\n\n        other_items = [p for p in output_dir.iterdir() if p.name != \".tmp_shards\"]\n        if other_items:\n            raise FileExistsError(\n                f\"output_dir must be empty (besides .tmp_shards). Found: {other_items}\"\n            )\n\n        if not (source_dir / first_shard_dir_name).exists():\n            raise Exception(f\"No shards in {source_dir} exist!\")\n\n        transfer_fn = shutil.copy2 if copy_files else shutil.move\n\n        # Move dataset_info.json from any shard (all the same)\n        transfer_fn(\n            source_dir / first_shard_dir_name / \"dataset_info.json\",\n            output_dir / \"dataset_info.json\",\n        )\n\n        arrow_files = []\n        file_count = 0\n\n        for shard_dir in sorted(source_dir.iterdir()):\n            if not shard_dir.name.startswith(\"shard_\"):\n                continue\n\n            # state.json contains arrow filenames\n            state = json.loads((shard_dir / \"state.json\").read_text())\n\n            for data_file in state[\"_data_files\"]:\n                src = shard_dir / data_file[\"filename\"]\n                new_name = f\"data-{file_count:05d}-of-{len(list(source_dir.iterdir())):05d}.arrow\"\n                dst = output_dir / new_name\n                transfer_fn(src, dst)\n                arrow_files.append({\"filename\": new_name})\n                file_count += 1\n\n        new_state = {\n            \"_data_files\": arrow_files,\n            \"_fingerprint\": None,  # temporary\n            \"_format_columns\": None,\n            \"_format_kwargs\": {},\n            \"_format_type\": None,\n            \"_output_all_columns\": False,\n            \"_split\": None,\n        }\n\n        # fingerprint is generated from dataset.__getstate__ (not includeing _fingerprint)\n        with open(output_dir / \"state.json\", \"w\") as f:\n            json.dump(new_state, f, indent=2)\n\n        ds = Dataset.load_from_disk(str(output_dir))\n        fingerprint = generate_fingerprint(ds)\n        del ds\n\n        with open(output_dir / \"state.json\", \"r+\") as f:\n            state = json.loads(f.read())\n            state[\"_fingerprint\"] = fingerprint\n            f.seek(0)\n            json.dump(state, f, indent=2)\n            f.truncate()\n\n        if not copy_files:  # cleanup source dir\n            shutil.rmtree(source_dir)\n\n        return Dataset.load_from_disk(output_dir)\n\n    @torch.no_grad()\n    def run(self) -&gt; Dataset:\n        activation_save_path = self.cfg.new_cached_activations_path\n        assert activation_save_path is not None\n\n        ### Paths setup\n        final_cached_activation_path = Path(activation_save_path)\n        final_cached_activation_path.mkdir(exist_ok=True, parents=True)\n        if any(final_cached_activation_path.iterdir()):\n            raise Exception(\n                f\"Activations directory ({final_cached_activation_path}) is not empty. Please delete it or specify a different path. Exiting the script to prevent accidental deletion of files.\"\n            )\n\n        tmp_cached_activation_path = final_cached_activation_path / \".tmp_shards/\"\n        tmp_cached_activation_path.mkdir(exist_ok=False, parents=False)\n\n        ### Create temporary sharded datasets\n\n        logger.info(f\"Started caching activations for {self.cfg.dataset_path}\")\n\n        for i in tqdm(range(self.cfg.n_buffers), desc=\"Caching activations\"):\n            try:\n                buffer = self.activations_store.get_raw_buffer(\n                    self.cfg.n_batches_in_buffer, shuffle=False\n                )\n                shard = self._create_shard(buffer)\n                shard.save_to_disk(\n                    f\"{tmp_cached_activation_path}/shard_{i:05d}\", num_shards=1\n                )\n                del buffer, shard\n            except StopIteration:\n                logger.warning(\n                    f\"Warning: Ran out of samples while filling the buffer at batch {i} before reaching {self.cfg.n_buffers} batches.\"\n                )\n                break\n\n        ### Concatenate shards and push to Huggingface Hub\n\n        dataset = self._consolidate_shards(\n            tmp_cached_activation_path, final_cached_activation_path, copy_files=False\n        )\n\n        if self.cfg.shuffle:\n            logger.info(\"Shuffling...\")\n            dataset = dataset.shuffle(seed=self.cfg.seed)\n\n        if self.cfg.hf_repo_id:\n            logger.info(\"Pushing to Huggingface Hub...\")\n            dataset.push_to_hub(\n                repo_id=self.cfg.hf_repo_id,\n                num_shards=self.cfg.hf_num_shards,\n                private=self.cfg.hf_is_private_repo,\n                revision=self.cfg.hf_revision,\n            )\n\n            meta_io = io.BytesIO()\n            meta_contents = json.dumps(\n                asdict(self.cfg), indent=2, ensure_ascii=False\n            ).encode(\"utf-8\")\n            meta_io.write(meta_contents)\n            meta_io.seek(0)\n\n            api = HfApi()\n            api.upload_file(\n                path_or_fileobj=meta_io,\n                path_in_repo=\"cache_activations_runner_cfg.json\",\n                repo_id=self.cfg.hf_repo_id,\n                repo_type=\"dataset\",\n                commit_message=\"Add cache_activations_runner metadata\",\n            )\n\n        return dataset\n\n    def _create_shard(\n        self,\n        buffer: tuple[\n            Float[torch.Tensor, \"(bs context_size) d_in\"],\n            Int[torch.Tensor, \"(bs context_size)\"] | None,\n        ],\n    ) -&gt; Dataset:\n        hook_names = [self.cfg.hook_name]\n        acts, token_ids = buffer\n        acts = einops.rearrange(\n            acts,\n            \"(bs context_size) d_in -&gt; bs context_size d_in\",\n            bs=self.cfg.n_seq_in_buffer,\n            context_size=self.context_size,\n            d_in=self.cfg.d_in,\n        )\n        shard_dict: dict[str, object] = {\n            hook_name: act_batch\n            for hook_name, act_batch in zip(hook_names, [acts], strict=True)\n        }\n\n        if token_ids is not None:\n            token_ids = einops.rearrange(\n                token_ids,\n                \"(bs context_size) -&gt; bs context_size\",\n                bs=self.cfg.n_seq_in_buffer,\n                context_size=self.context_size,\n            )\n            shard_dict[\"token_ids\"] = token_ids.to(torch.int32)\n        return Dataset.from_dict(\n            shard_dict,\n            features=self.features,\n        )\n\n    @staticmethod\n    def _get_sliced_context_size(\n        context_size: int, seqpos_slice: tuple[int | None, ...] | None\n    ) -&gt; int:\n        if seqpos_slice is not None:\n            context_size = len(range(context_size)[slice(*seqpos_slice)])\n        return context_size\n</code></pre>"},{"location":"api/#sae_lens.CacheActivationsRunner.__str__","title":"<code>__str__()</code>","text":"<p>Print the number of tokens to be cached. Print the number of buffers, and the number of tokens per buffer. Print the disk space required to store the activations.</p> Source code in <code>sae_lens/cache_activations_runner.py</code> <pre><code>def __str__(self):\n    \"\"\"\n    Print the number of tokens to be cached.\n    Print the number of buffers, and the number of tokens per buffer.\n    Print the disk space required to store the activations.\n\n    \"\"\"\n\n    bytes_per_token = (\n        self.cfg.d_in * self.cfg.dtype.itemsize\n        if isinstance(self.cfg.dtype, torch.dtype)\n        else DTYPE_MAP[self.cfg.dtype].itemsize\n    )\n    total_training_tokens = self.cfg.n_seq_in_dataset * self.context_size\n    total_disk_space_gb = total_training_tokens * bytes_per_token / 10**9\n\n    return (\n        f\"Activation Cache Runner:\\n\"\n        f\"Total training tokens: {total_training_tokens}\\n\"\n        f\"Number of buffers: {self.cfg.n_buffers}\\n\"\n        f\"Tokens per buffer: {self.cfg.n_tokens_in_buffer}\\n\"\n        f\"Disk space required: {total_disk_space_gb:.2f} GB\\n\"\n        f\"Configuration:\\n\"\n        f\"{self.cfg}\"\n    )\n</code></pre>"},{"location":"api/#sae_lens.CacheActivationsRunnerConfig","title":"<code>CacheActivationsRunnerConfig</code>  <code>dataclass</code>","text":"<p>Configuration for creating and caching activations of an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the Hugging Face dataset. This may be tokenized or not.</p> required <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>model_batch_size</code> <code>int</code> <p>How many prompts are in the batch of the language model when generating activations.</p> required <code>hook_name</code> <code>str</code> <p>The name of the hook to use.</p> required <code>d_in</code> <code>int</code> <p>Dimension of the model.</p> required <code>total_training_tokens</code> <code>int</code> <p>Total number of tokens to process.</p> required <code>context_size</code> <code>int</code> <p>Context size to process. Can be left as -1 if the dataset is tokenized.</p> <code>-1</code> <code>model_class_name</code> <code>str</code> <p>The name of the class of the model to use. This should be either <code>HookedTransformer</code> or <code>HookedMamba</code>.</p> <code>'HookedTransformer'</code> <code>new_cached_activations_path</code> <code>str</code> <p>The path to save the activations.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the dataset.</p> <code>True</code> <code>seed</code> <code>int</code> <p>The seed to use for shuffling.</p> <code>42</code> <code>dtype</code> <code>str</code> <p>Datatype of activations to be stored.</p> <code>'float32'</code> <code>device</code> <code>str</code> <p>The device for the model.</p> <code>'cuda' if is_available() else 'cpu'</code> <code>buffer_size_gb</code> <code>float</code> <p>The buffer size in GB. This should be &lt; 2GB.</p> <code>2.0</code> <code>hf_repo_id</code> <code>str</code> <p>The Hugging Face repository id to save the activations to.</p> <code>None</code> <code>hf_num_shards</code> <code>int</code> <p>The number of shards to save the activations to.</p> <code>None</code> <code>hf_revision</code> <code>str</code> <p>The revision to save the activations to.</p> <code>'main'</code> <code>hf_is_private_repo</code> <code>bool</code> <p>Whether the Hugging Face repository is private.</p> <code>False</code> <code>model_kwargs</code> <code>dict</code> <p>Keyword arguments for <code>model.run_with_cache</code>.</p> <code>dict()</code> <code>model_from_pretrained_kwargs</code> <code>dict</code> <p>Keyword arguments for the <code>from_pretrained</code> method of the model.</p> <code>dict()</code> <code>compile_llm</code> <code>bool</code> <p>Whether to compile the LLM.</p> <code>False</code> <code>llm_compilation_mode</code> <code>str</code> <p>The torch.compile mode to use.</p> <code>None</code> <code>prepend_bos</code> <code>bool</code> <p>Whether to prepend the beginning of sequence token. You should use whatever the model was trained with.</p> <code>True</code> <code>seqpos_slice</code> <code>tuple</code> <p>Determines slicing of activations when constructing batches during training. The slice should be (start_pos, end_pos, optional[step_size]), e.g. for Othello we sometimes use (5, -5). Note, step_size &gt; 0.</p> <code>(None,)</code> <code>streaming</code> <code>bool</code> <p>Whether to stream the dataset. Streaming large datasets is usually practical.</p> <code>True</code> <code>autocast_lm</code> <code>bool</code> <p>Whether to use autocast during activation fetching.</p> <code>False</code> <code>dataset_trust_remote_code</code> <code>bool</code> <p>Whether to trust remote code when loading datasets from Huggingface.</p> <code>None</code> Source code in <code>sae_lens/config.py</code> <pre><code>@dataclass\nclass CacheActivationsRunnerConfig:\n    \"\"\"\n    Configuration for creating and caching activations of an LLM.\n\n    Args:\n        dataset_path (str): The path to the Hugging Face dataset. This may be tokenized or not.\n        model_name (str): The name of the model to use.\n        model_batch_size (int): How many prompts are in the batch of the language model when generating activations.\n        hook_name (str): The name of the hook to use.\n        d_in (int): Dimension of the model.\n        total_training_tokens (int): Total number of tokens to process.\n        context_size (int): Context size to process. Can be left as -1 if the dataset is tokenized.\n        model_class_name (str): The name of the class of the model to use. This should be either `HookedTransformer` or `HookedMamba`.\n        new_cached_activations_path (str, optional): The path to save the activations.\n        shuffle (bool): Whether to shuffle the dataset.\n        seed (int): The seed to use for shuffling.\n        dtype (str): Datatype of activations to be stored.\n        device (str): The device for the model.\n        buffer_size_gb (float): The buffer size in GB. This should be &lt; 2GB.\n        hf_repo_id (str, optional): The Hugging Face repository id to save the activations to.\n        hf_num_shards (int, optional): The number of shards to save the activations to.\n        hf_revision (str): The revision to save the activations to.\n        hf_is_private_repo (bool): Whether the Hugging Face repository is private.\n        model_kwargs (dict): Keyword arguments for `model.run_with_cache`.\n        model_from_pretrained_kwargs (dict): Keyword arguments for the `from_pretrained` method of the model.\n        compile_llm (bool): Whether to compile the LLM.\n        llm_compilation_mode (str): The torch.compile mode to use.\n        prepend_bos (bool): Whether to prepend the beginning of sequence token. You should use whatever the model was trained with.\n        seqpos_slice (tuple): Determines slicing of activations when constructing batches during training. The slice should be (start_pos, end_pos, optional[step_size]), e.g. for Othello we sometimes use (5, -5). Note, step_size &gt; 0.\n        streaming (bool): Whether to stream the dataset. Streaming large datasets is usually practical.\n        autocast_lm (bool): Whether to use autocast during activation fetching.\n        dataset_trust_remote_code (bool): Whether to trust remote code when loading datasets from Huggingface.\n    \"\"\"\n\n    dataset_path: str\n    model_name: str\n    model_batch_size: int\n    hook_name: str\n    d_in: int\n    training_tokens: int\n\n    context_size: int = -1  # Required if dataset is not tokenized\n    model_class_name: str = \"HookedTransformer\"\n    # defaults to \"activations/{dataset}/{model}/{hook_name}\n    new_cached_activations_path: str | None = None\n    shuffle: bool = True\n    seed: int = 42\n    dtype: str = \"float32\"\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    buffer_size_gb: float = 2.0  # HF datasets writer have problems with shards &gt; 2GB\n\n    # Huggingface Integration\n    hf_repo_id: str | None = None\n    hf_num_shards: int | None = None\n    hf_revision: str = \"main\"\n    hf_is_private_repo: bool = False\n\n    # Model\n    model_kwargs: dict[str, Any] = field(default_factory=dict)\n    model_from_pretrained_kwargs: dict[str, Any] = field(default_factory=dict)\n    compile_llm: bool = False\n    llm_compilation_mode: str | None = None  # which torch.compile mode to use\n\n    # Activation Store\n    prepend_bos: bool = True\n    seqpos_slice: tuple[int | None, ...] = (None,)\n    streaming: bool = True\n    autocast_lm: bool = False\n    dataset_trust_remote_code: bool | None = None\n\n    def __post_init__(self):\n        # Automatically determine context_size if dataset is tokenized\n        if self.context_size == -1:\n            ds = load_dataset(self.dataset_path, split=\"train\", streaming=True)\n            assert isinstance(ds, IterableDataset)\n            first_sample = next(iter(ds))\n            toks = first_sample.get(\"tokens\") or first_sample.get(\"input_ids\") or None\n            if toks is None:\n                raise ValueError(\n                    \"Dataset is not tokenized. Please specify context_size.\"\n                )\n            token_length = len(toks)\n            self.context_size = token_length\n\n        if self.context_size == -1:\n            raise ValueError(\"context_size is still -1 after dataset inspection.\")\n\n        if self.seqpos_slice is not None:\n            _validate_seqpos(\n                seqpos=self.seqpos_slice,\n                context_size=self.context_size,\n            )\n\n        if self.new_cached_activations_path is None:\n            self.new_cached_activations_path = _default_cached_activations_path(  # type: ignore\n                self.dataset_path, self.model_name, self.hook_name, None\n            )\n\n    @property\n    def sliced_context_size(self) -&gt; int:\n        if self.seqpos_slice is not None:\n            return len(range(self.context_size)[slice(*self.seqpos_slice)])\n        return self.context_size\n\n    @property\n    def bytes_per_token(self) -&gt; int:\n        return self.d_in * DTYPE_MAP[self.dtype].itemsize\n\n    @property\n    def n_tokens_in_buffer(self) -&gt; int:\n        # Calculate raw tokens per buffer based on memory constraints\n        _tokens_per_buffer = int(self.buffer_size_gb * 1e9) // self.bytes_per_token\n        # Round down to nearest multiple of batch_token_size\n        return _tokens_per_buffer - (_tokens_per_buffer % self.n_tokens_in_batch)\n\n    @property\n    def n_tokens_in_batch(self) -&gt; int:\n        return self.model_batch_size * self.sliced_context_size\n\n    @property\n    def n_batches_in_buffer(self) -&gt; int:\n        return self.n_tokens_in_buffer // self.n_tokens_in_batch\n\n    @property\n    def n_seq_in_dataset(self) -&gt; int:\n        return self.training_tokens // self.sliced_context_size\n\n    @property\n    def n_seq_in_buffer(self) -&gt; int:\n        return self.n_tokens_in_buffer // self.sliced_context_size\n\n    @property\n    def n_buffers(self) -&gt; int:\n        return math.ceil(self.training_tokens / self.n_tokens_in_buffer)\n</code></pre>"},{"location":"api/#sae_lens.GatedSAE","title":"<code>GatedSAE</code>","text":"<p>               Bases: <code>SAE[GatedSAEConfig]</code></p> <p>GatedSAE is an inference-only implementation of a Sparse Autoencoder (SAE) using a gated linear encoder and a standard linear decoder.</p> Source code in <code>sae_lens/saes/gated_sae.py</code> <pre><code>class GatedSAE(SAE[GatedSAEConfig]):\n    \"\"\"\n    GatedSAE is an inference-only implementation of a Sparse Autoencoder (SAE)\n    using a gated linear encoder and a standard linear decoder.\n    \"\"\"\n\n    b_gate: nn.Parameter\n    b_mag: nn.Parameter\n    r_mag: nn.Parameter\n\n    def __init__(self, cfg: GatedSAEConfig, use_error_term: bool = False):\n        super().__init__(cfg, use_error_term)\n        # Ensure b_enc does not exist for the gated architecture\n        self.b_enc = None\n\n    @override\n    def initialize_weights(self) -&gt; None:\n        super().initialize_weights()\n        _init_weights_gated(self)\n\n    def encode(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n        \"\"\"\n        Encode the input tensor into the feature space using a gated encoder.\n        This must match the original encode_gated implementation from SAE class.\n        \"\"\"\n        # Preprocess the SAE input (casting type, applying hooks, normalization)\n        sae_in = self.process_sae_in(x)\n\n        # Gating path exactly as in original SAE.encode_gated\n        gating_pre_activation = sae_in @ self.W_enc + self.b_gate\n        active_features = (gating_pre_activation &gt; 0).to(self.dtype)\n\n        # Magnitude path (weight sharing with gated encoder)\n        magnitude_pre_activation = self.hook_sae_acts_pre(\n            sae_in @ (self.W_enc * self.r_mag.exp()) + self.b_mag\n        )\n        feature_magnitudes = self.activation_fn(magnitude_pre_activation)\n\n        # Combine gating and magnitudes\n        return self.hook_sae_acts_post(active_features * feature_magnitudes)\n\n    def decode(\n        self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n    ) -&gt; Float[torch.Tensor, \"... d_in\"]:\n        \"\"\"\n        Decode the feature activations back into the input space:\n          1) Apply optional finetuning scaling.\n          2) Linear transform plus bias.\n          3) Run any reconstruction hooks and out-normalization if configured.\n          4) If the SAE was reshaping hook_z activations, reshape back.\n        \"\"\"\n        # 1) optional finetuning scaling\n        # 2) linear transform\n        sae_out_pre = feature_acts @ self.W_dec + self.b_dec\n        # 3) hooking and normalization\n        sae_out_pre = self.hook_sae_recons(sae_out_pre)\n        sae_out_pre = self.run_time_activation_norm_fn_out(sae_out_pre)\n        # 4) reshape if needed (hook_z)\n        return self.reshape_fn_out(sae_out_pre, self.d_head)\n\n    @torch.no_grad()\n    def fold_W_dec_norm(self):\n        \"\"\"Override to handle gated-specific parameters.\"\"\"\n        W_dec_norms = self.W_dec.norm(dim=-1).unsqueeze(1)\n        self.W_dec.data = self.W_dec.data / W_dec_norms\n        self.W_enc.data = self.W_enc.data * W_dec_norms.T\n\n        # Gated-specific parameters need special handling\n        # r_mag doesn't need scaling since W_enc scaling is sufficient for magnitude path\n        self.b_gate.data = self.b_gate.data * W_dec_norms.squeeze()\n        self.b_mag.data = self.b_mag.data * W_dec_norms.squeeze()\n</code></pre>"},{"location":"api/#sae_lens.GatedSAE.decode","title":"<code>decode(feature_acts)</code>","text":"Decode the feature activations back into the input space <p>1) Apply optional finetuning scaling. 2) Linear transform plus bias. 3) Run any reconstruction hooks and out-normalization if configured. 4) If the SAE was reshaping hook_z activations, reshape back.</p> Source code in <code>sae_lens/saes/gated_sae.py</code> <pre><code>def decode(\n    self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n) -&gt; Float[torch.Tensor, \"... d_in\"]:\n    \"\"\"\n    Decode the feature activations back into the input space:\n      1) Apply optional finetuning scaling.\n      2) Linear transform plus bias.\n      3) Run any reconstruction hooks and out-normalization if configured.\n      4) If the SAE was reshaping hook_z activations, reshape back.\n    \"\"\"\n    # 1) optional finetuning scaling\n    # 2) linear transform\n    sae_out_pre = feature_acts @ self.W_dec + self.b_dec\n    # 3) hooking and normalization\n    sae_out_pre = self.hook_sae_recons(sae_out_pre)\n    sae_out_pre = self.run_time_activation_norm_fn_out(sae_out_pre)\n    # 4) reshape if needed (hook_z)\n    return self.reshape_fn_out(sae_out_pre, self.d_head)\n</code></pre>"},{"location":"api/#sae_lens.GatedSAE.encode","title":"<code>encode(x)</code>","text":"<p>Encode the input tensor into the feature space using a gated encoder. This must match the original encode_gated implementation from SAE class.</p> Source code in <code>sae_lens/saes/gated_sae.py</code> <pre><code>def encode(\n    self, x: Float[torch.Tensor, \"... d_in\"]\n) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n    \"\"\"\n    Encode the input tensor into the feature space using a gated encoder.\n    This must match the original encode_gated implementation from SAE class.\n    \"\"\"\n    # Preprocess the SAE input (casting type, applying hooks, normalization)\n    sae_in = self.process_sae_in(x)\n\n    # Gating path exactly as in original SAE.encode_gated\n    gating_pre_activation = sae_in @ self.W_enc + self.b_gate\n    active_features = (gating_pre_activation &gt; 0).to(self.dtype)\n\n    # Magnitude path (weight sharing with gated encoder)\n    magnitude_pre_activation = self.hook_sae_acts_pre(\n        sae_in @ (self.W_enc * self.r_mag.exp()) + self.b_mag\n    )\n    feature_magnitudes = self.activation_fn(magnitude_pre_activation)\n\n    # Combine gating and magnitudes\n    return self.hook_sae_acts_post(active_features * feature_magnitudes)\n</code></pre>"},{"location":"api/#sae_lens.GatedSAE.fold_W_dec_norm","title":"<code>fold_W_dec_norm()</code>","text":"<p>Override to handle gated-specific parameters.</p> Source code in <code>sae_lens/saes/gated_sae.py</code> <pre><code>@torch.no_grad()\ndef fold_W_dec_norm(self):\n    \"\"\"Override to handle gated-specific parameters.\"\"\"\n    W_dec_norms = self.W_dec.norm(dim=-1).unsqueeze(1)\n    self.W_dec.data = self.W_dec.data / W_dec_norms\n    self.W_enc.data = self.W_enc.data * W_dec_norms.T\n\n    # Gated-specific parameters need special handling\n    # r_mag doesn't need scaling since W_enc scaling is sufficient for magnitude path\n    self.b_gate.data = self.b_gate.data * W_dec_norms.squeeze()\n    self.b_mag.data = self.b_mag.data * W_dec_norms.squeeze()\n</code></pre>"},{"location":"api/#sae_lens.GatedSAEConfig","title":"<code>GatedSAEConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SAEConfig</code></p> <p>Configuration class for a GatedSAE.</p> Source code in <code>sae_lens/saes/gated_sae.py</code> <pre><code>@dataclass\nclass GatedSAEConfig(SAEConfig):\n    \"\"\"\n    Configuration class for a GatedSAE.\n    \"\"\"\n\n    @override\n    @classmethod\n    def architecture(cls) -&gt; str:\n        return \"gated\"\n</code></pre>"},{"location":"api/#sae_lens.GatedTrainingSAE","title":"<code>GatedTrainingSAE</code>","text":"<p>               Bases: <code>TrainingSAE[GatedTrainingSAEConfig]</code></p> <p>GatedTrainingSAE is a concrete implementation of BaseTrainingSAE for the \"gated\" SAE architecture. It implements:   - initialize_weights: sets up gating parameters (as in GatedSAE) plus optional training-specific init.   - encode: calls encode_with_hidden_pre (standard training approach).   - decode: linear transformation + hooking, same as GatedSAE or StandardTrainingSAE.   - encode_with_hidden_pre: gating logic.   - calculate_aux_loss: includes an auxiliary reconstruction path and gating-based sparsity penalty.   - training_forward_pass: calls encode_with_hidden_pre, decode, and sums up MSE + gating losses.</p> Source code in <code>sae_lens/saes/gated_sae.py</code> <pre><code>class GatedTrainingSAE(TrainingSAE[GatedTrainingSAEConfig]):\n    \"\"\"\n    GatedTrainingSAE is a concrete implementation of BaseTrainingSAE for the \"gated\" SAE architecture.\n    It implements:\n      - initialize_weights: sets up gating parameters (as in GatedSAE) plus optional training-specific init.\n      - encode: calls encode_with_hidden_pre (standard training approach).\n      - decode: linear transformation + hooking, same as GatedSAE or StandardTrainingSAE.\n      - encode_with_hidden_pre: gating logic.\n      - calculate_aux_loss: includes an auxiliary reconstruction path and gating-based sparsity penalty.\n      - training_forward_pass: calls encode_with_hidden_pre, decode, and sums up MSE + gating losses.\n    \"\"\"\n\n    b_gate: nn.Parameter  # type: ignore\n    b_mag: nn.Parameter  # type: ignore\n    r_mag: nn.Parameter  # type: ignore\n\n    def __init__(self, cfg: GatedTrainingSAEConfig, use_error_term: bool = False):\n        if use_error_term:\n            raise ValueError(\n                \"GatedSAE does not support `use_error_term`. Please set `use_error_term=False`.\"\n            )\n        super().__init__(cfg, use_error_term)\n\n    def initialize_weights(self) -&gt; None:\n        super().initialize_weights()\n        _init_weights_gated(self)\n\n    def encode_with_hidden_pre(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; tuple[Float[torch.Tensor, \"... d_sae\"], Float[torch.Tensor, \"... d_sae\"]]:\n        \"\"\"\n        Gated forward pass with pre-activation (for training).\n        \"\"\"\n        sae_in = self.process_sae_in(x)\n\n        # Gating path\n        gating_pre_activation = sae_in @ self.W_enc + self.b_gate\n        active_features = (gating_pre_activation &gt; 0).to(self.dtype)\n\n        # Magnitude path\n        magnitude_pre_activation = sae_in @ (self.W_enc * self.r_mag.exp()) + self.b_mag\n        magnitude_pre_activation = self.hook_sae_acts_pre(magnitude_pre_activation)\n\n        feature_magnitudes = self.activation_fn(magnitude_pre_activation)\n\n        # Combine gating path and magnitude path\n        feature_acts = self.hook_sae_acts_post(active_features * feature_magnitudes)\n\n        # Return both the final feature activations and the pre-activation (for logging or penalty)\n        return feature_acts, magnitude_pre_activation\n\n    def calculate_aux_loss(\n        self,\n        step_input: TrainStepInput,\n        feature_acts: torch.Tensor,\n        hidden_pre: torch.Tensor,\n        sae_out: torch.Tensor,\n    ) -&gt; dict[str, torch.Tensor]:\n        # Re-center the input if apply_b_dec_to_input is set\n        sae_in_centered = step_input.sae_in - (\n            self.b_dec * self.cfg.apply_b_dec_to_input\n        )\n\n        # The gating pre-activation (pi_gate) for the auxiliary path\n        pi_gate = sae_in_centered @ self.W_enc + self.b_gate\n        pi_gate_act = torch.relu(pi_gate)\n\n        # L1-like penalty scaled by W_dec norms\n        l1_loss = (\n            step_input.coefficients[\"l1\"]\n            * torch.sum(pi_gate_act * self.W_dec.norm(dim=1), dim=-1).mean()\n        )\n\n        # Aux reconstruction: reconstruct x purely from gating path\n        via_gate_reconstruction = pi_gate_act @ self.W_dec + self.b_dec\n        aux_recon_loss = (\n            (via_gate_reconstruction - step_input.sae_in).pow(2).sum(dim=-1).mean()\n        )\n\n        # Return both losses separately\n        return {\"l1_loss\": l1_loss, \"auxiliary_reconstruction_loss\": aux_recon_loss}\n\n    def log_histograms(self) -&gt; dict[str, NDArray[Any]]:\n        \"\"\"Log histograms of the weights and biases.\"\"\"\n        b_gate_dist = self.b_gate.detach().float().cpu().numpy()\n        b_mag_dist = self.b_mag.detach().float().cpu().numpy()\n        return {\n            **super().log_histograms(),\n            \"weights/b_gate\": b_gate_dist,\n            \"weights/b_mag\": b_mag_dist,\n        }\n\n    def get_coefficients(self) -&gt; dict[str, float | TrainCoefficientConfig]:\n        return {\n            \"l1\": TrainCoefficientConfig(\n                value=self.cfg.l1_coefficient,\n                warm_up_steps=self.cfg.l1_warm_up_steps,\n            ),\n        }\n\n    @torch.no_grad()\n    def fold_W_dec_norm(self):\n        \"\"\"Override to handle gated-specific parameters.\"\"\"\n        W_dec_norms = self.W_dec.norm(dim=-1).unsqueeze(1)\n        self.W_dec.data = self.W_dec.data / W_dec_norms\n        self.W_enc.data = self.W_enc.data * W_dec_norms.T\n\n        # Gated-specific parameters need special handling\n        # r_mag doesn't need scaling since W_enc scaling is sufficient for magnitude path\n        self.b_gate.data = self.b_gate.data * W_dec_norms.squeeze()\n        self.b_mag.data = self.b_mag.data * W_dec_norms.squeeze()\n</code></pre>"},{"location":"api/#sae_lens.GatedTrainingSAE.encode_with_hidden_pre","title":"<code>encode_with_hidden_pre(x)</code>","text":"<p>Gated forward pass with pre-activation (for training).</p> Source code in <code>sae_lens/saes/gated_sae.py</code> <pre><code>def encode_with_hidden_pre(\n    self, x: Float[torch.Tensor, \"... d_in\"]\n) -&gt; tuple[Float[torch.Tensor, \"... d_sae\"], Float[torch.Tensor, \"... d_sae\"]]:\n    \"\"\"\n    Gated forward pass with pre-activation (for training).\n    \"\"\"\n    sae_in = self.process_sae_in(x)\n\n    # Gating path\n    gating_pre_activation = sae_in @ self.W_enc + self.b_gate\n    active_features = (gating_pre_activation &gt; 0).to(self.dtype)\n\n    # Magnitude path\n    magnitude_pre_activation = sae_in @ (self.W_enc * self.r_mag.exp()) + self.b_mag\n    magnitude_pre_activation = self.hook_sae_acts_pre(magnitude_pre_activation)\n\n    feature_magnitudes = self.activation_fn(magnitude_pre_activation)\n\n    # Combine gating path and magnitude path\n    feature_acts = self.hook_sae_acts_post(active_features * feature_magnitudes)\n\n    # Return both the final feature activations and the pre-activation (for logging or penalty)\n    return feature_acts, magnitude_pre_activation\n</code></pre>"},{"location":"api/#sae_lens.GatedTrainingSAE.fold_W_dec_norm","title":"<code>fold_W_dec_norm()</code>","text":"<p>Override to handle gated-specific parameters.</p> Source code in <code>sae_lens/saes/gated_sae.py</code> <pre><code>@torch.no_grad()\ndef fold_W_dec_norm(self):\n    \"\"\"Override to handle gated-specific parameters.\"\"\"\n    W_dec_norms = self.W_dec.norm(dim=-1).unsqueeze(1)\n    self.W_dec.data = self.W_dec.data / W_dec_norms\n    self.W_enc.data = self.W_enc.data * W_dec_norms.T\n\n    # Gated-specific parameters need special handling\n    # r_mag doesn't need scaling since W_enc scaling is sufficient for magnitude path\n    self.b_gate.data = self.b_gate.data * W_dec_norms.squeeze()\n    self.b_mag.data = self.b_mag.data * W_dec_norms.squeeze()\n</code></pre>"},{"location":"api/#sae_lens.GatedTrainingSAE.log_histograms","title":"<code>log_histograms()</code>","text":"<p>Log histograms of the weights and biases.</p> Source code in <code>sae_lens/saes/gated_sae.py</code> <pre><code>def log_histograms(self) -&gt; dict[str, NDArray[Any]]:\n    \"\"\"Log histograms of the weights and biases.\"\"\"\n    b_gate_dist = self.b_gate.detach().float().cpu().numpy()\n    b_mag_dist = self.b_mag.detach().float().cpu().numpy()\n    return {\n        **super().log_histograms(),\n        \"weights/b_gate\": b_gate_dist,\n        \"weights/b_mag\": b_mag_dist,\n    }\n</code></pre>"},{"location":"api/#sae_lens.GatedTrainingSAEConfig","title":"<code>GatedTrainingSAEConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingSAEConfig</code></p> <p>Configuration class for training a GatedTrainingSAE.</p> Source code in <code>sae_lens/saes/gated_sae.py</code> <pre><code>@dataclass\nclass GatedTrainingSAEConfig(TrainingSAEConfig):\n    \"\"\"\n    Configuration class for training a GatedTrainingSAE.\n    \"\"\"\n\n    l1_coefficient: float = 1.0\n    l1_warm_up_steps: int = 0\n\n    @override\n    @classmethod\n    def architecture(cls) -&gt; str:\n        return \"gated\"\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer","title":"<code>HookedSAETransformer</code>","text":"<p>               Bases: <code>HookedTransformer</code></p> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>class HookedSAETransformer(HookedTransformer):\n    def __init__(\n        self,\n        *model_args: Any,\n        **model_kwargs: Any,\n    ):\n        \"\"\"Model initialization. Just HookedTransformer init, but adds a dictionary to keep track of attached SAEs.\n\n        Note that if you want to load the model from pretrained weights, you should use\n        :meth:`from_pretrained` instead.\n\n        Args:\n            *model_args: Positional arguments for HookedTransformer initialization\n            **model_kwargs: Keyword arguments for HookedTransformer initialization\n        \"\"\"\n        super().__init__(*model_args, **model_kwargs)\n\n        for block in self.blocks:\n            add_hook_in_to_mlp(block.mlp)  # type: ignore\n        self.setup()\n\n        self.acts_to_saes: dict[str, SAE] = {}  # type: ignore\n\n    def add_sae(self, sae: SAE[Any], use_error_term: bool | None = None):\n        \"\"\"Attaches an SAE to the model\n\n        WARNING: This sae will be permanantly attached until you remove it with reset_saes. This function will also overwrite any existing SAE attached to the same hook point.\n\n        Args:\n            sae: SparseAutoencoderBase. The SAE to attach to the model\n            use_error_term: (bool | None) If provided, will set the use_error_term attribute of the SAE to this value. Determines whether the SAE returns input or reconstruction. Defaults to None.\n        \"\"\"\n        act_name = sae.cfg.metadata.hook_name\n        if (act_name not in self.acts_to_saes) and (act_name not in self.hook_dict):\n            logging.warning(\n                f\"No hook found for {act_name}. Skipping. Check model.hook_dict for available hooks.\"\n            )\n            return\n\n        if use_error_term is not None:\n            if not hasattr(sae, \"_original_use_error_term\"):\n                sae._original_use_error_term = sae.use_error_term  # type: ignore\n            sae.use_error_term = use_error_term\n        self.acts_to_saes[act_name] = sae\n        set_deep_attr(self, act_name, sae)\n        self.setup()\n\n    def _reset_sae(self, act_name: str, prev_sae: SAE[Any] | None = None):\n        \"\"\"Resets an SAE that was attached to the model\n\n        By default will remove the SAE from that hook_point.\n        If prev_sae is provided, will replace the current SAE with the provided one.\n        This is mainly used to restore previously attached SAEs after temporarily running with different SAEs (eg with run_with_saes)\n\n        Args:\n            act_name: str. The hook_name of the SAE to reset\n            prev_sae: SAE | None. The SAE to replace the current one with. If None, will just remove the SAE from this hook point. Defaults to None\n        \"\"\"\n        if act_name not in self.acts_to_saes:\n            logging.warning(\n                f\"No SAE is attached to {act_name}. There's nothing to reset.\"\n            )\n            return\n\n        current_sae = self.acts_to_saes[act_name]\n        if hasattr(current_sae, \"_original_use_error_term\"):\n            current_sae.use_error_term = current_sae._original_use_error_term  # type: ignore\n            delattr(current_sae, \"_original_use_error_term\")\n\n        if prev_sae:\n            set_deep_attr(self, act_name, prev_sae)\n            self.acts_to_saes[act_name] = prev_sae\n        else:\n            set_deep_attr(self, act_name, HookPoint())\n            del self.acts_to_saes[act_name]\n\n    def reset_saes(\n        self,\n        act_names: str | list[str] | None = None,\n        prev_saes: list[SAE[Any] | None] | None = None,\n    ):\n        \"\"\"Reset the SAEs attached to the model\n\n        If act_names are provided will just reset SAEs attached to those hooks. Otherwise will reset all SAEs attached to the model.\n        Optionally can provide a list of prev_saes to reset to. This is mainly used to restore previously attached SAEs after temporarily running with different SAEs (eg with run_with_saes).\n\n        Args:\n            act_names (str | list[str] | None): The act_names of the SAEs to reset. If None, will reset all SAEs attached to the model. Defaults to None.\n            prev_saes (list[SAE | None] | None): List of SAEs to replace the current ones with. If None, will just remove the SAEs. Defaults to None.\n        \"\"\"\n        if isinstance(act_names, str):\n            act_names = [act_names]\n        elif act_names is None:\n            act_names = list(self.acts_to_saes.keys())\n\n        if prev_saes:\n            if len(act_names) != len(prev_saes):\n                raise ValueError(\"act_names and prev_saes must have the same length\")\n        else:\n            prev_saes = [None] * len(act_names)  # type: ignore\n\n        for act_name, prev_sae in zip(act_names, prev_saes):  # type: ignore\n            self._reset_sae(act_name, prev_sae)\n\n        self.setup()\n\n    def run_with_saes(\n        self,\n        *model_args: Any,\n        saes: SAE[Any] | list[SAE[Any]] = [],\n        reset_saes_end: bool = True,\n        use_error_term: bool | None = None,\n        **model_kwargs: Any,\n    ) -&gt; (\n        None\n        | Float[torch.Tensor, \"batch pos d_vocab\"]\n        | Loss\n        | tuple[Float[torch.Tensor, \"batch pos d_vocab\"], Loss]\n    ):\n        \"\"\"Wrapper around HookedTransformer forward pass.\n\n        Runs the model with the given SAEs attached for one forward pass, then removes them. By default, will reset all SAEs to original state after.\n\n        Args:\n            *model_args: Positional arguments for the model forward pass\n            saes: (SAE | list[SAE]) The SAEs to be attached for this forward pass\n            reset_saes_end (bool): If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.\n            use_error_term: (bool | None) If provided, will set the use_error_term attribute of all SAEs attached during this run to this value. Defaults to None.\n            **model_kwargs: Keyword arguments for the model forward pass\n        \"\"\"\n        with self.saes(\n            saes=saes, reset_saes_end=reset_saes_end, use_error_term=use_error_term\n        ):\n            return self(*model_args, **model_kwargs)\n\n    def run_with_cache_with_saes(\n        self,\n        *model_args: Any,\n        saes: SAE[Any] | list[SAE[Any]] = [],\n        reset_saes_end: bool = True,\n        use_error_term: bool | None = None,\n        return_cache_object: bool = True,\n        remove_batch_dim: bool = False,\n        **kwargs: Any,\n    ) -&gt; tuple[\n        None\n        | Float[torch.Tensor, \"batch pos d_vocab\"]\n        | Loss\n        | tuple[Float[torch.Tensor, \"batch pos d_vocab\"], Loss],\n        ActivationCache | dict[str, torch.Tensor],\n    ]:\n        \"\"\"Wrapper around 'run_with_cache' in HookedTransformer.\n\n        Attaches given SAEs before running the model with cache and then removes them.\n        By default, will reset all SAEs to original state after.\n\n        Args:\n            *model_args: Positional arguments for the model forward pass\n            saes: (SAE | list[SAE]) The SAEs to be attached for this forward pass\n            reset_saes_end: (bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.\n            use_error_term: (bool | None) If provided, will set the use_error_term attribute of all SAEs attached during this run to this value. Determines whether the SAE returns input or reconstruction. Defaults to None.\n            return_cache_object: (bool) if True, this will return an ActivationCache object, with a bunch of\n                useful HookedTransformer specific methods, otherwise it will return a dictionary of\n                activations as in HookedRootModule.\n            remove_batch_dim: (bool) Whether to remove the batch dimension (only works for batch_size==1). Defaults to False.\n            **kwargs: Keyword arguments for the model forward pass\n        \"\"\"\n        with self.saes(\n            saes=saes, reset_saes_end=reset_saes_end, use_error_term=use_error_term\n        ):\n            return self.run_with_cache(  # type: ignore\n                *model_args,\n                return_cache_object=return_cache_object,  # type: ignore\n                remove_batch_dim=remove_batch_dim,\n                **kwargs,\n            )\n\n    def run_with_hooks_with_saes(\n        self,\n        *model_args: Any,\n        saes: SAE[Any] | list[SAE[Any]] = [],\n        reset_saes_end: bool = True,\n        fwd_hooks: list[tuple[str | Callable, Callable]] = [],  # type: ignore\n        bwd_hooks: list[tuple[str | Callable, Callable]] = [],  # type: ignore\n        reset_hooks_end: bool = True,\n        clear_contexts: bool = False,\n        **model_kwargs: Any,\n    ):\n        \"\"\"Wrapper around 'run_with_hooks' in HookedTransformer.\n\n        Attaches the given SAEs to the model before running the model with hooks and then removes them.\n        By default, will reset all SAEs to original state after.\n\n        Args:\n            *model_args: Positional arguments for the model forward pass\n            saes: (SAE | list[SAE]) The SAEs to be attached for this forward pass\n            reset_saes_end: (bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. (default: True)\n            fwd_hooks: (list[tuple[str | Callable, Callable]]) List of forward hooks to apply\n            bwd_hooks: (list[tuple[str | Callable, Callable]]) List of backward hooks to apply\n            reset_hooks_end: (bool) Whether to reset the hooks at the end of the forward pass (default: True)\n            clear_contexts: (bool) Whether to clear the contexts at the end of the forward pass (default: False)\n            **model_kwargs: Keyword arguments for the model forward pass\n        \"\"\"\n        with self.saes(saes=saes, reset_saes_end=reset_saes_end):\n            return self.run_with_hooks(\n                *model_args,\n                fwd_hooks=fwd_hooks,\n                bwd_hooks=bwd_hooks,\n                reset_hooks_end=reset_hooks_end,\n                clear_contexts=clear_contexts,\n                **model_kwargs,\n            )\n\n    @contextmanager\n    def saes(\n        self,\n        saes: SAE[Any] | list[SAE[Any]] = [],\n        reset_saes_end: bool = True,\n        use_error_term: bool | None = None,\n    ):\n        \"\"\"\n        A context manager for adding temporary SAEs to the model.\n        See HookedTransformer.hooks for a similar context manager for hooks.\n        By default will keep track of previously attached SAEs, and restore them when the context manager exits.\n\n        Example:\n\n        .. code-block:: python\n\n            from transformer_lens import HookedSAETransformer\n            from sae_lens.saes.sae import SAE\n\n            model = HookedSAETransformer.from_pretrained('gpt2-small')\n            sae_cfg = SAEConfig(...)\n            sae = SAE(sae_cfg)\n            with model.saes(saes=[sae]):\n                spliced_logits = model(text)\n\n\n        Args:\n            saes (SAE | list[SAE]): SAEs to be attached.\n            reset_saes_end (bool): If True, removes all SAEs added by this context manager when the context manager exits, returning previously attached SAEs to their original state.\n            use_error_term (bool | None): If provided, will set the use_error_term attribute of all SAEs attached during this run to this value. Defaults to None.\n        \"\"\"\n        act_names_to_reset = []\n        prev_saes = []\n        if isinstance(saes, SAE):\n            saes = [saes]\n        try:\n            for sae in saes:\n                act_names_to_reset.append(sae.cfg.metadata.hook_name)\n                prev_sae = self.acts_to_saes.get(sae.cfg.metadata.hook_name, None)\n                prev_saes.append(prev_sae)\n                self.add_sae(sae, use_error_term=use_error_term)\n            yield self\n        finally:\n            if reset_saes_end:\n                self.reset_saes(act_names_to_reset, prev_saes)\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.__init__","title":"<code>__init__(*model_args, **model_kwargs)</code>","text":"<p>Model initialization. Just HookedTransformer init, but adds a dictionary to keep track of attached SAEs.</p> <p>Note that if you want to load the model from pretrained weights, you should use :meth:<code>from_pretrained</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>*model_args</code> <code>Any</code> <p>Positional arguments for HookedTransformer initialization</p> <code>()</code> <code>**model_kwargs</code> <code>Any</code> <p>Keyword arguments for HookedTransformer initialization</p> <code>{}</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>def __init__(\n    self,\n    *model_args: Any,\n    **model_kwargs: Any,\n):\n    \"\"\"Model initialization. Just HookedTransformer init, but adds a dictionary to keep track of attached SAEs.\n\n    Note that if you want to load the model from pretrained weights, you should use\n    :meth:`from_pretrained` instead.\n\n    Args:\n        *model_args: Positional arguments for HookedTransformer initialization\n        **model_kwargs: Keyword arguments for HookedTransformer initialization\n    \"\"\"\n    super().__init__(*model_args, **model_kwargs)\n\n    for block in self.blocks:\n        add_hook_in_to_mlp(block.mlp)  # type: ignore\n    self.setup()\n\n    self.acts_to_saes: dict[str, SAE] = {}  # type: ignore\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.add_sae","title":"<code>add_sae(sae, use_error_term=None)</code>","text":"<p>Attaches an SAE to the model</p> <p>WARNING: This sae will be permanantly attached until you remove it with reset_saes. This function will also overwrite any existing SAE attached to the same hook point.</p> <p>Parameters:</p> Name Type Description Default <code>sae</code> <code>SAE[Any]</code> <p>SparseAutoencoderBase. The SAE to attach to the model</p> required <code>use_error_term</code> <code>bool | None</code> <p>(bool | None) If provided, will set the use_error_term attribute of the SAE to this value. Determines whether the SAE returns input or reconstruction. Defaults to None.</p> <code>None</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>def add_sae(self, sae: SAE[Any], use_error_term: bool | None = None):\n    \"\"\"Attaches an SAE to the model\n\n    WARNING: This sae will be permanantly attached until you remove it with reset_saes. This function will also overwrite any existing SAE attached to the same hook point.\n\n    Args:\n        sae: SparseAutoencoderBase. The SAE to attach to the model\n        use_error_term: (bool | None) If provided, will set the use_error_term attribute of the SAE to this value. Determines whether the SAE returns input or reconstruction. Defaults to None.\n    \"\"\"\n    act_name = sae.cfg.metadata.hook_name\n    if (act_name not in self.acts_to_saes) and (act_name not in self.hook_dict):\n        logging.warning(\n            f\"No hook found for {act_name}. Skipping. Check model.hook_dict for available hooks.\"\n        )\n        return\n\n    if use_error_term is not None:\n        if not hasattr(sae, \"_original_use_error_term\"):\n            sae._original_use_error_term = sae.use_error_term  # type: ignore\n        sae.use_error_term = use_error_term\n    self.acts_to_saes[act_name] = sae\n    set_deep_attr(self, act_name, sae)\n    self.setup()\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.reset_saes","title":"<code>reset_saes(act_names=None, prev_saes=None)</code>","text":"<p>Reset the SAEs attached to the model</p> <p>If act_names are provided will just reset SAEs attached to those hooks. Otherwise will reset all SAEs attached to the model. Optionally can provide a list of prev_saes to reset to. This is mainly used to restore previously attached SAEs after temporarily running with different SAEs (eg with run_with_saes).</p> <p>Parameters:</p> Name Type Description Default <code>act_names</code> <code>str | list[str] | None</code> <p>The act_names of the SAEs to reset. If None, will reset all SAEs attached to the model. Defaults to None.</p> <code>None</code> <code>prev_saes</code> <code>list[SAE | None] | None</code> <p>List of SAEs to replace the current ones with. If None, will just remove the SAEs. Defaults to None.</p> <code>None</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>def reset_saes(\n    self,\n    act_names: str | list[str] | None = None,\n    prev_saes: list[SAE[Any] | None] | None = None,\n):\n    \"\"\"Reset the SAEs attached to the model\n\n    If act_names are provided will just reset SAEs attached to those hooks. Otherwise will reset all SAEs attached to the model.\n    Optionally can provide a list of prev_saes to reset to. This is mainly used to restore previously attached SAEs after temporarily running with different SAEs (eg with run_with_saes).\n\n    Args:\n        act_names (str | list[str] | None): The act_names of the SAEs to reset. If None, will reset all SAEs attached to the model. Defaults to None.\n        prev_saes (list[SAE | None] | None): List of SAEs to replace the current ones with. If None, will just remove the SAEs. Defaults to None.\n    \"\"\"\n    if isinstance(act_names, str):\n        act_names = [act_names]\n    elif act_names is None:\n        act_names = list(self.acts_to_saes.keys())\n\n    if prev_saes:\n        if len(act_names) != len(prev_saes):\n            raise ValueError(\"act_names and prev_saes must have the same length\")\n    else:\n        prev_saes = [None] * len(act_names)  # type: ignore\n\n    for act_name, prev_sae in zip(act_names, prev_saes):  # type: ignore\n        self._reset_sae(act_name, prev_sae)\n\n    self.setup()\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.run_with_cache_with_saes","title":"<code>run_with_cache_with_saes(*model_args, saes=[], reset_saes_end=True, use_error_term=None, return_cache_object=True, remove_batch_dim=False, **kwargs)</code>","text":"<p>Wrapper around 'run_with_cache' in HookedTransformer.</p> <p>Attaches given SAEs before running the model with cache and then removes them. By default, will reset all SAEs to original state after.</p> <p>Parameters:</p> Name Type Description Default <code>*model_args</code> <code>Any</code> <p>Positional arguments for the model forward pass</p> <code>()</code> <code>saes</code> <code>SAE[Any] | list[SAE[Any]]</code> <p>(SAE | list[SAE]) The SAEs to be attached for this forward pass</p> <code>[]</code> <code>reset_saes_end</code> <code>bool</code> <p>(bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.</p> <code>True</code> <code>use_error_term</code> <code>bool | None</code> <p>(bool | None) If provided, will set the use_error_term attribute of all SAEs attached during this run to this value. Determines whether the SAE returns input or reconstruction. Defaults to None.</p> <code>None</code> <code>return_cache_object</code> <code>bool</code> <p>(bool) if True, this will return an ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a dictionary of activations as in HookedRootModule.</p> <code>True</code> <code>remove_batch_dim</code> <code>bool</code> <p>(bool) Whether to remove the batch dimension (only works for batch_size==1). Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for the model forward pass</p> <code>{}</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>def run_with_cache_with_saes(\n    self,\n    *model_args: Any,\n    saes: SAE[Any] | list[SAE[Any]] = [],\n    reset_saes_end: bool = True,\n    use_error_term: bool | None = None,\n    return_cache_object: bool = True,\n    remove_batch_dim: bool = False,\n    **kwargs: Any,\n) -&gt; tuple[\n    None\n    | Float[torch.Tensor, \"batch pos d_vocab\"]\n    | Loss\n    | tuple[Float[torch.Tensor, \"batch pos d_vocab\"], Loss],\n    ActivationCache | dict[str, torch.Tensor],\n]:\n    \"\"\"Wrapper around 'run_with_cache' in HookedTransformer.\n\n    Attaches given SAEs before running the model with cache and then removes them.\n    By default, will reset all SAEs to original state after.\n\n    Args:\n        *model_args: Positional arguments for the model forward pass\n        saes: (SAE | list[SAE]) The SAEs to be attached for this forward pass\n        reset_saes_end: (bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.\n        use_error_term: (bool | None) If provided, will set the use_error_term attribute of all SAEs attached during this run to this value. Determines whether the SAE returns input or reconstruction. Defaults to None.\n        return_cache_object: (bool) if True, this will return an ActivationCache object, with a bunch of\n            useful HookedTransformer specific methods, otherwise it will return a dictionary of\n            activations as in HookedRootModule.\n        remove_batch_dim: (bool) Whether to remove the batch dimension (only works for batch_size==1). Defaults to False.\n        **kwargs: Keyword arguments for the model forward pass\n    \"\"\"\n    with self.saes(\n        saes=saes, reset_saes_end=reset_saes_end, use_error_term=use_error_term\n    ):\n        return self.run_with_cache(  # type: ignore\n            *model_args,\n            return_cache_object=return_cache_object,  # type: ignore\n            remove_batch_dim=remove_batch_dim,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.run_with_hooks_with_saes","title":"<code>run_with_hooks_with_saes(*model_args, saes=[], reset_saes_end=True, fwd_hooks=[], bwd_hooks=[], reset_hooks_end=True, clear_contexts=False, **model_kwargs)</code>","text":"<p>Wrapper around 'run_with_hooks' in HookedTransformer.</p> <p>Attaches the given SAEs to the model before running the model with hooks and then removes them. By default, will reset all SAEs to original state after.</p> <p>Parameters:</p> Name Type Description Default <code>*model_args</code> <code>Any</code> <p>Positional arguments for the model forward pass</p> <code>()</code> <code>saes</code> <code>SAE[Any] | list[SAE[Any]]</code> <p>(SAE | list[SAE]) The SAEs to be attached for this forward pass</p> <code>[]</code> <code>reset_saes_end</code> <code>bool</code> <p>(bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. (default: True)</p> <code>True</code> <code>fwd_hooks</code> <code>list[tuple[str | Callable, Callable]]</code> <p>(list[tuple[str | Callable, Callable]]) List of forward hooks to apply</p> <code>[]</code> <code>bwd_hooks</code> <code>list[tuple[str | Callable, Callable]]</code> <p>(list[tuple[str | Callable, Callable]]) List of backward hooks to apply</p> <code>[]</code> <code>reset_hooks_end</code> <code>bool</code> <p>(bool) Whether to reset the hooks at the end of the forward pass (default: True)</p> <code>True</code> <code>clear_contexts</code> <code>bool</code> <p>(bool) Whether to clear the contexts at the end of the forward pass (default: False)</p> <code>False</code> <code>**model_kwargs</code> <code>Any</code> <p>Keyword arguments for the model forward pass</p> <code>{}</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>def run_with_hooks_with_saes(\n    self,\n    *model_args: Any,\n    saes: SAE[Any] | list[SAE[Any]] = [],\n    reset_saes_end: bool = True,\n    fwd_hooks: list[tuple[str | Callable, Callable]] = [],  # type: ignore\n    bwd_hooks: list[tuple[str | Callable, Callable]] = [],  # type: ignore\n    reset_hooks_end: bool = True,\n    clear_contexts: bool = False,\n    **model_kwargs: Any,\n):\n    \"\"\"Wrapper around 'run_with_hooks' in HookedTransformer.\n\n    Attaches the given SAEs to the model before running the model with hooks and then removes them.\n    By default, will reset all SAEs to original state after.\n\n    Args:\n        *model_args: Positional arguments for the model forward pass\n        saes: (SAE | list[SAE]) The SAEs to be attached for this forward pass\n        reset_saes_end: (bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. (default: True)\n        fwd_hooks: (list[tuple[str | Callable, Callable]]) List of forward hooks to apply\n        bwd_hooks: (list[tuple[str | Callable, Callable]]) List of backward hooks to apply\n        reset_hooks_end: (bool) Whether to reset the hooks at the end of the forward pass (default: True)\n        clear_contexts: (bool) Whether to clear the contexts at the end of the forward pass (default: False)\n        **model_kwargs: Keyword arguments for the model forward pass\n    \"\"\"\n    with self.saes(saes=saes, reset_saes_end=reset_saes_end):\n        return self.run_with_hooks(\n            *model_args,\n            fwd_hooks=fwd_hooks,\n            bwd_hooks=bwd_hooks,\n            reset_hooks_end=reset_hooks_end,\n            clear_contexts=clear_contexts,\n            **model_kwargs,\n        )\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.run_with_saes","title":"<code>run_with_saes(*model_args, saes=[], reset_saes_end=True, use_error_term=None, **model_kwargs)</code>","text":"<p>Wrapper around HookedTransformer forward pass.</p> <p>Runs the model with the given SAEs attached for one forward pass, then removes them. By default, will reset all SAEs to original state after.</p> <p>Parameters:</p> Name Type Description Default <code>*model_args</code> <code>Any</code> <p>Positional arguments for the model forward pass</p> <code>()</code> <code>saes</code> <code>SAE[Any] | list[SAE[Any]]</code> <p>(SAE | list[SAE]) The SAEs to be attached for this forward pass</p> <code>[]</code> <code>reset_saes_end</code> <code>bool</code> <p>If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.</p> <code>True</code> <code>use_error_term</code> <code>bool | None</code> <p>(bool | None) If provided, will set the use_error_term attribute of all SAEs attached during this run to this value. Defaults to None.</p> <code>None</code> <code>**model_kwargs</code> <code>Any</code> <p>Keyword arguments for the model forward pass</p> <code>{}</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>def run_with_saes(\n    self,\n    *model_args: Any,\n    saes: SAE[Any] | list[SAE[Any]] = [],\n    reset_saes_end: bool = True,\n    use_error_term: bool | None = None,\n    **model_kwargs: Any,\n) -&gt; (\n    None\n    | Float[torch.Tensor, \"batch pos d_vocab\"]\n    | Loss\n    | tuple[Float[torch.Tensor, \"batch pos d_vocab\"], Loss]\n):\n    \"\"\"Wrapper around HookedTransformer forward pass.\n\n    Runs the model with the given SAEs attached for one forward pass, then removes them. By default, will reset all SAEs to original state after.\n\n    Args:\n        *model_args: Positional arguments for the model forward pass\n        saes: (SAE | list[SAE]) The SAEs to be attached for this forward pass\n        reset_saes_end (bool): If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.\n        use_error_term: (bool | None) If provided, will set the use_error_term attribute of all SAEs attached during this run to this value. Defaults to None.\n        **model_kwargs: Keyword arguments for the model forward pass\n    \"\"\"\n    with self.saes(\n        saes=saes, reset_saes_end=reset_saes_end, use_error_term=use_error_term\n    ):\n        return self(*model_args, **model_kwargs)\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.saes","title":"<code>saes(saes=[], reset_saes_end=True, use_error_term=None)</code>","text":"<p>A context manager for adding temporary SAEs to the model. See HookedTransformer.hooks for a similar context manager for hooks. By default will keep track of previously attached SAEs, and restore them when the context manager exits.</p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code>from transformer_lens import HookedSAETransformer\nfrom sae_lens.saes.sae import SAE\n\nmodel = HookedSAETransformer.from_pretrained('gpt2-small')\nsae_cfg = SAEConfig(...)\nsae = SAE(sae_cfg)\nwith model.saes(saes=[sae]):\n    spliced_logits = model(text)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>saes</code> <code>SAE | list[SAE]</code> <p>SAEs to be attached.</p> <code>[]</code> <code>reset_saes_end</code> <code>bool</code> <p>If True, removes all SAEs added by this context manager when the context manager exits, returning previously attached SAEs to their original state.</p> <code>True</code> <code>use_error_term</code> <code>bool | None</code> <p>If provided, will set the use_error_term attribute of all SAEs attached during this run to this value. Defaults to None.</p> <code>None</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>@contextmanager\ndef saes(\n    self,\n    saes: SAE[Any] | list[SAE[Any]] = [],\n    reset_saes_end: bool = True,\n    use_error_term: bool | None = None,\n):\n    \"\"\"\n    A context manager for adding temporary SAEs to the model.\n    See HookedTransformer.hooks for a similar context manager for hooks.\n    By default will keep track of previously attached SAEs, and restore them when the context manager exits.\n\n    Example:\n\n    .. code-block:: python\n\n        from transformer_lens import HookedSAETransformer\n        from sae_lens.saes.sae import SAE\n\n        model = HookedSAETransformer.from_pretrained('gpt2-small')\n        sae_cfg = SAEConfig(...)\n        sae = SAE(sae_cfg)\n        with model.saes(saes=[sae]):\n            spliced_logits = model(text)\n\n\n    Args:\n        saes (SAE | list[SAE]): SAEs to be attached.\n        reset_saes_end (bool): If True, removes all SAEs added by this context manager when the context manager exits, returning previously attached SAEs to their original state.\n        use_error_term (bool | None): If provided, will set the use_error_term attribute of all SAEs attached during this run to this value. Defaults to None.\n    \"\"\"\n    act_names_to_reset = []\n    prev_saes = []\n    if isinstance(saes, SAE):\n        saes = [saes]\n    try:\n        for sae in saes:\n            act_names_to_reset.append(sae.cfg.metadata.hook_name)\n            prev_sae = self.acts_to_saes.get(sae.cfg.metadata.hook_name, None)\n            prev_saes.append(prev_sae)\n            self.add_sae(sae, use_error_term=use_error_term)\n        yield self\n    finally:\n        if reset_saes_end:\n            self.reset_saes(act_names_to_reset, prev_saes)\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUSAE","title":"<code>JumpReLUSAE</code>","text":"<p>               Bases: <code>SAE[JumpReLUSAEConfig]</code></p> <p>JumpReLUSAE is an inference-only implementation of a Sparse Autoencoder (SAE) using a JumpReLU activation. For each unit, if its pre-activation is &lt;= threshold, that unit is zeroed out; otherwise, it follows a user-specified activation function (e.g., ReLU etc.).</p> It implements <ul> <li>initialize_weights: sets up parameters, including a threshold.</li> <li>encode: computes the feature activations using JumpReLU.</li> <li>decode: reconstructs the input from the feature activations.</li> </ul> <p>The BaseSAE.forward() method automatically calls encode and decode, including any error-term processing if configured.</p> Source code in <code>sae_lens/saes/jumprelu_sae.py</code> <pre><code>class JumpReLUSAE(SAE[JumpReLUSAEConfig]):\n    \"\"\"\n    JumpReLUSAE is an inference-only implementation of a Sparse Autoencoder (SAE)\n    using a JumpReLU activation. For each unit, if its pre-activation is\n    &lt;= threshold, that unit is zeroed out; otherwise, it follows a user-specified\n    activation function (e.g., ReLU etc.).\n\n    It implements:\n      - initialize_weights: sets up parameters, including a threshold.\n      - encode: computes the feature activations using JumpReLU.\n      - decode: reconstructs the input from the feature activations.\n\n    The BaseSAE.forward() method automatically calls encode and decode,\n    including any error-term processing if configured.\n    \"\"\"\n\n    b_enc: nn.Parameter\n    threshold: nn.Parameter\n\n    def __init__(self, cfg: JumpReLUSAEConfig, use_error_term: bool = False):\n        super().__init__(cfg, use_error_term)\n\n    @override\n    def initialize_weights(self) -&gt; None:\n        super().initialize_weights()\n        self.threshold = nn.Parameter(\n            torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)\n        )\n        self.b_enc = nn.Parameter(\n            torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)\n        )\n\n    def encode(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n        \"\"\"\n        Encode the input tensor into the feature space using JumpReLU.\n        The threshold parameter determines which units remain active.\n        \"\"\"\n        sae_in = self.process_sae_in(x)\n        hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n\n        # 1) Apply the base \"activation_fn\" from config (e.g., ReLU).\n        base_acts = self.activation_fn(hidden_pre)\n\n        # 2) Zero out any unit whose (hidden_pre &lt;= threshold).\n        #    We cast the boolean mask to the same dtype for safe multiplication.\n        jump_relu_mask = (hidden_pre &gt; self.threshold).to(base_acts.dtype)\n\n        # 3) Multiply the normally activated units by that mask.\n        return self.hook_sae_acts_post(base_acts * jump_relu_mask)\n\n    def decode(\n        self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n    ) -&gt; Float[torch.Tensor, \"... d_in\"]:\n        \"\"\"\n        Decode the feature activations back to the input space.\n        Follows the same steps as StandardSAE: apply scaling, transform, hook, and optionally reshape.\n        \"\"\"\n        sae_out_pre = feature_acts @ self.W_dec + self.b_dec\n        sae_out_pre = self.hook_sae_recons(sae_out_pre)\n        sae_out_pre = self.run_time_activation_norm_fn_out(sae_out_pre)\n        return self.reshape_fn_out(sae_out_pre, self.d_head)\n\n    @torch.no_grad()\n    def fold_W_dec_norm(self):\n        \"\"\"\n        Override to properly handle threshold adjustment with W_dec norms.\n        When we scale the encoder weights, we need to scale the threshold\n        by the same factor to maintain the same sparsity pattern.\n        \"\"\"\n        # Save the current threshold before calling parent method\n        current_thresh = self.threshold.clone()\n\n        # Get W_dec norms that will be used for scaling\n        W_dec_norms = self.W_dec.norm(dim=-1)\n\n        # Call parent implementation to handle W_enc, W_dec, and b_enc adjustment\n        super().fold_W_dec_norm()\n\n        # Scale the threshold by the same factor as we scaled b_enc\n        # This ensures the same features remain active/inactive after folding\n        self.threshold.data = current_thresh * W_dec_norms\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUSAE.decode","title":"<code>decode(feature_acts)</code>","text":"<p>Decode the feature activations back to the input space. Follows the same steps as StandardSAE: apply scaling, transform, hook, and optionally reshape.</p> Source code in <code>sae_lens/saes/jumprelu_sae.py</code> <pre><code>def decode(\n    self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n) -&gt; Float[torch.Tensor, \"... d_in\"]:\n    \"\"\"\n    Decode the feature activations back to the input space.\n    Follows the same steps as StandardSAE: apply scaling, transform, hook, and optionally reshape.\n    \"\"\"\n    sae_out_pre = feature_acts @ self.W_dec + self.b_dec\n    sae_out_pre = self.hook_sae_recons(sae_out_pre)\n    sae_out_pre = self.run_time_activation_norm_fn_out(sae_out_pre)\n    return self.reshape_fn_out(sae_out_pre, self.d_head)\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUSAE.encode","title":"<code>encode(x)</code>","text":"<p>Encode the input tensor into the feature space using JumpReLU. The threshold parameter determines which units remain active.</p> Source code in <code>sae_lens/saes/jumprelu_sae.py</code> <pre><code>def encode(\n    self, x: Float[torch.Tensor, \"... d_in\"]\n) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n    \"\"\"\n    Encode the input tensor into the feature space using JumpReLU.\n    The threshold parameter determines which units remain active.\n    \"\"\"\n    sae_in = self.process_sae_in(x)\n    hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n\n    # 1) Apply the base \"activation_fn\" from config (e.g., ReLU).\n    base_acts = self.activation_fn(hidden_pre)\n\n    # 2) Zero out any unit whose (hidden_pre &lt;= threshold).\n    #    We cast the boolean mask to the same dtype for safe multiplication.\n    jump_relu_mask = (hidden_pre &gt; self.threshold).to(base_acts.dtype)\n\n    # 3) Multiply the normally activated units by that mask.\n    return self.hook_sae_acts_post(base_acts * jump_relu_mask)\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUSAE.fold_W_dec_norm","title":"<code>fold_W_dec_norm()</code>","text":"<p>Override to properly handle threshold adjustment with W_dec norms. When we scale the encoder weights, we need to scale the threshold by the same factor to maintain the same sparsity pattern.</p> Source code in <code>sae_lens/saes/jumprelu_sae.py</code> <pre><code>@torch.no_grad()\ndef fold_W_dec_norm(self):\n    \"\"\"\n    Override to properly handle threshold adjustment with W_dec norms.\n    When we scale the encoder weights, we need to scale the threshold\n    by the same factor to maintain the same sparsity pattern.\n    \"\"\"\n    # Save the current threshold before calling parent method\n    current_thresh = self.threshold.clone()\n\n    # Get W_dec norms that will be used for scaling\n    W_dec_norms = self.W_dec.norm(dim=-1)\n\n    # Call parent implementation to handle W_enc, W_dec, and b_enc adjustment\n    super().fold_W_dec_norm()\n\n    # Scale the threshold by the same factor as we scaled b_enc\n    # This ensures the same features remain active/inactive after folding\n    self.threshold.data = current_thresh * W_dec_norms\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUSAEConfig","title":"<code>JumpReLUSAEConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SAEConfig</code></p> <p>Configuration class for a JumpReLUSAE.</p> Source code in <code>sae_lens/saes/jumprelu_sae.py</code> <pre><code>@dataclass\nclass JumpReLUSAEConfig(SAEConfig):\n    \"\"\"\n    Configuration class for a JumpReLUSAE.\n    \"\"\"\n\n    @override\n    @classmethod\n    def architecture(cls) -&gt; str:\n        return \"jumprelu\"\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTrainingSAE","title":"<code>JumpReLUTrainingSAE</code>","text":"<p>               Bases: <code>TrainingSAE[JumpReLUTrainingSAEConfig]</code></p> <p>JumpReLUTrainingSAE is a training-focused implementation of a SAE using a JumpReLU activation.</p> <p>Similar to the inference-only JumpReLUSAE, but with:   - A learnable log-threshold parameter (instead of a raw threshold).   - A specialized auxiliary loss term for sparsity (L0 or similar).</p> <p>Methods of interest include: - initialize_weights: sets up W_enc, b_enc, W_dec, b_dec, and log_threshold. - encode_with_hidden_pre_jumprelu: runs a forward pass for training. - training_forward_pass: calculates MSE and auxiliary losses, returning a TrainStepOutput.</p> Source code in <code>sae_lens/saes/jumprelu_sae.py</code> <pre><code>class JumpReLUTrainingSAE(TrainingSAE[JumpReLUTrainingSAEConfig]):\n    \"\"\"\n    JumpReLUTrainingSAE is a training-focused implementation of a SAE using a JumpReLU activation.\n\n    Similar to the inference-only JumpReLUSAE, but with:\n      - A learnable log-threshold parameter (instead of a raw threshold).\n      - A specialized auxiliary loss term for sparsity (L0 or similar).\n\n    Methods of interest include:\n    - initialize_weights: sets up W_enc, b_enc, W_dec, b_dec, and log_threshold.\n    - encode_with_hidden_pre_jumprelu: runs a forward pass for training.\n    - training_forward_pass: calculates MSE and auxiliary losses, returning a TrainStepOutput.\n    \"\"\"\n\n    b_enc: nn.Parameter\n    log_threshold: nn.Parameter\n\n    def __init__(self, cfg: JumpReLUTrainingSAEConfig, use_error_term: bool = False):\n        super().__init__(cfg, use_error_term)\n\n        # We'll store a bandwidth for the training approach, if needed\n        self.bandwidth = cfg.jumprelu_bandwidth\n\n        # In typical JumpReLU training code, we may track a log_threshold:\n        self.log_threshold = nn.Parameter(\n            torch.ones(self.cfg.d_sae, dtype=self.dtype, device=self.device)\n            * np.log(cfg.jumprelu_init_threshold)\n        )\n\n    @override\n    def initialize_weights(self) -&gt; None:\n        \"\"\"\n        Initialize parameters like the base SAE, but also add log_threshold.\n        \"\"\"\n        super().initialize_weights()\n        # Encoder Bias\n        self.b_enc = nn.Parameter(\n            torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)\n        )\n\n    @property\n    def threshold(self) -&gt; torch.Tensor:\n        \"\"\"\n        Returns the parameterized threshold &gt; 0 for each unit.\n        threshold = exp(log_threshold).\n        \"\"\"\n        return torch.exp(self.log_threshold)\n\n    def encode_with_hidden_pre(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; tuple[Float[torch.Tensor, \"... d_sae\"], Float[torch.Tensor, \"... d_sae\"]]:\n        sae_in = self.process_sae_in(x)\n\n        hidden_pre = sae_in @ self.W_enc + self.b_enc\n        feature_acts = JumpReLU.apply(hidden_pre, self.threshold, self.bandwidth)\n\n        return feature_acts, hidden_pre  # type: ignore\n\n    @override\n    def calculate_aux_loss(\n        self,\n        step_input: TrainStepInput,\n        feature_acts: torch.Tensor,\n        hidden_pre: torch.Tensor,\n        sae_out: torch.Tensor,\n    ) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Calculate architecture-specific auxiliary loss terms.\"\"\"\n\n        threshold = self.threshold\n        W_dec_norm = self.W_dec.norm(dim=1)\n        if self.cfg.jumprelu_sparsity_loss_mode == \"step\":\n            l0 = torch.sum(\n                Step.apply(hidden_pre, threshold, self.bandwidth),  # type: ignore\n                dim=-1,\n            )\n            l0_loss = (step_input.coefficients[\"l0\"] * l0).mean()\n        elif self.cfg.jumprelu_sparsity_loss_mode == \"tanh\":\n            per_item_l0_loss = torch.tanh(\n                self.cfg.jumprelu_tanh_scale * feature_acts * W_dec_norm\n            ).sum(dim=-1)\n            l0_loss = (step_input.coefficients[\"l0\"] * per_item_l0_loss).mean()\n        else:\n            raise ValueError(\n                f\"Invalid sparsity loss mode: {self.cfg.jumprelu_sparsity_loss_mode}\"\n            )\n        losses = {\"l0_loss\": l0_loss}\n\n        if self.cfg.pre_act_loss_coefficient is not None:\n            losses[\"pre_act_loss\"] = calculate_pre_act_loss(\n                self.cfg.pre_act_loss_coefficient,\n                threshold,\n                hidden_pre,\n                step_input.dead_neuron_mask,\n                W_dec_norm,\n            )\n        return losses\n\n    @override\n    def get_coefficients(self) -&gt; dict[str, float | TrainCoefficientConfig]:\n        return {\n            \"l0\": TrainCoefficientConfig(\n                value=self.cfg.l0_coefficient,\n                warm_up_steps=self.cfg.l0_warm_up_steps,\n            ),\n        }\n\n    @torch.no_grad()\n    def fold_W_dec_norm(self):\n        \"\"\"\n        Override to properly handle threshold adjustment with W_dec norms.\n        \"\"\"\n        # Save the current threshold before we call the parent method\n        current_thresh = self.threshold.clone()\n\n        # Get W_dec norms\n        W_dec_norms = self.W_dec.norm(dim=-1).unsqueeze(1)\n\n        # Call parent implementation to handle W_enc and W_dec adjustment\n        super().fold_W_dec_norm()\n\n        # Fix: Use squeeze() instead of squeeze(-1) to match old behavior\n        self.log_threshold.data = torch.log(current_thresh * W_dec_norms.squeeze())\n\n    def process_state_dict_for_saving(self, state_dict: dict[str, Any]) -&gt; None:\n        \"\"\"Convert log_threshold to threshold for saving\"\"\"\n        if \"log_threshold\" in state_dict:\n            threshold = torch.exp(state_dict[\"log_threshold\"]).detach().contiguous()\n            del state_dict[\"log_threshold\"]\n            state_dict[\"threshold\"] = threshold\n\n    def process_state_dict_for_loading(self, state_dict: dict[str, Any]) -&gt; None:\n        \"\"\"Convert threshold to log_threshold for loading\"\"\"\n        if \"threshold\" in state_dict:\n            threshold = state_dict[\"threshold\"]\n            del state_dict[\"threshold\"]\n            state_dict[\"log_threshold\"] = torch.log(threshold).detach().contiguous()\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTrainingSAE.threshold","title":"<code>threshold: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the parameterized threshold &gt; 0 for each unit. threshold = exp(log_threshold).</p>"},{"location":"api/#sae_lens.JumpReLUTrainingSAE.calculate_aux_loss","title":"<code>calculate_aux_loss(step_input, feature_acts, hidden_pre, sae_out)</code>","text":"<p>Calculate architecture-specific auxiliary loss terms.</p> Source code in <code>sae_lens/saes/jumprelu_sae.py</code> <pre><code>@override\ndef calculate_aux_loss(\n    self,\n    step_input: TrainStepInput,\n    feature_acts: torch.Tensor,\n    hidden_pre: torch.Tensor,\n    sae_out: torch.Tensor,\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Calculate architecture-specific auxiliary loss terms.\"\"\"\n\n    threshold = self.threshold\n    W_dec_norm = self.W_dec.norm(dim=1)\n    if self.cfg.jumprelu_sparsity_loss_mode == \"step\":\n        l0 = torch.sum(\n            Step.apply(hidden_pre, threshold, self.bandwidth),  # type: ignore\n            dim=-1,\n        )\n        l0_loss = (step_input.coefficients[\"l0\"] * l0).mean()\n    elif self.cfg.jumprelu_sparsity_loss_mode == \"tanh\":\n        per_item_l0_loss = torch.tanh(\n            self.cfg.jumprelu_tanh_scale * feature_acts * W_dec_norm\n        ).sum(dim=-1)\n        l0_loss = (step_input.coefficients[\"l0\"] * per_item_l0_loss).mean()\n    else:\n        raise ValueError(\n            f\"Invalid sparsity loss mode: {self.cfg.jumprelu_sparsity_loss_mode}\"\n        )\n    losses = {\"l0_loss\": l0_loss}\n\n    if self.cfg.pre_act_loss_coefficient is not None:\n        losses[\"pre_act_loss\"] = calculate_pre_act_loss(\n            self.cfg.pre_act_loss_coefficient,\n            threshold,\n            hidden_pre,\n            step_input.dead_neuron_mask,\n            W_dec_norm,\n        )\n    return losses\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTrainingSAE.fold_W_dec_norm","title":"<code>fold_W_dec_norm()</code>","text":"<p>Override to properly handle threshold adjustment with W_dec norms.</p> Source code in <code>sae_lens/saes/jumprelu_sae.py</code> <pre><code>@torch.no_grad()\ndef fold_W_dec_norm(self):\n    \"\"\"\n    Override to properly handle threshold adjustment with W_dec norms.\n    \"\"\"\n    # Save the current threshold before we call the parent method\n    current_thresh = self.threshold.clone()\n\n    # Get W_dec norms\n    W_dec_norms = self.W_dec.norm(dim=-1).unsqueeze(1)\n\n    # Call parent implementation to handle W_enc and W_dec adjustment\n    super().fold_W_dec_norm()\n\n    # Fix: Use squeeze() instead of squeeze(-1) to match old behavior\n    self.log_threshold.data = torch.log(current_thresh * W_dec_norms.squeeze())\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTrainingSAE.initialize_weights","title":"<code>initialize_weights()</code>","text":"<p>Initialize parameters like the base SAE, but also add log_threshold.</p> Source code in <code>sae_lens/saes/jumprelu_sae.py</code> <pre><code>@override\ndef initialize_weights(self) -&gt; None:\n    \"\"\"\n    Initialize parameters like the base SAE, but also add log_threshold.\n    \"\"\"\n    super().initialize_weights()\n    # Encoder Bias\n    self.b_enc = nn.Parameter(\n        torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)\n    )\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTrainingSAE.process_state_dict_for_loading","title":"<code>process_state_dict_for_loading(state_dict)</code>","text":"<p>Convert threshold to log_threshold for loading</p> Source code in <code>sae_lens/saes/jumprelu_sae.py</code> <pre><code>def process_state_dict_for_loading(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Convert threshold to log_threshold for loading\"\"\"\n    if \"threshold\" in state_dict:\n        threshold = state_dict[\"threshold\"]\n        del state_dict[\"threshold\"]\n        state_dict[\"log_threshold\"] = torch.log(threshold).detach().contiguous()\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTrainingSAE.process_state_dict_for_saving","title":"<code>process_state_dict_for_saving(state_dict)</code>","text":"<p>Convert log_threshold to threshold for saving</p> Source code in <code>sae_lens/saes/jumprelu_sae.py</code> <pre><code>def process_state_dict_for_saving(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Convert log_threshold to threshold for saving\"\"\"\n    if \"log_threshold\" in state_dict:\n        threshold = torch.exp(state_dict[\"log_threshold\"]).detach().contiguous()\n        del state_dict[\"log_threshold\"]\n        state_dict[\"threshold\"] = threshold\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTrainingSAEConfig","title":"<code>JumpReLUTrainingSAEConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingSAEConfig</code></p> <p>Configuration class for training a JumpReLUTrainingSAE.</p> <ul> <li>jumprelu_init_threshold: initial threshold for the JumpReLU activation</li> <li>jumprelu_bandwidth: bandwidth for the JumpReLU activation</li> <li>jumprelu_sparsity_loss_mode: mode for the sparsity loss, either \"step\" or \"tanh\". \"step\" is Google Deepmind's L0 loss, \"tanh\" is Anthropic's sparsity loss.</li> <li>l0_coefficient: coefficient for the l0 sparsity loss</li> <li>l0_warm_up_steps: number of warm-up steps for the l0 sparsity loss</li> <li>pre_act_loss_coefficient: coefficient for the pre-activation loss. Set to None to disable. Set to 3e-6 to match Anthropic's setup. Default is None.</li> <li>jumprelu_tanh_scale: scale for the tanh sparsity loss. Only relevant for \"tanh\" sparsity loss mode. Default is 4.0.</li> </ul> Source code in <code>sae_lens/saes/jumprelu_sae.py</code> <pre><code>@dataclass\nclass JumpReLUTrainingSAEConfig(TrainingSAEConfig):\n    \"\"\"\n    Configuration class for training a JumpReLUTrainingSAE.\n\n    - jumprelu_init_threshold: initial threshold for the JumpReLU activation\n    - jumprelu_bandwidth: bandwidth for the JumpReLU activation\n    - jumprelu_sparsity_loss_mode: mode for the sparsity loss, either \"step\" or \"tanh\". \"step\" is Google Deepmind's L0 loss, \"tanh\" is Anthropic's sparsity loss.\n    - l0_coefficient: coefficient for the l0 sparsity loss\n    - l0_warm_up_steps: number of warm-up steps for the l0 sparsity loss\n    - pre_act_loss_coefficient: coefficient for the pre-activation loss. Set to None to disable. Set to 3e-6 to match Anthropic's setup. Default is None.\n    - jumprelu_tanh_scale: scale for the tanh sparsity loss. Only relevant for \"tanh\" sparsity loss mode. Default is 4.0.\n    \"\"\"\n\n    jumprelu_init_threshold: float = 0.01\n    jumprelu_bandwidth: float = 0.05\n    # step is Google Deepmind, tanh is Anthropic\n    jumprelu_sparsity_loss_mode: Literal[\"step\", \"tanh\"] = \"step\"\n    l0_coefficient: float = 1.0\n    l0_warm_up_steps: int = 0\n\n    # anthropic's auxiliary loss to avoid dead features\n    pre_act_loss_coefficient: float | None = None\n\n    # only relevant for tanh sparsity loss mode\n    jumprelu_tanh_scale: float = 4.0\n\n    @override\n    @classmethod\n    def architecture(cls) -&gt; str:\n        return \"jumprelu\"\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTranscoder","title":"<code>JumpReLUTranscoder</code>","text":"<p>               Bases: <code>Transcoder</code></p> <p>A transcoder with JumpReLU activation function.</p> <p>JumpReLU applies a threshold to activations: if pre-activation &lt;= threshold, the unit is zeroed out; otherwise, it follows the base activation function.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>class JumpReLUTranscoder(Transcoder):\n    \"\"\"\n    A transcoder with JumpReLU activation function.\n\n    JumpReLU applies a threshold to activations: if pre-activation &lt;=\n    threshold, the unit is zeroed out; otherwise, it follows the base\n    activation function.\n    \"\"\"\n\n    cfg: JumpReLUTranscoderConfig  # type: ignore[assignment]\n    threshold: nn.Parameter\n\n    def __init__(self, cfg: JumpReLUTranscoderConfig):\n        super().__init__(cfg)\n        self.cfg = cfg\n\n    def initialize_weights(self):\n        \"\"\"Initialize transcoder weights including threshold parameter.\"\"\"\n        super().initialize_weights()\n\n        # Initialize threshold parameter for JumpReLU\n        self.threshold = nn.Parameter(\n            torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)\n        )\n\n    def encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Encode using JumpReLU activation.\n\n        Applies base activation function (ReLU) then masks based on threshold.\n        \"\"\"\n        # Preprocess the SAE input\n        sae_in = self.process_sae_in(x)\n\n        # Compute pre-activation values\n        hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n\n        # Apply base activation function (ReLU)\n        feature_acts = self.activation_fn(hidden_pre)\n\n        # Apply JumpReLU threshold\n        # During training, use detached threshold to prevent gradient flow\n        threshold = self.threshold.detach() if self.training else self.threshold\n        jump_relu_mask = (hidden_pre &gt; threshold).to(self.dtype)\n\n        # Apply mask and hook\n        return self.hook_sae_acts_post(feature_acts * jump_relu_mask)\n\n    def fold_W_dec_norm(self) -&gt; None:\n        \"\"\"\n        Fold the decoder weight norm into the threshold parameter.\n\n        This is important for JumpReLU as the threshold needs to be scaled\n        along with the decoder weights.\n        \"\"\"\n        # Get the decoder weight norms before normalizing\n        with torch.no_grad():\n            W_dec_norms = self.W_dec.norm(dim=1)\n\n        # Fold the decoder norms as in the parent class\n        super().fold_W_dec_norm()\n\n        # Scale the threshold by the decoder weight norms\n        with torch.no_grad():\n            self.threshold.data = self.threshold.data * W_dec_norms\n\n    @classmethod\n    def from_dict(cls, config_dict: dict[str, Any]) -&gt; \"JumpReLUTranscoder\":\n        cfg = JumpReLUTranscoderConfig.from_dict(config_dict)\n        return cls(cfg)\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTranscoder.encode","title":"<code>encode(x)</code>","text":"<p>Encode using JumpReLU activation.</p> <p>Applies base activation function (ReLU) then masks based on threshold.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>def encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Encode using JumpReLU activation.\n\n    Applies base activation function (ReLU) then masks based on threshold.\n    \"\"\"\n    # Preprocess the SAE input\n    sae_in = self.process_sae_in(x)\n\n    # Compute pre-activation values\n    hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n\n    # Apply base activation function (ReLU)\n    feature_acts = self.activation_fn(hidden_pre)\n\n    # Apply JumpReLU threshold\n    # During training, use detached threshold to prevent gradient flow\n    threshold = self.threshold.detach() if self.training else self.threshold\n    jump_relu_mask = (hidden_pre &gt; threshold).to(self.dtype)\n\n    # Apply mask and hook\n    return self.hook_sae_acts_post(feature_acts * jump_relu_mask)\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTranscoder.fold_W_dec_norm","title":"<code>fold_W_dec_norm()</code>","text":"<p>Fold the decoder weight norm into the threshold parameter.</p> <p>This is important for JumpReLU as the threshold needs to be scaled along with the decoder weights.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>def fold_W_dec_norm(self) -&gt; None:\n    \"\"\"\n    Fold the decoder weight norm into the threshold parameter.\n\n    This is important for JumpReLU as the threshold needs to be scaled\n    along with the decoder weights.\n    \"\"\"\n    # Get the decoder weight norms before normalizing\n    with torch.no_grad():\n        W_dec_norms = self.W_dec.norm(dim=1)\n\n    # Fold the decoder norms as in the parent class\n    super().fold_W_dec_norm()\n\n    # Scale the threshold by the decoder weight norms\n    with torch.no_grad():\n        self.threshold.data = self.threshold.data * W_dec_norms\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTranscoder.initialize_weights","title":"<code>initialize_weights()</code>","text":"<p>Initialize transcoder weights including threshold parameter.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>def initialize_weights(self):\n    \"\"\"Initialize transcoder weights including threshold parameter.\"\"\"\n    super().initialize_weights()\n\n    # Initialize threshold parameter for JumpReLU\n    self.threshold = nn.Parameter(\n        torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)\n    )\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTranscoderConfig","title":"<code>JumpReLUTranscoderConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TranscoderConfig</code></p> <p>Configuration for JumpReLU transcoder.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>@dataclass\nclass JumpReLUTranscoderConfig(TranscoderConfig):\n    \"\"\"Configuration for JumpReLU transcoder.\"\"\"\n\n    @classmethod\n    def architecture(cls) -&gt; str:\n        \"\"\"Return the architecture name for this config.\"\"\"\n        return \"jumprelu_transcoder\"\n\n    @classmethod\n    def from_dict(cls, config_dict: dict[str, Any]) -&gt; \"JumpReLUTranscoderConfig\":\n        \"\"\"Create a JumpReLUTranscoderConfig from a dictionary.\"\"\"\n        # Filter to only include valid dataclass fields\n        filtered_config_dict = filter_valid_dataclass_fields(config_dict, cls)\n\n        # Create the config instance\n        res = cls(**filtered_config_dict)\n\n        # Handle metadata if present\n        if \"metadata\" in config_dict:\n            res.metadata = SAEMetadata(**config_dict[\"metadata\"])\n\n        return res\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTranscoderConfig.architecture","title":"<code>architecture()</code>  <code>classmethod</code>","text":"<p>Return the architecture name for this config.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>@classmethod\ndef architecture(cls) -&gt; str:\n    \"\"\"Return the architecture name for this config.\"\"\"\n    return \"jumprelu_transcoder\"\n</code></pre>"},{"location":"api/#sae_lens.JumpReLUTranscoderConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Create a JumpReLUTranscoderConfig from a dictionary.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict[str, Any]) -&gt; \"JumpReLUTranscoderConfig\":\n    \"\"\"Create a JumpReLUTranscoderConfig from a dictionary.\"\"\"\n    # Filter to only include valid dataclass fields\n    filtered_config_dict = filter_valid_dataclass_fields(config_dict, cls)\n\n    # Create the config instance\n    res = cls(**filtered_config_dict)\n\n    # Handle metadata if present\n    if \"metadata\" in config_dict:\n        res.metadata = SAEMetadata(**config_dict[\"metadata\"])\n\n    return res\n</code></pre>"},{"location":"api/#sae_lens.LanguageModelSAERunnerConfig","title":"<code>LanguageModelSAERunnerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[T_TRAINING_SAE_CONFIG]</code></p> <p>Configuration for training a sparse autoencoder on a language model.</p> <p>Parameters:</p> Name Type Description Default <code>sae</code> <code>T_TRAINING_SAE_CONFIG</code> <p>The configuration for the SAE itself (e.g. StandardSAEConfig, GatedSAEConfig).</p> required <code>model_name</code> <code>str</code> <p>The name of the model to use. This should be the name of the model in the Hugging Face model hub.</p> <code>'gelu-2l'</code> <code>model_class_name</code> <code>str</code> <p>The name of the class of the model to use. This should be either <code>HookedTransformer</code> or <code>HookedMamba</code>.</p> <code>'HookedTransformer'</code> <code>hook_name</code> <code>str</code> <p>The name of the hook to use. This should be a valid TransformerLens hook.</p> <code>'blocks.0.hook_mlp_out'</code> <code>hook_eval</code> <code>str</code> <p>DEPRECATED: Will be removed in v7.0.0. NOT CURRENTLY IN USE. The name of the hook to use for evaluation.</p> <code>'NOT_IN_USE'</code> <code>hook_head_index</code> <code>int</code> <p>When the hook is for an activation with a head index, we can specify a specific head to use here.</p> <code>None</code> <code>dataset_path</code> <code>str</code> <p>A Hugging Face dataset path.</p> <code>''</code> <code>dataset_trust_remote_code</code> <code>bool</code> <p>Whether to trust remote code when loading datasets from Huggingface.</p> <code>True</code> <code>streaming</code> <code>bool</code> <p>Whether to stream the dataset. Streaming large datasets is usually practical.</p> <code>True</code> <code>is_dataset_tokenized</code> <code>bool</code> <p>Whether the dataset is already tokenized.</p> <code>True</code> <code>context_size</code> <code>int</code> <p>The context size to use when generating activations on which to train the SAE.</p> <code>128</code> <code>use_cached_activations</code> <code>bool</code> <p>Whether to use cached activations. This is useful when doing sweeps over the same activations.</p> <code>False</code> <code>cached_activations_path</code> <code>str</code> <p>The path to the cached activations. Defaults to \"activations/{dataset_path}/{model_name}/{hook_name}_{hook_head_index}\".</p> <code>None</code> <code>from_pretrained_path</code> <code>str</code> <p>The path to a pretrained SAE. We can finetune an existing SAE if needed.</p> <code>None</code> <code>n_batches_in_buffer</code> <code>int</code> <p>The number of batches in the buffer. When not using cached activations, a buffer in RAM is used. The larger it is, the better shuffled the activations will be.</p> <code>20</code> <code>training_tokens</code> <code>int</code> <p>The number of training tokens.</p> <code>2000000</code> <code>store_batch_size_prompts</code> <code>int</code> <p>The batch size for storing activations. This controls how many prompts are in the batch of the language model when generating activations.</p> <code>32</code> <code>seqpos_slice</code> <code>tuple[int | None, ...]</code> <p>Determines slicing of activations when constructing batches during training. The slice should be (start_pos, end_pos, optional[step_size]), e.g. for Othello we sometimes use (5, -5). Note, step_size &gt; 0.</p> <code>(None,)</code> <code>disable_concat_sequences</code> <code>bool</code> <p>Whether to disable concatenating sequences and ignore sequences shorter than the context size. If True, disables concatenating and ignores short sequences.</p> <code>False</code> <code>sequence_separator_token</code> <code>int | Literal['bos', 'eos', 'sep'] | None</code> <p>If not <code>None</code>, this token will be placed between sentences in a batch to act as a separator. By default, this is the <code>&lt;bos&gt;</code> token.</p> <code>special_token_field(default='bos')</code> <code>device</code> <code>str</code> <p>The device to use. Usually \"cuda\".</p> <code>'cpu'</code> <code>act_store_device</code> <code>str</code> <p>The device to use for the activation store. \"cpu\" is advised in order to save VRAM. Defaults to \"with_model\" which uses the same device as the main model.</p> <code>'with_model'</code> <code>seed</code> <code>int</code> <p>The seed to use.</p> <code>42</code> <code>dtype</code> <code>str</code> <p>The data type to use for the SAE and activations.</p> <code>'float32'</code> <code>prepend_bos</code> <code>bool</code> <p>Whether to prepend the beginning of sequence token. You should use whatever the model was trained with.</p> <code>True</code> <code>autocast</code> <code>bool</code> <p>Whether to use autocast (mixed-precision) during SAE training. Saves VRAM.</p> <code>False</code> <code>autocast_lm</code> <code>bool</code> <p>Whether to use autocast (mixed-precision) during activation fetching. Saves VRAM.</p> <code>False</code> <code>compile_llm</code> <code>bool</code> <p>Whether to compile the LLM using <code>torch.compile</code>.</p> <code>False</code> <code>llm_compilation_mode</code> <code>str</code> <p>The compilation mode to use for the LLM if <code>compile_llm</code> is True.</p> <code>None</code> <code>compile_sae</code> <code>bool</code> <p>Whether to compile the SAE using <code>torch.compile</code>.</p> <code>False</code> <code>sae_compilation_mode</code> <code>str</code> <p>The compilation mode to use for the SAE if <code>compile_sae</code> is True.</p> <code>None</code> <code>train_batch_size_tokens</code> <code>int</code> <p>The batch size for training, in tokens. This controls the batch size of the SAE training loop.</p> <code>4096</code> <code>adam_beta1</code> <code>float</code> <p>The beta1 parameter for the Adam optimizer.</p> <code>0.9</code> <code>adam_beta2</code> <code>float</code> <p>The beta2 parameter for the Adam optimizer.</p> <code>0.999</code> <code>lr</code> <code>float</code> <p>The learning rate.</p> <code>0.0003</code> <code>lr_scheduler_name</code> <code>str</code> <p>The name of the learning rate scheduler to use (e.g., \"constant\", \"cosineannealing\", \"cosineannealingwarmrestarts\").</p> <code>'constant'</code> <code>lr_warm_up_steps</code> <code>int</code> <p>The number of warm-up steps for the learning rate.</p> <code>0</code> <code>lr_end</code> <code>float</code> <p>The end learning rate if using a scheduler like cosine annealing. Defaults to <code>lr / 10</code>.</p> <code>None</code> <code>lr_decay_steps</code> <code>int</code> <p>The number of decay steps for the learning rate if using a scheduler with decay.</p> <code>0</code> <code>n_restart_cycles</code> <code>int</code> <p>The number of restart cycles for the cosine annealing with warm restarts scheduler.</p> <code>1</code> <code>dead_feature_window</code> <code>int</code> <p>The window size (in training steps) for detecting dead features.</p> <code>1000</code> <code>feature_sampling_window</code> <code>int</code> <p>The window size (in training steps) for resampling features (e.g. dead features).</p> <code>2000</code> <code>dead_feature_threshold</code> <code>float</code> <p>The threshold below which a feature's activation frequency is considered dead.</p> <code>1e-08</code> <code>n_eval_batches</code> <code>int</code> <p>The number of batches to use for evaluation.</p> <code>10</code> <code>eval_batch_size_prompts</code> <code>int</code> <p>The batch size for evaluation, in prompts. Useful if evals cause OOM.</p> <code>None</code> <code>logger</code> <code>LoggingConfig</code> <p>Configuration for logging (e.g. W&amp;B).</p> <code>LoggingConfig()</code> <code>n_checkpoints</code> <code>int</code> <p>The number of checkpoints to save during training. 0 means no checkpoints.</p> <code>0</code> <code>checkpoint_path</code> <code>str | None</code> <p>The path to save checkpoints. A unique ID will be appended to this path. Set to None to disable checkpoint saving. (default is \"checkpoints\")</p> <code>'checkpoints'</code> <code>save_final_checkpoint</code> <code>bool</code> <p>Whether to include an additional final checkpoint when training is finished. (default is False).</p> <code>False</code> <code>output_path</code> <code>str | None</code> <p>The path to save outputs. Set to None to disable output saving. (default is \"output\")</p> <code>'output'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output. (default is True)</p> <code>True</code> <code>model_kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for <code>model.run_with_cache</code></p> <code>dict_field(default={})</code> <code>model_from_pretrained_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments to pass to the model's <code>from_pretrained</code> method.</p> <code>dict_field(default=None)</code> <code>sae_lens_version</code> <code>str</code> <p>The version of the sae_lens library.</p> <code>(lambda: __version__)()</code> <code>sae_lens_training_version</code> <code>str</code> <p>The version of the sae_lens training library.</p> <code>(lambda: __version__)()</code> <code>exclude_special_tokens</code> <code>bool | list[int]</code> <p>Whether to exclude special tokens from the activations. If True, excludes all special tokens. If a list of ints, excludes those token IDs.</p> <code>False</code> Source code in <code>sae_lens/config.py</code> <pre><code>@dataclass\nclass LanguageModelSAERunnerConfig(Generic[T_TRAINING_SAE_CONFIG]):\n    \"\"\"\n    Configuration for training a sparse autoencoder on a language model.\n\n    Args:\n        sae (T_TRAINING_SAE_CONFIG): The configuration for the SAE itself (e.g. StandardSAEConfig, GatedSAEConfig).\n        model_name (str): The name of the model to use. This should be the name of the model in the Hugging Face model hub.\n        model_class_name (str): The name of the class of the model to use. This should be either `HookedTransformer` or `HookedMamba`.\n        hook_name (str): The name of the hook to use. This should be a valid TransformerLens hook.\n        hook_eval (str): DEPRECATED: Will be removed in v7.0.0. NOT CURRENTLY IN USE. The name of the hook to use for evaluation.\n        hook_head_index (int, optional): When the hook is for an activation with a head index, we can specify a specific head to use here.\n        dataset_path (str): A Hugging Face dataset path.\n        dataset_trust_remote_code (bool): Whether to trust remote code when loading datasets from Huggingface.\n        streaming (bool): Whether to stream the dataset. Streaming large datasets is usually practical.\n        is_dataset_tokenized (bool): Whether the dataset is already tokenized.\n        context_size (int): The context size to use when generating activations on which to train the SAE.\n        use_cached_activations (bool): Whether to use cached activations. This is useful when doing sweeps over the same activations.\n        cached_activations_path (str, optional): The path to the cached activations. Defaults to \"activations/{dataset_path}/{model_name}/{hook_name}_{hook_head_index}\".\n        from_pretrained_path (str, optional): The path to a pretrained SAE. We can finetune an existing SAE if needed.\n        n_batches_in_buffer (int): The number of batches in the buffer. When not using cached activations, a buffer in RAM is used. The larger it is, the better shuffled the activations will be.\n        training_tokens (int): The number of training tokens.\n        store_batch_size_prompts (int): The batch size for storing activations. This controls how many prompts are in the batch of the language model when generating activations.\n        seqpos_slice (tuple[int | None, ...]): Determines slicing of activations when constructing batches during training. The slice should be (start_pos, end_pos, optional[step_size]), e.g. for Othello we sometimes use (5, -5). Note, step_size &gt; 0.\n        disable_concat_sequences (bool): Whether to disable concatenating sequences and ignore sequences shorter than the context size. If True, disables concatenating and ignores short sequences.\n        sequence_separator_token (int | Literal[\"bos\", \"eos\", \"sep\"] | None): If not `None`, this token will be placed between sentences in a batch to act as a separator. By default, this is the `&lt;bos&gt;` token.\n        device (str): The device to use. Usually \"cuda\".\n        act_store_device (str): The device to use for the activation store. \"cpu\" is advised in order to save VRAM. Defaults to \"with_model\" which uses the same device as the main model.\n        seed (int): The seed to use.\n        dtype (str): The data type to use for the SAE and activations.\n        prepend_bos (bool): Whether to prepend the beginning of sequence token. You should use whatever the model was trained with.\n        autocast (bool): Whether to use autocast (mixed-precision) during SAE training. Saves VRAM.\n        autocast_lm (bool): Whether to use autocast (mixed-precision) during activation fetching. Saves VRAM.\n        compile_llm (bool): Whether to compile the LLM using `torch.compile`.\n        llm_compilation_mode (str, optional): The compilation mode to use for the LLM if `compile_llm` is True.\n        compile_sae (bool): Whether to compile the SAE using `torch.compile`.\n        sae_compilation_mode (str, optional): The compilation mode to use for the SAE if `compile_sae` is True.\n        train_batch_size_tokens (int): The batch size for training, in tokens. This controls the batch size of the SAE training loop.\n        adam_beta1 (float): The beta1 parameter for the Adam optimizer.\n        adam_beta2 (float): The beta2 parameter for the Adam optimizer.\n        lr (float): The learning rate.\n        lr_scheduler_name (str): The name of the learning rate scheduler to use (e.g., \"constant\", \"cosineannealing\", \"cosineannealingwarmrestarts\").\n        lr_warm_up_steps (int): The number of warm-up steps for the learning rate.\n        lr_end (float, optional): The end learning rate if using a scheduler like cosine annealing. Defaults to `lr / 10`.\n        lr_decay_steps (int): The number of decay steps for the learning rate if using a scheduler with decay.\n        n_restart_cycles (int): The number of restart cycles for the cosine annealing with warm restarts scheduler.\n        dead_feature_window (int): The window size (in training steps) for detecting dead features.\n        feature_sampling_window (int): The window size (in training steps) for resampling features (e.g. dead features).\n        dead_feature_threshold (float): The threshold below which a feature's activation frequency is considered dead.\n        n_eval_batches (int): The number of batches to use for evaluation.\n        eval_batch_size_prompts (int, optional): The batch size for evaluation, in prompts. Useful if evals cause OOM.\n        logger (LoggingConfig): Configuration for logging (e.g. W&amp;B).\n        n_checkpoints (int): The number of checkpoints to save during training. 0 means no checkpoints.\n        checkpoint_path (str | None): The path to save checkpoints. A unique ID will be appended to this path. Set to None to disable checkpoint saving. (default is \"checkpoints\")\n        save_final_checkpoint (bool): Whether to include an additional final checkpoint when training is finished. (default is False).\n        output_path (str | None): The path to save outputs. Set to None to disable output saving. (default is \"output\")\n        verbose (bool): Whether to print verbose output. (default is True)\n        model_kwargs (dict[str, Any]): Keyword arguments for `model.run_with_cache`\n        model_from_pretrained_kwargs (dict[str, Any], optional): Additional keyword arguments to pass to the model's `from_pretrained` method.\n        sae_lens_version (str): The version of the sae_lens library.\n        sae_lens_training_version (str): The version of the sae_lens training library.\n        exclude_special_tokens (bool | list[int]): Whether to exclude special tokens from the activations. If True, excludes all special tokens. If a list of ints, excludes those token IDs.\n    \"\"\"\n\n    sae: T_TRAINING_SAE_CONFIG\n\n    # Data Generating Function (Model + Training Distibuion)\n    model_name: str = \"gelu-2l\"\n    model_class_name: str = \"HookedTransformer\"\n    hook_name: str = \"blocks.0.hook_mlp_out\"\n    hook_eval: str = \"NOT_IN_USE\"\n    hook_head_index: int | None = None\n    dataset_path: str = \"\"\n    dataset_trust_remote_code: bool = True\n    streaming: bool = True\n    is_dataset_tokenized: bool = True\n    context_size: int = 128\n    use_cached_activations: bool = False\n    cached_activations_path: str | None = (\n        None  # Defaults to \"activations/{dataset}/{model}/{full_hook_name}_{hook_head_index}\"\n    )\n\n    # SAE Parameters\n    from_pretrained_path: str | None = None\n\n    # Activation Store Parameters\n    n_batches_in_buffer: int = 20\n    training_tokens: int = 2_000_000\n    store_batch_size_prompts: int = 32\n    seqpos_slice: tuple[int | None, ...] = (None,)\n    disable_concat_sequences: bool = False\n    sequence_separator_token: int | Literal[\"bos\", \"eos\", \"sep\"] | None = (\n        special_token_field(default=\"bos\")\n    )\n\n    # Misc\n    device: str = \"cpu\"\n    act_store_device: str = \"with_model\"  # will be set by post init if with_model\n    seed: int = 42\n    dtype: str = \"float32\"  # type: ignore #\n    prepend_bos: bool = True\n\n    # Performance - see compilation section of lm_runner.py for info\n    autocast: bool = False  # autocast to autocast_dtype during training\n    autocast_lm: bool = False  # autocast lm during activation fetching\n    compile_llm: bool = False  # use torch.compile on the LLM\n    llm_compilation_mode: str | None = None  # which torch.compile mode to use\n    compile_sae: bool = False  # use torch.compile on the SAE\n    sae_compilation_mode: str | None = None\n\n    # Training Parameters\n\n    ## Batch size\n    train_batch_size_tokens: int = 4096\n\n    ## Adam\n    adam_beta1: float = 0.9\n    adam_beta2: float = 0.999\n\n    ## Learning Rate Schedule\n    lr: float = 3e-4\n    lr_scheduler_name: str = (\n        \"constant\"  # constant, cosineannealing, cosineannealingwarmrestarts\n    )\n    lr_warm_up_steps: int = 0\n    lr_end: float | None = None  # only used for cosine annealing, default is lr / 10\n    lr_decay_steps: int = 0\n    n_restart_cycles: int = 1  # used only for cosineannealingwarmrestarts\n\n    # Resampling protocol args\n    dead_feature_window: int = 1000  # unless this window is larger feature sampling,\n    feature_sampling_window: int = 2000\n    dead_feature_threshold: float = 1e-8\n\n    # Evals\n    n_eval_batches: int = 10\n    eval_batch_size_prompts: int | None = None  # useful if evals cause OOM\n\n    logger: LoggingConfig = field(default_factory=LoggingConfig)\n\n    # Outputs/Checkpoints\n    n_checkpoints: int = 0\n    checkpoint_path: str | None = \"checkpoints\"\n    save_final_checkpoint: bool = False\n    output_path: str | None = \"output\"\n\n    # Misc\n    verbose: bool = True\n    model_kwargs: dict[str, Any] = dict_field(default={})\n    model_from_pretrained_kwargs: dict[str, Any] | None = dict_field(default=None)\n    sae_lens_version: str = field(default_factory=lambda: __version__)\n    sae_lens_training_version: str = field(default_factory=lambda: __version__)\n    exclude_special_tokens: bool | list[int] = False\n\n    def __post_init__(self):\n        if self.hook_eval != \"NOT_IN_USE\":\n            warnings.warn(\n                \"The 'hook_eval' field is deprecated and will be removed in v7.0.0. \"\n                \"It is not currently used and can be safely removed from your config.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        if self.use_cached_activations and self.cached_activations_path is None:\n            self.cached_activations_path = _default_cached_activations_path(\n                self.dataset_path,\n                self.model_name,\n                self.hook_name,\n                self.hook_head_index,\n            )\n        self.tokens_per_buffer = (\n            self.train_batch_size_tokens * self.context_size * self.n_batches_in_buffer\n        )\n\n        if self.logger.run_name is None:\n            self.logger.run_name = f\"{self.sae.architecture()}-{self.sae.d_sae}-LR-{self.lr}-Tokens-{self.training_tokens:3.3e}\"\n\n        if self.model_from_pretrained_kwargs is None:\n            if self.model_class_name == \"HookedTransformer\":\n                self.model_from_pretrained_kwargs = {\"center_writing_weights\": False}\n            else:\n                self.model_from_pretrained_kwargs = {}\n\n        if self.act_store_device == \"with_model\":\n            self.act_store_device = self.device\n\n        if self.lr_end is None:\n            self.lr_end = self.lr / 10\n\n        unique_id = self.logger.wandb_id\n        if unique_id is None:\n            unique_id = cast(\n                Any, wandb\n            ).util.generate_id()  # not sure why this type is erroring\n        self.checkpoint_path = f\"{self.checkpoint_path}/{unique_id}\"\n\n        if self.verbose:\n            logger.info(\n                f\"Run name: {self.sae.architecture()}-{self.sae.d_sae}-LR-{self.lr}-Tokens-{self.training_tokens:3.3e}\"\n            )\n            # Print out some useful info:\n            n_tokens_per_buffer = (\n                self.store_batch_size_prompts\n                * self.context_size\n                * self.n_batches_in_buffer\n            )\n            logger.info(\n                f\"n_tokens_per_buffer (millions): {n_tokens_per_buffer / 10**6}\"\n            )\n            n_contexts_per_buffer = (\n                self.store_batch_size_prompts * self.n_batches_in_buffer\n            )\n            logger.info(\n                f\"Lower bound: n_contexts_per_buffer (millions): {n_contexts_per_buffer / 10**6}\"\n            )\n\n            total_training_steps = (\n                self.training_tokens\n            ) // self.train_batch_size_tokens\n            logger.info(f\"Total training steps: {total_training_steps}\")\n\n            total_wandb_updates = (\n                total_training_steps // self.logger.wandb_log_frequency\n            )\n            logger.info(f\"Total wandb updates: {total_wandb_updates}\")\n\n            # how many times will we sample dead neurons?\n            # assert self.dead_feature_window &lt;= self.feature_sampling_window, \"dead_feature_window must be smaller than feature_sampling_window\"\n            n_feature_window_samples = (\n                total_training_steps // self.feature_sampling_window\n            )\n            logger.info(\n                f\"n_tokens_per_feature_sampling_window (millions): {(self.feature_sampling_window * self.context_size * self.train_batch_size_tokens) / 10**6}\"\n            )\n            logger.info(\n                f\"n_tokens_per_dead_feature_window (millions): {(self.dead_feature_window * self.context_size * self.train_batch_size_tokens) / 10**6}\"\n            )\n            logger.info(\n                f\"We will reset the sparsity calculation {n_feature_window_samples} times.\"\n            )\n            # logger.info(\"Number tokens in dead feature calculation window: \", self.dead_feature_window * self.train_batch_size_tokens)\n            logger.info(\n                f\"Number tokens in sparsity calculation window: {self.feature_sampling_window * self.train_batch_size_tokens:.2e}\"\n            )\n\n        if self.context_size &lt; 0:\n            raise ValueError(\n                f\"The provided context_size is {self.context_size} is negative. Expecting positive context_size.\"\n            )\n\n        _validate_seqpos(seqpos=self.seqpos_slice, context_size=self.context_size)\n\n        if isinstance(self.exclude_special_tokens, list) and not all(\n            isinstance(x, int) for x in self.exclude_special_tokens\n        ):\n            raise ValueError(\"exclude_special_tokens list must contain only integers\")\n\n    @property\n    def total_training_tokens(self) -&gt; int:\n        return self.training_tokens\n\n    @property\n    def total_training_steps(self) -&gt; int:\n        return self.total_training_tokens // self.train_batch_size_tokens\n\n    def get_training_sae_cfg_dict(self) -&gt; dict[str, Any]:\n        return self.sae.to_dict()\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        # Make a shallow copy of config's dictionary\n        d = dict(self.__dict__)\n\n        d[\"logger\"] = asdict(self.logger)\n        d[\"sae\"] = self.sae.to_dict()\n        # Overwrite fields that might not be JSON-serializable\n        d[\"dtype\"] = str(self.dtype)\n        d[\"device\"] = str(self.device)\n        d[\"act_store_device\"] = str(self.act_store_device)\n        return d\n\n    def to_sae_trainer_config(self) -&gt; \"SAETrainerConfig\":\n        return SAETrainerConfig(\n            n_checkpoints=self.n_checkpoints,\n            checkpoint_path=self.checkpoint_path,\n            save_final_checkpoint=self.save_final_checkpoint,\n            total_training_samples=self.total_training_tokens,\n            device=self.device,\n            autocast=self.autocast,\n            lr=self.lr,\n            lr_end=self.lr_end,\n            lr_scheduler_name=self.lr_scheduler_name,\n            lr_warm_up_steps=self.lr_warm_up_steps,\n            adam_beta1=self.adam_beta1,\n            adam_beta2=self.adam_beta2,\n            lr_decay_steps=self.lr_decay_steps,\n            n_restart_cycles=self.n_restart_cycles,\n            train_batch_size_samples=self.train_batch_size_tokens,\n            dead_feature_window=self.dead_feature_window,\n            feature_sampling_window=self.feature_sampling_window,\n            logger=self.logger,\n        )\n</code></pre>"},{"location":"api/#sae_lens.LanguageModelSAETrainingRunner","title":"<code>LanguageModelSAETrainingRunner</code>","text":"<p>Class to run the training of a Sparse Autoencoder (SAE) on a TransformerLens model.</p> Source code in <code>sae_lens/llm_sae_training_runner.py</code> <pre><code>class LanguageModelSAETrainingRunner:\n    \"\"\"\n    Class to run the training of a Sparse Autoencoder (SAE) on a TransformerLens model.\n    \"\"\"\n\n    cfg: LanguageModelSAERunnerConfig[Any]\n    model: HookedRootModule\n    sae: TrainingSAE[Any]\n    activations_store: ActivationsStore\n\n    def __init__(\n        self,\n        cfg: LanguageModelSAERunnerConfig[T_TRAINING_SAE_CONFIG],\n        override_dataset: HfDataset | None = None,\n        override_model: HookedRootModule | None = None,\n        override_sae: TrainingSAE[Any] | None = None,\n    ):\n        if override_dataset is not None:\n            logger.warning(\n                f\"You just passed in a dataset which will override the one specified in your configuration: {cfg.dataset_path}. As a consequence this run will not be reproducible via configuration alone.\"\n            )\n        if override_model is not None:\n            logger.warning(\n                f\"You just passed in a model which will override the one specified in your configuration: {cfg.model_name}. As a consequence this run will not be reproducible via configuration alone.\"\n            )\n\n        self.cfg = cfg\n\n        if override_model is None:\n            self.model = load_model(\n                self.cfg.model_class_name,\n                self.cfg.model_name,\n                device=self.cfg.device,\n                model_from_pretrained_kwargs=self.cfg.model_from_pretrained_kwargs,\n            )\n        else:\n            self.model = override_model\n\n        self.activations_store = ActivationsStore.from_config(\n            self.model,\n            self.cfg,\n            override_dataset=override_dataset,\n        )\n\n        if override_sae is None:\n            if self.cfg.from_pretrained_path is not None:\n                self.sae = TrainingSAE.load_from_disk(\n                    self.cfg.from_pretrained_path, self.cfg.device\n                )\n            else:\n                self.sae = TrainingSAE.from_dict(\n                    TrainingSAEConfig.from_dict(\n                        self.cfg.get_training_sae_cfg_dict(),\n                    ).to_dict()\n                )\n        else:\n            self.sae = override_sae\n        self.sae.to(self.cfg.device)\n\n    def run(self):\n        \"\"\"\n        Run the training of the SAE.\n        \"\"\"\n        self._set_sae_metadata()\n        if self.cfg.logger.log_to_wandb:\n            wandb.init(\n                project=self.cfg.logger.wandb_project,\n                entity=self.cfg.logger.wandb_entity,\n                config=self.cfg.to_dict(),\n                name=self.cfg.logger.run_name,\n                id=self.cfg.logger.wandb_id,\n            )\n\n        evaluator = LLMSaeEvaluator(\n            model=self.model,\n            activations_store=self.activations_store,\n            eval_batch_size_prompts=self.cfg.eval_batch_size_prompts,\n            n_eval_batches=self.cfg.n_eval_batches,\n            model_kwargs=self.cfg.model_kwargs,\n        )\n\n        trainer = SAETrainer(\n            sae=self.sae,\n            data_provider=self.activations_store,\n            evaluator=evaluator,\n            save_checkpoint_fn=self.save_checkpoint,\n            cfg=self.cfg.to_sae_trainer_config(),\n        )\n\n        self._compile_if_needed()\n        sae = self.run_trainer_with_interruption_handling(trainer)\n\n        if self.cfg.output_path is not None:\n            self.save_final_sae(\n                sae=sae,\n                output_path=self.cfg.output_path,\n                log_feature_sparsity=trainer.log_feature_sparsity,\n            )\n\n        if self.cfg.logger.log_to_wandb:\n            wandb.finish()\n\n        return sae\n\n    def save_final_sae(\n        self,\n        sae: TrainingSAE[Any],\n        output_path: str,\n        log_feature_sparsity: torch.Tensor | None = None,\n    ):\n        base_output_path = Path(output_path)\n        base_output_path.mkdir(exist_ok=True, parents=True)\n\n        weights_path, cfg_path = sae.save_inference_model(str(base_output_path))\n\n        sparsity_path = None\n        if log_feature_sparsity is not None:\n            sparsity_path = base_output_path / SPARSITY_FILENAME\n            save_file({\"sparsity\": log_feature_sparsity}, sparsity_path)\n\n        runner_config = self.cfg.to_dict()\n        with open(base_output_path / RUNNER_CFG_FILENAME, \"w\") as f:\n            json.dump(runner_config, f)\n\n        if self.cfg.logger.log_to_wandb:\n            self.cfg.logger.log(\n                self,\n                weights_path,\n                cfg_path,\n                sparsity_path=sparsity_path,\n                wandb_aliases=[\"final_model\"],\n            )\n\n    def _set_sae_metadata(self):\n        self.sae.cfg.metadata.dataset_path = self.cfg.dataset_path\n        self.sae.cfg.metadata.hook_name = self.cfg.hook_name\n        self.sae.cfg.metadata.model_name = self.cfg.model_name\n        self.sae.cfg.metadata.model_class_name = self.cfg.model_class_name\n        self.sae.cfg.metadata.hook_head_index = self.cfg.hook_head_index\n        self.sae.cfg.metadata.context_size = self.cfg.context_size\n        self.sae.cfg.metadata.seqpos_slice = self.cfg.seqpos_slice\n        self.sae.cfg.metadata.model_from_pretrained_kwargs = (\n            self.cfg.model_from_pretrained_kwargs\n        )\n        self.sae.cfg.metadata.prepend_bos = self.cfg.prepend_bos\n        self.sae.cfg.metadata.exclude_special_tokens = self.cfg.exclude_special_tokens\n        self.sae.cfg.metadata.sequence_separator_token = (\n            self.cfg.sequence_separator_token\n        )\n        self.sae.cfg.metadata.disable_concat_sequences = (\n            self.cfg.disable_concat_sequences\n        )\n\n    def _compile_if_needed(self):\n        # Compile model and SAE\n        #  torch.compile can provide significant speedups (10-20% in testing)\n        # using max-autotune gives the best speedups but:\n        # (a) increases VRAM usage,\n        # (b) can't be used on both SAE and LM (some issue with cudagraphs), and\n        # (c) takes some time to compile\n        # optimal settings seem to be:\n        # use max-autotune on SAE and max-autotune-no-cudagraphs on LM\n        # (also pylance seems to really hate this)\n        if self.cfg.compile_llm:\n            self.model = torch.compile(\n                self.model,\n                mode=self.cfg.llm_compilation_mode,\n            )  # type: ignore\n\n        if self.cfg.compile_sae:\n            backend = \"aot_eager\" if self.cfg.device == \"mps\" else \"inductor\"\n\n            self.sae.training_forward_pass = torch.compile(  # type: ignore\n                self.sae.training_forward_pass,\n                mode=self.cfg.sae_compilation_mode,\n                backend=backend,\n            )  # type: ignore\n\n    def run_trainer_with_interruption_handling(\n        self, trainer: SAETrainer[TrainingSAE[TrainingSAEConfig], TrainingSAEConfig]\n    ):\n        try:\n            # signal handlers (if preempted)\n            signal.signal(signal.SIGINT, interrupt_callback)\n            signal.signal(signal.SIGTERM, interrupt_callback)\n\n            # train SAE\n            sae = trainer.fit()\n\n        except (KeyboardInterrupt, InterruptedException):\n            if self.cfg.checkpoint_path is not None:\n                logger.warning(\"interrupted, saving progress\")\n                checkpoint_path = Path(self.cfg.checkpoint_path) / str(\n                    trainer.n_training_samples\n                )\n                self.save_checkpoint(checkpoint_path)\n                logger.info(\"done saving\")\n            raise\n\n        return sae\n\n    def save_checkpoint(\n        self,\n        checkpoint_path: Path | None,\n    ) -&gt; None:\n        if checkpoint_path is None:\n            return\n\n        self.activations_store.save(\n            str(checkpoint_path / ACTIVATIONS_STORE_STATE_FILENAME)\n        )\n\n        runner_config = self.cfg.to_dict()\n        with open(checkpoint_path / RUNNER_CFG_FILENAME, \"w\") as f:\n            json.dump(runner_config, f)\n</code></pre>"},{"location":"api/#sae_lens.LanguageModelSAETrainingRunner.run","title":"<code>run()</code>","text":"<p>Run the training of the SAE.</p> Source code in <code>sae_lens/llm_sae_training_runner.py</code> <pre><code>def run(self):\n    \"\"\"\n    Run the training of the SAE.\n    \"\"\"\n    self._set_sae_metadata()\n    if self.cfg.logger.log_to_wandb:\n        wandb.init(\n            project=self.cfg.logger.wandb_project,\n            entity=self.cfg.logger.wandb_entity,\n            config=self.cfg.to_dict(),\n            name=self.cfg.logger.run_name,\n            id=self.cfg.logger.wandb_id,\n        )\n\n    evaluator = LLMSaeEvaluator(\n        model=self.model,\n        activations_store=self.activations_store,\n        eval_batch_size_prompts=self.cfg.eval_batch_size_prompts,\n        n_eval_batches=self.cfg.n_eval_batches,\n        model_kwargs=self.cfg.model_kwargs,\n    )\n\n    trainer = SAETrainer(\n        sae=self.sae,\n        data_provider=self.activations_store,\n        evaluator=evaluator,\n        save_checkpoint_fn=self.save_checkpoint,\n        cfg=self.cfg.to_sae_trainer_config(),\n    )\n\n    self._compile_if_needed()\n    sae = self.run_trainer_with_interruption_handling(trainer)\n\n    if self.cfg.output_path is not None:\n        self.save_final_sae(\n            sae=sae,\n            output_path=self.cfg.output_path,\n            log_feature_sparsity=trainer.log_feature_sparsity,\n        )\n\n    if self.cfg.logger.log_to_wandb:\n        wandb.finish()\n\n    return sae\n</code></pre>"},{"location":"api/#sae_lens.PretokenizeRunner","title":"<code>PretokenizeRunner</code>","text":"<p>Runner to pretokenize a dataset using a given tokenizer, and optionally upload to Huggingface.</p> Source code in <code>sae_lens/pretokenize_runner.py</code> <pre><code>class PretokenizeRunner:\n    \"\"\"\n    Runner to pretokenize a dataset using a given tokenizer, and optionally upload to Huggingface.\n    \"\"\"\n\n    def __init__(self, cfg: PretokenizeRunnerConfig):\n        self.cfg = cfg\n\n    def run(self):\n        \"\"\"\n        Load the dataset, tokenize it, and save it to disk and/or upload to Huggingface.\n        \"\"\"\n        dataset = load_dataset(\n            self.cfg.dataset_path,\n            name=self.cfg.dataset_name,\n            data_dir=self.cfg.data_dir,\n            data_files=self.cfg.data_files,\n            split=self.cfg.split,\n            streaming=self.cfg.streaming,\n        )\n        if isinstance(dataset, DatasetDict):\n            raise ValueError(\n                \"Dataset has multiple splits. Must provide a 'split' param.\"\n            )\n        tokenizer = AutoTokenizer.from_pretrained(self.cfg.tokenizer_name)\n        tokenizer.model_max_length = sys.maxsize\n        tokenized_dataset = pretokenize_dataset(\n            cast(Dataset, dataset), tokenizer, self.cfg\n        )\n\n        if self.cfg.save_path is not None:\n            tokenized_dataset.save_to_disk(self.cfg.save_path)\n            metadata = metadata_from_config(self.cfg)\n            metadata_path = Path(self.cfg.save_path) / \"sae_lens.json\"\n            with open(metadata_path, \"w\") as f:\n                json.dump(metadata.__dict__, f, indent=2, ensure_ascii=False)\n\n        if self.cfg.hf_repo_id is not None:\n            push_to_hugging_face_hub(tokenized_dataset, self.cfg)\n\n        return tokenized_dataset\n</code></pre>"},{"location":"api/#sae_lens.PretokenizeRunner.run","title":"<code>run()</code>","text":"<p>Load the dataset, tokenize it, and save it to disk and/or upload to Huggingface.</p> Source code in <code>sae_lens/pretokenize_runner.py</code> <pre><code>def run(self):\n    \"\"\"\n    Load the dataset, tokenize it, and save it to disk and/or upload to Huggingface.\n    \"\"\"\n    dataset = load_dataset(\n        self.cfg.dataset_path,\n        name=self.cfg.dataset_name,\n        data_dir=self.cfg.data_dir,\n        data_files=self.cfg.data_files,\n        split=self.cfg.split,\n        streaming=self.cfg.streaming,\n    )\n    if isinstance(dataset, DatasetDict):\n        raise ValueError(\n            \"Dataset has multiple splits. Must provide a 'split' param.\"\n        )\n    tokenizer = AutoTokenizer.from_pretrained(self.cfg.tokenizer_name)\n    tokenizer.model_max_length = sys.maxsize\n    tokenized_dataset = pretokenize_dataset(\n        cast(Dataset, dataset), tokenizer, self.cfg\n    )\n\n    if self.cfg.save_path is not None:\n        tokenized_dataset.save_to_disk(self.cfg.save_path)\n        metadata = metadata_from_config(self.cfg)\n        metadata_path = Path(self.cfg.save_path) / \"sae_lens.json\"\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata.__dict__, f, indent=2, ensure_ascii=False)\n\n    if self.cfg.hf_repo_id is not None:\n        push_to_hugging_face_hub(tokenized_dataset, self.cfg)\n\n    return tokenized_dataset\n</code></pre>"},{"location":"api/#sae_lens.PretokenizeRunnerConfig","title":"<code>PretokenizeRunnerConfig</code>  <code>dataclass</code>","text":"<p>Configuration class for pretokenizing a dataset.</p> Source code in <code>sae_lens/config.py</code> <pre><code>@dataclass\nclass PretokenizeRunnerConfig:\n    \"\"\"\n    Configuration class for pretokenizing a dataset.\n    \"\"\"\n\n    tokenizer_name: str = \"gpt2\"\n    dataset_path: str = \"\"\n    dataset_name: str | None = None\n    dataset_trust_remote_code: bool | None = None\n    split: str | None = \"train\"\n    data_files: list[str] | None = None\n    data_dir: str | None = None\n    num_proc: int = 4\n    context_size: int = 128\n    column_name: str = \"text\"\n    shuffle: bool = True\n    seed: int | None = None\n    streaming: bool = False\n    pretokenize_batch_size: int | None = 1000\n\n    # special tokens\n    begin_batch_token: int | Literal[\"bos\", \"eos\", \"sep\"] | None = \"bos\"\n    begin_sequence_token: int | Literal[\"bos\", \"eos\", \"sep\"] | None = None\n    sequence_separator_token: int | Literal[\"bos\", \"eos\", \"sep\"] | None = \"bos\"\n\n    # sequence processing\n    disable_concat_sequences: bool = False\n\n    # if saving locally, set save_path\n    save_path: str | None = None\n\n    # if saving to huggingface, set hf_repo_id\n    hf_repo_id: str | None = None\n    hf_num_shards: int = 64\n    hf_revision: str = \"main\"\n    hf_is_private_repo: bool = False\n</code></pre>"},{"location":"api/#sae_lens.SAE","title":"<code>SAE</code>","text":"<p>               Bases: <code>HookedRootModule</code>, <code>Generic[T_SAE_CONFIG]</code>, <code>ABC</code></p> <p>Abstract base class for all SAE architectures.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>class SAE(HookedRootModule, Generic[T_SAE_CONFIG], ABC):\n    \"\"\"Abstract base class for all SAE architectures.\"\"\"\n\n    cfg: T_SAE_CONFIG\n    dtype: torch.dtype\n    device: torch.device\n    use_error_term: bool\n\n    # For type checking only - don't provide default values\n    # These will be initialized by subclasses\n    W_enc: nn.Parameter\n    W_dec: nn.Parameter\n    b_dec: nn.Parameter\n\n    def __init__(self, cfg: T_SAE_CONFIG, use_error_term: bool = False):\n        \"\"\"Initialize the SAE.\"\"\"\n        super().__init__()\n\n        self.cfg = cfg\n\n        if cfg.metadata and cfg.metadata:\n            warnings.warn(\n                \"\\nThis SAE has non-empty model_from_pretrained_kwargs. \"\n                \"\\nFor optimal performance, load the model like so:\\n\"\n                \"model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\",\n                category=UserWarning,\n                stacklevel=1,\n            )\n\n        self.dtype = DTYPE_MAP[cfg.dtype]\n        self.device = torch.device(cfg.device)\n        self.use_error_term = use_error_term\n\n        # Set up activation function\n        self.activation_fn = self.get_activation_fn()\n\n        # Initialize weights\n        self.initialize_weights()\n\n        # Set up hooks\n        self.hook_sae_input = HookPoint()\n        self.hook_sae_acts_pre = HookPoint()\n        self.hook_sae_acts_post = HookPoint()\n        self.hook_sae_output = HookPoint()\n        self.hook_sae_recons = HookPoint()\n        self.hook_sae_error = HookPoint()\n\n        # handle hook_z reshaping if needed.\n        if self.cfg.reshape_activations == \"hook_z\":\n            self.turn_on_forward_pass_hook_z_reshaping()\n        else:\n            self.turn_off_forward_pass_hook_z_reshaping()\n\n        # Set up activation normalization\n        self._setup_activation_normalization()\n\n        self.setup()  # Required for HookedRootModule\n\n    @torch.no_grad()\n    def fold_activation_norm_scaling_factor(self, scaling_factor: float):\n        self.W_enc.data *= scaling_factor  # type: ignore\n        self.W_dec.data /= scaling_factor  # type: ignore\n        self.b_dec.data /= scaling_factor  # type: ignore\n        self.cfg.normalize_activations = \"none\"\n\n    def get_activation_fn(self) -&gt; Callable[[torch.Tensor], torch.Tensor]:\n        \"\"\"Get the activation function specified in config.\"\"\"\n        return nn.ReLU()\n\n    def _setup_activation_normalization(self):\n        \"\"\"Set up activation normalization functions based on config.\"\"\"\n        if self.cfg.normalize_activations == \"constant_norm_rescale\":\n\n            def run_time_activation_norm_fn_in(x: torch.Tensor) -&gt; torch.Tensor:\n                self.x_norm_coeff = (self.cfg.d_in**0.5) / x.norm(dim=-1, keepdim=True)\n                return x * self.x_norm_coeff\n\n            def run_time_activation_norm_fn_out(x: torch.Tensor) -&gt; torch.Tensor:\n                x = x / self.x_norm_coeff  # type: ignore\n                del self.x_norm_coeff\n                return x\n\n            self.run_time_activation_norm_fn_in = run_time_activation_norm_fn_in\n            self.run_time_activation_norm_fn_out = run_time_activation_norm_fn_out\n        elif self.cfg.normalize_activations == \"layer_norm\":\n            #  we need to scale the norm of the input and store the scaling factor\n            def run_time_activation_ln_in(\n                x: torch.Tensor, eps: float = 1e-5\n            ) -&gt; torch.Tensor:\n                mu = x.mean(dim=-1, keepdim=True)\n                x = x - mu\n                std = x.std(dim=-1, keepdim=True)\n                x = x / (std + eps)\n                self.ln_mu = mu\n                self.ln_std = std\n                return x\n\n            def run_time_activation_ln_out(\n                x: torch.Tensor,\n                eps: float = 1e-5,  # noqa: ARG001\n            ) -&gt; torch.Tensor:\n                return x * self.ln_std + self.ln_mu  # type: ignore\n\n            self.run_time_activation_norm_fn_in = run_time_activation_ln_in\n            self.run_time_activation_norm_fn_out = run_time_activation_ln_out\n        else:\n            self.run_time_activation_norm_fn_in = lambda x: x\n            self.run_time_activation_norm_fn_out = lambda x: x\n\n    def initialize_weights(self):\n        \"\"\"Initialize model weights.\"\"\"\n        self.b_dec = nn.Parameter(\n            torch.zeros(self.cfg.d_in, dtype=self.dtype, device=self.device)\n        )\n\n        w_dec_data = torch.empty(\n            self.cfg.d_sae, self.cfg.d_in, dtype=self.dtype, device=self.device\n        )\n        nn.init.kaiming_uniform_(w_dec_data)\n        self.W_dec = nn.Parameter(w_dec_data)\n\n        w_enc_data = self.W_dec.data.T.clone().detach().contiguous()\n        self.W_enc = nn.Parameter(w_enc_data)\n\n    @abstractmethod\n    def encode(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n        \"\"\"Encode input tensor to feature space.\"\"\"\n        pass\n\n    @abstractmethod\n    def decode(\n        self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n    ) -&gt; Float[torch.Tensor, \"... d_in\"]:\n        \"\"\"Decode feature activations back to input space.\"\"\"\n        pass\n\n    def turn_on_forward_pass_hook_z_reshaping(self):\n        if (\n            self.cfg.metadata.hook_name is not None\n            and not self.cfg.metadata.hook_name.endswith(\"_z\")\n        ):\n            raise ValueError(\"This method should only be called for hook_z SAEs.\")\n\n        # print(f\"Turning on hook_z reshaping for {self.cfg.hook_name}\")\n\n        def reshape_fn_in(x: torch.Tensor):\n            # print(f\"reshape_fn_in input shape: {x.shape}\")\n            self.d_head = x.shape[-1]\n            # print(f\"Setting d_head to: {self.d_head}\")\n            self.reshape_fn_in = lambda x: einops.rearrange(\n                x, \"... n_heads d_head -&gt; ... (n_heads d_head)\"\n            )\n            return einops.rearrange(x, \"... n_heads d_head -&gt; ... (n_heads d_head)\")\n\n        self.reshape_fn_in = reshape_fn_in\n        self.reshape_fn_out = lambda x, d_head: einops.rearrange(\n            x, \"... (n_heads d_head) -&gt; ... n_heads d_head\", d_head=d_head\n        )\n        self.hook_z_reshaping_mode = True\n        # print(f\"hook_z reshaping turned on, self.d_head={getattr(self, 'd_head', None)}\")\n\n    def turn_off_forward_pass_hook_z_reshaping(self):\n        self.reshape_fn_in = lambda x: x\n        self.reshape_fn_out = lambda x, d_head: x  # noqa: ARG005\n        self.d_head = None\n        self.hook_z_reshaping_mode = False\n\n    @overload\n    def to(\n        self: T_SAE,\n        device: torch.device | str | None = ...,\n        dtype: torch.dtype | None = ...,\n        non_blocking: bool = ...,\n    ) -&gt; T_SAE: ...\n\n    @overload\n    def to(self: T_SAE, dtype: torch.dtype, non_blocking: bool = ...) -&gt; T_SAE: ...\n\n    @overload\n    def to(self: T_SAE, tensor: torch.Tensor, non_blocking: bool = ...) -&gt; T_SAE: ...\n\n    def to(self: T_SAE, *args: Any, **kwargs: Any) -&gt; T_SAE:  # type: ignore\n        device_arg = None\n        dtype_arg = None\n\n        # Check args\n        for arg in args:\n            if isinstance(arg, (torch.device, str)):\n                device_arg = arg\n            elif isinstance(arg, torch.dtype):\n                dtype_arg = arg\n            elif isinstance(arg, torch.Tensor):\n                device_arg = arg.device\n                dtype_arg = arg.dtype\n\n        # Check kwargs\n        device_arg = kwargs.get(\"device\", device_arg)\n        dtype_arg = kwargs.get(\"dtype\", dtype_arg)\n\n        # Update device in config if provided\n        if device_arg is not None:\n            # Convert device to torch.device if it's a string\n            device = (\n                torch.device(device_arg) if isinstance(device_arg, str) else device_arg\n            )\n\n            # Update the cfg.device\n            self.cfg.device = str(device)\n\n            # Update the device property\n            self.device = device\n\n        # Update dtype in config if provided\n        if dtype_arg is not None:\n            # Update the cfg.dtype\n            self.cfg.dtype = str(dtype_arg)\n\n            # Update the dtype property\n            self.dtype = dtype_arg\n\n        return super().to(*args, **kwargs)\n\n    def process_sae_in(\n        self, sae_in: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; Float[torch.Tensor, \"... d_in\"]:\n        # print(f\"Input shape to process_sae_in: {sae_in.shape}\")\n        # print(f\"self.cfg.hook_name: {self.cfg.hook_name}\")\n        # print(f\"self.b_dec shape: {self.b_dec.shape}\")\n        # print(f\"Hook z reshaping mode: {getattr(self, 'hook_z_reshaping_mode', False)}\")\n\n        sae_in = sae_in.to(self.dtype)\n\n        # print(f\"Shape before reshape_fn_in: {sae_in.shape}\")\n        sae_in = self.reshape_fn_in(sae_in)\n        # print(f\"Shape after reshape_fn_in: {sae_in.shape}\")\n\n        sae_in = self.hook_sae_input(sae_in)\n        sae_in = self.run_time_activation_norm_fn_in(sae_in)\n\n        # Here's where the error happens\n        bias_term = self.b_dec * self.cfg.apply_b_dec_to_input\n        # print(f\"Bias term shape: {bias_term.shape}\")\n\n        return sae_in - bias_term\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the SAE.\"\"\"\n        feature_acts = self.encode(x)\n        sae_out = self.decode(feature_acts)\n\n        if self.use_error_term:\n            with torch.no_grad():\n                # Recompute without hooks for true error term\n                with _disable_hooks(self):\n                    feature_acts_clean = self.encode(x)\n                    x_reconstruct_clean = self.decode(feature_acts_clean)\n                sae_error = self.hook_sae_error(x - x_reconstruct_clean)\n            sae_out = sae_out + sae_error\n\n        return self.hook_sae_output(sae_out)\n\n    # overwrite this in subclasses to modify the state_dict in-place before saving\n    def process_state_dict_for_saving(self, state_dict: dict[str, Any]) -&gt; None:\n        pass\n\n    # overwrite this in subclasses to modify the state_dict in-place after loading\n    def process_state_dict_for_loading(self, state_dict: dict[str, Any]) -&gt; None:\n        pass\n\n    @torch.no_grad()\n    def fold_W_dec_norm(self):\n        \"\"\"Fold decoder norms into encoder.\"\"\"\n        W_dec_norms = self.W_dec.norm(dim=-1).unsqueeze(1)\n        self.W_dec.data = self.W_dec.data / W_dec_norms\n        self.W_enc.data = self.W_enc.data * W_dec_norms.T\n\n        # Only update b_enc if it exists (standard/jumprelu architectures)\n        if hasattr(self, \"b_enc\") and isinstance(self.b_enc, nn.Parameter):\n            self.b_enc.data = self.b_enc.data * W_dec_norms.squeeze()\n\n    def get_name(self):\n        \"\"\"Generate a name for this SAE.\"\"\"\n        return f\"sae_{self.cfg.metadata.model_name}_{self.cfg.metadata.hook_name}_{self.cfg.d_sae}\"\n\n    def save_model(self, path: str | Path) -&gt; tuple[Path, Path]:\n        \"\"\"Save model weights and config to disk.\"\"\"\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Generate the weights\n        state_dict = self.state_dict()  # Use internal SAE state dict\n        self.process_state_dict_for_saving(state_dict)\n        model_weights_path = path / SAE_WEIGHTS_FILENAME\n        save_file(state_dict, model_weights_path)\n\n        # Save the config\n        config = self.cfg.to_dict()\n        cfg_path = path / SAE_CFG_FILENAME\n        with open(cfg_path, \"w\") as f:\n            json.dump(config, f)\n\n        return model_weights_path, cfg_path\n\n    # Class methods for loading models\n    @classmethod\n    @deprecated(\"Use load_from_disk instead\")\n    def load_from_pretrained(\n        cls: Type[T_SAE],\n        path: str | Path,\n        device: str = \"cpu\",\n        dtype: str | None = None,\n    ) -&gt; T_SAE:\n        return cls.load_from_disk(path, device=device, dtype=dtype)\n\n    @classmethod\n    def load_from_disk(\n        cls: Type[T_SAE],\n        path: str | Path,\n        device: str = \"cpu\",\n        dtype: str | None = None,\n        converter: PretrainedSaeDiskLoader = sae_lens_disk_loader,\n    ) -&gt; T_SAE:\n        overrides = {\"dtype\": dtype} if dtype is not None else None\n        cfg_dict, state_dict = converter(path, device, cfg_overrides=overrides)\n        cfg_dict = handle_config_defaulting(cfg_dict)\n        sae_config_cls = cls.get_sae_config_class_for_architecture(\n            cfg_dict[\"architecture\"]\n        )\n        sae_cfg = sae_config_cls.from_dict(cfg_dict)\n        sae_cls = cls.get_sae_class_for_architecture(sae_cfg.architecture())\n        sae = sae_cls(sae_cfg)\n        sae.process_state_dict_for_loading(state_dict)\n        sae.load_state_dict(state_dict)\n        return sae\n\n    @classmethod\n    def from_pretrained(\n        cls: Type[T_SAE],\n        release: str,\n        sae_id: str,\n        device: str = \"cpu\",\n        force_download: bool = False,\n        converter: PretrainedSaeHuggingfaceLoader | None = None,\n    ) -&gt; T_SAE:\n        \"\"\"\n        Load a pretrained SAE from the Hugging Face model hub.\n\n        Args:\n            release: The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.\n            id: The id of the SAE to load. This will be mapped to a path in the huggingface repo.\n            device: The device to load the SAE on.\n        \"\"\"\n        return cls.from_pretrained_with_cfg_and_sparsity(\n            release, sae_id, device, force_download, converter=converter\n        )[0]\n\n    @classmethod\n    def from_pretrained_with_cfg_and_sparsity(\n        cls: Type[T_SAE],\n        release: str,\n        sae_id: str,\n        device: str = \"cpu\",\n        force_download: bool = False,\n        converter: PretrainedSaeHuggingfaceLoader | None = None,\n    ) -&gt; tuple[T_SAE, dict[str, Any], torch.Tensor | None]:\n        \"\"\"\n        Load a pretrained SAE from the Hugging Face model hub, along with its config dict and sparsity, if present.\n        In SAELens &lt;= 5.x.x, this was called SAE.from_pretrained().\n\n        Args:\n            release: The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.\n            id: The id of the SAE to load. This will be mapped to a path in the huggingface repo.\n            device: The device to load the SAE on.\n        \"\"\"\n\n        # get sae directory\n        sae_directory = get_pretrained_saes_directory()\n\n        # Validate release and sae_id\n        if release not in sae_directory:\n            if \"/\" not in release:\n                raise ValueError(\n                    f\"Release {release} not found in pretrained SAEs directory, and is not a valid huggingface repo.\"\n                )\n        elif sae_id not in sae_directory[release].saes_map:\n            # Handle special cases like Gemma Scope\n            if (\n                \"gemma-scope\" in release\n                and \"canonical\" not in release\n                and f\"{release}-canonical\" in sae_directory\n            ):\n                canonical_ids = list(\n                    sae_directory[release + \"-canonical\"].saes_map.keys()\n                )\n                # Shorten the lengthy string of valid IDs\n                if len(canonical_ids) &gt; 5:\n                    str_canonical_ids = str(canonical_ids[:5])[:-1] + \", ...]\"\n                else:\n                    str_canonical_ids = str(canonical_ids)\n                value_suffix = f\" If you don't want to specify an L0 value, consider using release {release}-canonical which has valid IDs {str_canonical_ids}\"\n            else:\n                value_suffix = \"\"\n\n            valid_ids = list(sae_directory[release].saes_map.keys())\n            # Shorten the lengthy string of valid IDs\n            if len(valid_ids) &gt; 5:\n                str_valid_ids = str(valid_ids[:5])[:-1] + \", ...]\"\n            else:\n                str_valid_ids = str(valid_ids)\n\n            raise ValueError(\n                f\"ID {sae_id} not found in release {release}. Valid IDs are {str_valid_ids}.\"\n                + value_suffix\n            )\n\n        conversion_loader = (\n            converter\n            or NAMED_PRETRAINED_SAE_LOADERS[get_conversion_loader_name(release)]\n        )\n        repo_id, folder_name = get_repo_id_and_folder_name(release, sae_id)\n        config_overrides = get_config_overrides(release, sae_id)\n        config_overrides[\"device\"] = device\n\n        # Load config and weights\n        cfg_dict, state_dict, log_sparsities = conversion_loader(\n            repo_id=repo_id,\n            folder_name=folder_name,\n            device=device,\n            force_download=force_download,\n            cfg_overrides=config_overrides,\n        )\n        cfg_dict = handle_config_defaulting(cfg_dict)\n\n        # Create SAE with appropriate architecture\n        sae_config_cls = cls.get_sae_config_class_for_architecture(\n            cfg_dict[\"architecture\"]\n        )\n        sae_cfg = sae_config_cls.from_dict(cfg_dict)\n        sae_cls = cls.get_sae_class_for_architecture(sae_cfg.architecture())\n        sae = sae_cls(sae_cfg)\n        sae.process_state_dict_for_loading(state_dict)\n        sae.load_state_dict(state_dict)\n\n        # Apply normalization if needed\n        if cfg_dict.get(\"normalize_activations\") == \"expected_average_only_in\":\n            norm_scaling_factor = get_norm_scaling_factor(release, sae_id)\n            if norm_scaling_factor is not None:\n                sae.fold_activation_norm_scaling_factor(norm_scaling_factor)\n                cfg_dict[\"normalize_activations\"] = \"none\"\n            else:\n                warnings.warn(\n                    f\"norm_scaling_factor not found for {release} and {sae_id}, but normalize_activations is 'expected_average_only_in'. Skipping normalization folding.\"\n                )\n\n        return sae, cfg_dict, log_sparsities\n\n    @classmethod\n    def from_dict(cls: Type[T_SAE], config_dict: dict[str, Any]) -&gt; T_SAE:\n        \"\"\"Create an SAE from a config dictionary.\"\"\"\n        sae_cls = cls.get_sae_class_for_architecture(config_dict[\"architecture\"])\n        sae_config_cls = cls.get_sae_config_class_for_architecture(\n            config_dict[\"architecture\"]\n        )\n        return sae_cls(sae_config_cls.from_dict(config_dict))\n\n    @classmethod\n    def get_sae_class_for_architecture(\n        cls: Type[T_SAE], architecture: str\n    ) -&gt; Type[T_SAE]:\n        \"\"\"Get the SAE class for a given architecture.\"\"\"\n        sae_cls, _ = get_sae_class(architecture)\n        if not issubclass(sae_cls, cls):\n            raise ValueError(\n                f\"Loaded SAE is not of type {cls.__name__}. Use {sae_cls.__name__} instead\"\n            )\n        return sae_cls\n\n    # in the future, this can be used to load different config classes for different architectures\n    @classmethod\n    def get_sae_config_class_for_architecture(\n        cls,\n        architecture: str,  # noqa: ARG003\n    ) -&gt; type[SAEConfig]:\n        return SAEConfig\n\n    ### Methods to support deprecated usage of SAE.from_pretrained() ###\n\n    def __getitem__(self, index: int) -&gt; Any:\n        \"\"\"\n        Support indexing for backward compatibility with tuple unpacking.\n        DEPRECATED: SAE.from_pretrained() no longer returns a tuple.\n        Use SAE.from_pretrained_with_cfg_and_sparsity() instead.\n        \"\"\"\n        warnings.warn(\n            \"Indexing SAE objects is deprecated. SAE.from_pretrained() now returns \"\n            \"only the SAE object. Use SAE.from_pretrained_with_cfg_and_sparsity() \"\n            \"to get the config dict and sparsity as well.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n        if index == 0:\n            return self\n        if index == 1:\n            return self.cfg.to_dict()\n        if index == 2:\n            return None\n        raise IndexError(f\"SAE tuple index {index} out of range\")\n\n    def __iter__(self):\n        \"\"\"\n        Support unpacking for backward compatibility with tuple unpacking.\n        DEPRECATED: SAE.from_pretrained() no longer returns a tuple.\n        Use SAE.from_pretrained_with_cfg_and_sparsity() instead.\n        \"\"\"\n        warnings.warn(\n            \"Unpacking SAE objects is deprecated. SAE.from_pretrained() now returns \"\n            \"only the SAE object. Use SAE.from_pretrained_with_cfg_and_sparsity() \"\n            \"to get the config dict and sparsity as well.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n        yield self\n        yield self.cfg.to_dict()\n        yield None\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Support len() for backward compatibility with tuple unpacking.\n        DEPRECATED: SAE.from_pretrained() no longer returns a tuple.\n        Use SAE.from_pretrained_with_cfg_and_sparsity() instead.\n        \"\"\"\n        warnings.warn(\n            \"Getting length of SAE objects is deprecated. SAE.from_pretrained() now returns \"\n            \"only the SAE object. Use SAE.from_pretrained_with_cfg_and_sparsity() \"\n            \"to get the config dict and sparsity as well.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n        return 3\n</code></pre>"},{"location":"api/#sae_lens.SAE.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Support indexing for backward compatibility with tuple unpacking. DEPRECATED: SAE.from_pretrained() no longer returns a tuple. Use SAE.from_pretrained_with_cfg_and_sparsity() instead.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def __getitem__(self, index: int) -&gt; Any:\n    \"\"\"\n    Support indexing for backward compatibility with tuple unpacking.\n    DEPRECATED: SAE.from_pretrained() no longer returns a tuple.\n    Use SAE.from_pretrained_with_cfg_and_sparsity() instead.\n    \"\"\"\n    warnings.warn(\n        \"Indexing SAE objects is deprecated. SAE.from_pretrained() now returns \"\n        \"only the SAE object. Use SAE.from_pretrained_with_cfg_and_sparsity() \"\n        \"to get the config dict and sparsity as well.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    if index == 0:\n        return self\n    if index == 1:\n        return self.cfg.to_dict()\n    if index == 2:\n        return None\n    raise IndexError(f\"SAE tuple index {index} out of range\")\n</code></pre>"},{"location":"api/#sae_lens.SAE.__init__","title":"<code>__init__(cfg, use_error_term=False)</code>","text":"<p>Initialize the SAE.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def __init__(self, cfg: T_SAE_CONFIG, use_error_term: bool = False):\n    \"\"\"Initialize the SAE.\"\"\"\n    super().__init__()\n\n    self.cfg = cfg\n\n    if cfg.metadata and cfg.metadata:\n        warnings.warn(\n            \"\\nThis SAE has non-empty model_from_pretrained_kwargs. \"\n            \"\\nFor optimal performance, load the model like so:\\n\"\n            \"model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\",\n            category=UserWarning,\n            stacklevel=1,\n        )\n\n    self.dtype = DTYPE_MAP[cfg.dtype]\n    self.device = torch.device(cfg.device)\n    self.use_error_term = use_error_term\n\n    # Set up activation function\n    self.activation_fn = self.get_activation_fn()\n\n    # Initialize weights\n    self.initialize_weights()\n\n    # Set up hooks\n    self.hook_sae_input = HookPoint()\n    self.hook_sae_acts_pre = HookPoint()\n    self.hook_sae_acts_post = HookPoint()\n    self.hook_sae_output = HookPoint()\n    self.hook_sae_recons = HookPoint()\n    self.hook_sae_error = HookPoint()\n\n    # handle hook_z reshaping if needed.\n    if self.cfg.reshape_activations == \"hook_z\":\n        self.turn_on_forward_pass_hook_z_reshaping()\n    else:\n        self.turn_off_forward_pass_hook_z_reshaping()\n\n    # Set up activation normalization\n    self._setup_activation_normalization()\n\n    self.setup()  # Required for HookedRootModule\n</code></pre>"},{"location":"api/#sae_lens.SAE.__iter__","title":"<code>__iter__()</code>","text":"<p>Support unpacking for backward compatibility with tuple unpacking. DEPRECATED: SAE.from_pretrained() no longer returns a tuple. Use SAE.from_pretrained_with_cfg_and_sparsity() instead.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    Support unpacking for backward compatibility with tuple unpacking.\n    DEPRECATED: SAE.from_pretrained() no longer returns a tuple.\n    Use SAE.from_pretrained_with_cfg_and_sparsity() instead.\n    \"\"\"\n    warnings.warn(\n        \"Unpacking SAE objects is deprecated. SAE.from_pretrained() now returns \"\n        \"only the SAE object. Use SAE.from_pretrained_with_cfg_and_sparsity() \"\n        \"to get the config dict and sparsity as well.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    yield self\n    yield self.cfg.to_dict()\n    yield None\n</code></pre>"},{"location":"api/#sae_lens.SAE.__len__","title":"<code>__len__()</code>","text":"<p>Support len() for backward compatibility with tuple unpacking. DEPRECATED: SAE.from_pretrained() no longer returns a tuple. Use SAE.from_pretrained_with_cfg_and_sparsity() instead.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Support len() for backward compatibility with tuple unpacking.\n    DEPRECATED: SAE.from_pretrained() no longer returns a tuple.\n    Use SAE.from_pretrained_with_cfg_and_sparsity() instead.\n    \"\"\"\n    warnings.warn(\n        \"Getting length of SAE objects is deprecated. SAE.from_pretrained() now returns \"\n        \"only the SAE object. Use SAE.from_pretrained_with_cfg_and_sparsity() \"\n        \"to get the config dict and sparsity as well.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    return 3\n</code></pre>"},{"location":"api/#sae_lens.SAE.decode","title":"<code>decode(feature_acts)</code>  <code>abstractmethod</code>","text":"<p>Decode feature activations back to input space.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@abstractmethod\ndef decode(\n    self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n) -&gt; Float[torch.Tensor, \"... d_in\"]:\n    \"\"\"Decode feature activations back to input space.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#sae_lens.SAE.encode","title":"<code>encode(x)</code>  <code>abstractmethod</code>","text":"<p>Encode input tensor to feature space.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@abstractmethod\ndef encode(\n    self, x: Float[torch.Tensor, \"... d_in\"]\n) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n    \"\"\"Encode input tensor to feature space.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#sae_lens.SAE.fold_W_dec_norm","title":"<code>fold_W_dec_norm()</code>","text":"<p>Fold decoder norms into encoder.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@torch.no_grad()\ndef fold_W_dec_norm(self):\n    \"\"\"Fold decoder norms into encoder.\"\"\"\n    W_dec_norms = self.W_dec.norm(dim=-1).unsqueeze(1)\n    self.W_dec.data = self.W_dec.data / W_dec_norms\n    self.W_enc.data = self.W_enc.data * W_dec_norms.T\n\n    # Only update b_enc if it exists (standard/jumprelu architectures)\n    if hasattr(self, \"b_enc\") and isinstance(self.b_enc, nn.Parameter):\n        self.b_enc.data = self.b_enc.data * W_dec_norms.squeeze()\n</code></pre>"},{"location":"api/#sae_lens.SAE.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the SAE.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the SAE.\"\"\"\n    feature_acts = self.encode(x)\n    sae_out = self.decode(feature_acts)\n\n    if self.use_error_term:\n        with torch.no_grad():\n            # Recompute without hooks for true error term\n            with _disable_hooks(self):\n                feature_acts_clean = self.encode(x)\n                x_reconstruct_clean = self.decode(feature_acts_clean)\n            sae_error = self.hook_sae_error(x - x_reconstruct_clean)\n        sae_out = sae_out + sae_error\n\n    return self.hook_sae_output(sae_out)\n</code></pre>"},{"location":"api/#sae_lens.SAE.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Create an SAE from a config dictionary.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@classmethod\ndef from_dict(cls: Type[T_SAE], config_dict: dict[str, Any]) -&gt; T_SAE:\n    \"\"\"Create an SAE from a config dictionary.\"\"\"\n    sae_cls = cls.get_sae_class_for_architecture(config_dict[\"architecture\"])\n    sae_config_cls = cls.get_sae_config_class_for_architecture(\n        config_dict[\"architecture\"]\n    )\n    return sae_cls(sae_config_cls.from_dict(config_dict))\n</code></pre>"},{"location":"api/#sae_lens.SAE.from_pretrained","title":"<code>from_pretrained(release, sae_id, device='cpu', force_download=False, converter=None)</code>  <code>classmethod</code>","text":"<p>Load a pretrained SAE from the Hugging Face model hub.</p> <p>Parameters:</p> Name Type Description Default <code>release</code> <code>str</code> <p>The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.</p> required <code>id</code> <p>The id of the SAE to load. This will be mapped to a path in the huggingface repo.</p> required <code>device</code> <code>str</code> <p>The device to load the SAE on.</p> <code>'cpu'</code> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls: Type[T_SAE],\n    release: str,\n    sae_id: str,\n    device: str = \"cpu\",\n    force_download: bool = False,\n    converter: PretrainedSaeHuggingfaceLoader | None = None,\n) -&gt; T_SAE:\n    \"\"\"\n    Load a pretrained SAE from the Hugging Face model hub.\n\n    Args:\n        release: The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.\n        id: The id of the SAE to load. This will be mapped to a path in the huggingface repo.\n        device: The device to load the SAE on.\n    \"\"\"\n    return cls.from_pretrained_with_cfg_and_sparsity(\n        release, sae_id, device, force_download, converter=converter\n    )[0]\n</code></pre>"},{"location":"api/#sae_lens.SAE.from_pretrained_with_cfg_and_sparsity","title":"<code>from_pretrained_with_cfg_and_sparsity(release, sae_id, device='cpu', force_download=False, converter=None)</code>  <code>classmethod</code>","text":"<p>Load a pretrained SAE from the Hugging Face model hub, along with its config dict and sparsity, if present. In SAELens &lt;= 5.x.x, this was called SAE.from_pretrained().</p> <p>Parameters:</p> Name Type Description Default <code>release</code> <code>str</code> <p>The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.</p> required <code>id</code> <p>The id of the SAE to load. This will be mapped to a path in the huggingface repo.</p> required <code>device</code> <code>str</code> <p>The device to load the SAE on.</p> <code>'cpu'</code> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@classmethod\ndef from_pretrained_with_cfg_and_sparsity(\n    cls: Type[T_SAE],\n    release: str,\n    sae_id: str,\n    device: str = \"cpu\",\n    force_download: bool = False,\n    converter: PretrainedSaeHuggingfaceLoader | None = None,\n) -&gt; tuple[T_SAE, dict[str, Any], torch.Tensor | None]:\n    \"\"\"\n    Load a pretrained SAE from the Hugging Face model hub, along with its config dict and sparsity, if present.\n    In SAELens &lt;= 5.x.x, this was called SAE.from_pretrained().\n\n    Args:\n        release: The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.\n        id: The id of the SAE to load. This will be mapped to a path in the huggingface repo.\n        device: The device to load the SAE on.\n    \"\"\"\n\n    # get sae directory\n    sae_directory = get_pretrained_saes_directory()\n\n    # Validate release and sae_id\n    if release not in sae_directory:\n        if \"/\" not in release:\n            raise ValueError(\n                f\"Release {release} not found in pretrained SAEs directory, and is not a valid huggingface repo.\"\n            )\n    elif sae_id not in sae_directory[release].saes_map:\n        # Handle special cases like Gemma Scope\n        if (\n            \"gemma-scope\" in release\n            and \"canonical\" not in release\n            and f\"{release}-canonical\" in sae_directory\n        ):\n            canonical_ids = list(\n                sae_directory[release + \"-canonical\"].saes_map.keys()\n            )\n            # Shorten the lengthy string of valid IDs\n            if len(canonical_ids) &gt; 5:\n                str_canonical_ids = str(canonical_ids[:5])[:-1] + \", ...]\"\n            else:\n                str_canonical_ids = str(canonical_ids)\n            value_suffix = f\" If you don't want to specify an L0 value, consider using release {release}-canonical which has valid IDs {str_canonical_ids}\"\n        else:\n            value_suffix = \"\"\n\n        valid_ids = list(sae_directory[release].saes_map.keys())\n        # Shorten the lengthy string of valid IDs\n        if len(valid_ids) &gt; 5:\n            str_valid_ids = str(valid_ids[:5])[:-1] + \", ...]\"\n        else:\n            str_valid_ids = str(valid_ids)\n\n        raise ValueError(\n            f\"ID {sae_id} not found in release {release}. Valid IDs are {str_valid_ids}.\"\n            + value_suffix\n        )\n\n    conversion_loader = (\n        converter\n        or NAMED_PRETRAINED_SAE_LOADERS[get_conversion_loader_name(release)]\n    )\n    repo_id, folder_name = get_repo_id_and_folder_name(release, sae_id)\n    config_overrides = get_config_overrides(release, sae_id)\n    config_overrides[\"device\"] = device\n\n    # Load config and weights\n    cfg_dict, state_dict, log_sparsities = conversion_loader(\n        repo_id=repo_id,\n        folder_name=folder_name,\n        device=device,\n        force_download=force_download,\n        cfg_overrides=config_overrides,\n    )\n    cfg_dict = handle_config_defaulting(cfg_dict)\n\n    # Create SAE with appropriate architecture\n    sae_config_cls = cls.get_sae_config_class_for_architecture(\n        cfg_dict[\"architecture\"]\n    )\n    sae_cfg = sae_config_cls.from_dict(cfg_dict)\n    sae_cls = cls.get_sae_class_for_architecture(sae_cfg.architecture())\n    sae = sae_cls(sae_cfg)\n    sae.process_state_dict_for_loading(state_dict)\n    sae.load_state_dict(state_dict)\n\n    # Apply normalization if needed\n    if cfg_dict.get(\"normalize_activations\") == \"expected_average_only_in\":\n        norm_scaling_factor = get_norm_scaling_factor(release, sae_id)\n        if norm_scaling_factor is not None:\n            sae.fold_activation_norm_scaling_factor(norm_scaling_factor)\n            cfg_dict[\"normalize_activations\"] = \"none\"\n        else:\n            warnings.warn(\n                f\"norm_scaling_factor not found for {release} and {sae_id}, but normalize_activations is 'expected_average_only_in'. Skipping normalization folding.\"\n            )\n\n    return sae, cfg_dict, log_sparsities\n</code></pre>"},{"location":"api/#sae_lens.SAE.get_activation_fn","title":"<code>get_activation_fn()</code>","text":"<p>Get the activation function specified in config.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def get_activation_fn(self) -&gt; Callable[[torch.Tensor], torch.Tensor]:\n    \"\"\"Get the activation function specified in config.\"\"\"\n    return nn.ReLU()\n</code></pre>"},{"location":"api/#sae_lens.SAE.get_name","title":"<code>get_name()</code>","text":"<p>Generate a name for this SAE.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def get_name(self):\n    \"\"\"Generate a name for this SAE.\"\"\"\n    return f\"sae_{self.cfg.metadata.model_name}_{self.cfg.metadata.hook_name}_{self.cfg.d_sae}\"\n</code></pre>"},{"location":"api/#sae_lens.SAE.get_sae_class_for_architecture","title":"<code>get_sae_class_for_architecture(architecture)</code>  <code>classmethod</code>","text":"<p>Get the SAE class for a given architecture.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@classmethod\ndef get_sae_class_for_architecture(\n    cls: Type[T_SAE], architecture: str\n) -&gt; Type[T_SAE]:\n    \"\"\"Get the SAE class for a given architecture.\"\"\"\n    sae_cls, _ = get_sae_class(architecture)\n    if not issubclass(sae_cls, cls):\n        raise ValueError(\n            f\"Loaded SAE is not of type {cls.__name__}. Use {sae_cls.__name__} instead\"\n        )\n    return sae_cls\n</code></pre>"},{"location":"api/#sae_lens.SAE.initialize_weights","title":"<code>initialize_weights()</code>","text":"<p>Initialize model weights.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def initialize_weights(self):\n    \"\"\"Initialize model weights.\"\"\"\n    self.b_dec = nn.Parameter(\n        torch.zeros(self.cfg.d_in, dtype=self.dtype, device=self.device)\n    )\n\n    w_dec_data = torch.empty(\n        self.cfg.d_sae, self.cfg.d_in, dtype=self.dtype, device=self.device\n    )\n    nn.init.kaiming_uniform_(w_dec_data)\n    self.W_dec = nn.Parameter(w_dec_data)\n\n    w_enc_data = self.W_dec.data.T.clone().detach().contiguous()\n    self.W_enc = nn.Parameter(w_enc_data)\n</code></pre>"},{"location":"api/#sae_lens.SAE.save_model","title":"<code>save_model(path)</code>","text":"<p>Save model weights and config to disk.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def save_model(self, path: str | Path) -&gt; tuple[Path, Path]:\n    \"\"\"Save model weights and config to disk.\"\"\"\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Generate the weights\n    state_dict = self.state_dict()  # Use internal SAE state dict\n    self.process_state_dict_for_saving(state_dict)\n    model_weights_path = path / SAE_WEIGHTS_FILENAME\n    save_file(state_dict, model_weights_path)\n\n    # Save the config\n    config = self.cfg.to_dict()\n    cfg_path = path / SAE_CFG_FILENAME\n    with open(cfg_path, \"w\") as f:\n        json.dump(config, f)\n\n    return model_weights_path, cfg_path\n</code></pre>"},{"location":"api/#sae_lens.SAEConfig","title":"<code>SAEConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base configuration for SAE models.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@dataclass\nclass SAEConfig(ABC):\n    \"\"\"Base configuration for SAE models.\"\"\"\n\n    d_in: int\n    d_sae: int\n    dtype: str = \"float32\"\n    device: str = \"cpu\"\n    apply_b_dec_to_input: bool = True\n    normalize_activations: Literal[\n        \"none\", \"expected_average_only_in\", \"constant_norm_rescale\", \"layer_norm\"\n    ] = \"none\"  # none, expected_average_only_in (Anthropic April Update), constant_norm_rescale (Anthropic Feb Update)\n    reshape_activations: Literal[\"none\", \"hook_z\"] = \"none\"\n    metadata: SAEMetadata = field(default_factory=SAEMetadata)\n\n    @classmethod\n    @abstractmethod\n    def architecture(cls) -&gt; str: ...\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        res = {field.name: getattr(self, field.name) for field in fields(self)}\n        res[\"metadata\"] = self.metadata.to_dict()\n        res[\"architecture\"] = self.architecture()\n        return res\n\n    @classmethod\n    def from_dict(cls: type[T_SAE_CONFIG], config_dict: dict[str, Any]) -&gt; T_SAE_CONFIG:\n        cfg_class = get_sae_class(config_dict[\"architecture\"])[1]\n        filtered_config_dict = filter_valid_dataclass_fields(config_dict, cfg_class)\n        res = cfg_class(**filtered_config_dict)\n        if \"metadata\" in config_dict:\n            res.metadata = SAEMetadata(**config_dict[\"metadata\"])\n        if not isinstance(res, cls):\n            raise ValueError(\n                f\"SAE config class {cls} does not match dict config class {type(res)}\"\n            )\n        return res\n\n    def __post_init__(self):\n        if self.normalize_activations not in [\n            \"none\",\n            \"expected_average_only_in\",\n            \"constant_norm_rescale\",\n            \"layer_norm\",\n        ]:\n            raise ValueError(\n                f\"normalize_activations must be none, expected_average_only_in, layer_norm, or constant_norm_rescale. Got {self.normalize_activations}\"\n            )\n</code></pre>"},{"location":"api/#sae_lens.SkipTranscoder","title":"<code>SkipTranscoder</code>","text":"<p>               Bases: <code>Transcoder</code></p> <p>A transcoder with a learnable skip connection.</p> <p>Implements: f(x) = W_dec @ relu(W_enc @ x + b_enc) + W_skip @ x + b_dec where W_skip is initialized to zeros.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>class SkipTranscoder(Transcoder):\n    \"\"\"\n    A transcoder with a learnable skip connection.\n\n    Implements: f(x) = W_dec @ relu(W_enc @ x + b_enc) + W_skip @ x + b_dec\n    where W_skip is initialized to zeros.\n    \"\"\"\n\n    cfg: SkipTranscoderConfig  # type: ignore[assignment]\n    W_skip: nn.Parameter\n\n    def __init__(self, cfg: SkipTranscoderConfig):\n        super().__init__(cfg)\n        self.cfg = cfg\n\n        # Initialize skip connection matrix\n        # Shape: [d_out, d_in] to map from input to output dimension\n        self.W_skip = nn.Parameter(torch.zeros(self.cfg.d_out, self.cfg.d_in))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass for skip transcoder.\n\n        Args:\n            x: Input activations from the input hook point [batch, d_in]\n\n        Returns:\n            sae_out: Reconstructed activations for the output hook point\n            [batch, d_out]\n        \"\"\"\n        feature_acts = self.encode(x)\n        sae_out = self.decode(feature_acts)\n\n        # Add skip connection: W_skip @ x\n        # x has shape [batch, d_in], W_skip has shape [d_out, d_in]\n        skip_out = x @ self.W_skip.T.to(x.device)\n        return sae_out + skip_out\n\n    def forward_with_activations(\n        self,\n        x: torch.Tensor,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass returning both output and feature activations.\n\n        Args:\n            x: Input activations from the input hook point [batch, d_in]\n\n        Returns:\n            sae_out: Reconstructed activations for the output hook point\n            [batch, d_out]\n            feature_acts: Hidden activations [batch, d_sae]\n        \"\"\"\n        feature_acts = self.encode(x)\n        sae_out = self.decode(feature_acts)\n\n        # Add skip connection: W_skip @ x\n        # x has shape [batch, d_in], W_skip has shape [d_out, d_in]\n        skip_out = x @ self.W_skip.T.to(x.device)\n        sae_out = sae_out + skip_out\n\n        return sae_out, feature_acts\n\n    @classmethod\n    def from_dict(cls, config_dict: dict[str, Any]) -&gt; \"SkipTranscoder\":\n        cfg = SkipTranscoderConfig.from_dict(config_dict)\n        return cls(cfg)\n</code></pre>"},{"location":"api/#sae_lens.SkipTranscoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass for skip transcoder.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input activations from the input hook point [batch, d_in]</p> required <p>Returns:</p> Name Type Description <code>sae_out</code> <code>Tensor</code> <p>Reconstructed activations for the output hook point</p> <code>Tensor</code> <p>[batch, d_out]</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for skip transcoder.\n\n    Args:\n        x: Input activations from the input hook point [batch, d_in]\n\n    Returns:\n        sae_out: Reconstructed activations for the output hook point\n        [batch, d_out]\n    \"\"\"\n    feature_acts = self.encode(x)\n    sae_out = self.decode(feature_acts)\n\n    # Add skip connection: W_skip @ x\n    # x has shape [batch, d_in], W_skip has shape [d_out, d_in]\n    skip_out = x @ self.W_skip.T.to(x.device)\n    return sae_out + skip_out\n</code></pre>"},{"location":"api/#sae_lens.SkipTranscoder.forward_with_activations","title":"<code>forward_with_activations(x)</code>","text":"<p>Forward pass returning both output and feature activations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input activations from the input hook point [batch, d_in]</p> required <p>Returns:</p> Name Type Description <code>sae_out</code> <code>Tensor</code> <p>Reconstructed activations for the output hook point</p> <code>Tensor</code> <p>[batch, d_out]</p> <code>feature_acts</code> <code>tuple[Tensor, Tensor]</code> <p>Hidden activations [batch, d_sae]</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>def forward_with_activations(\n    self,\n    x: torch.Tensor,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Forward pass returning both output and feature activations.\n\n    Args:\n        x: Input activations from the input hook point [batch, d_in]\n\n    Returns:\n        sae_out: Reconstructed activations for the output hook point\n        [batch, d_out]\n        feature_acts: Hidden activations [batch, d_sae]\n    \"\"\"\n    feature_acts = self.encode(x)\n    sae_out = self.decode(feature_acts)\n\n    # Add skip connection: W_skip @ x\n    # x has shape [batch, d_in], W_skip has shape [d_out, d_in]\n    skip_out = x @ self.W_skip.T.to(x.device)\n    sae_out = sae_out + skip_out\n\n    return sae_out, feature_acts\n</code></pre>"},{"location":"api/#sae_lens.SkipTranscoderConfig","title":"<code>SkipTranscoderConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TranscoderConfig</code></p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>@dataclass\nclass SkipTranscoderConfig(TranscoderConfig):\n    @classmethod\n    def architecture(cls) -&gt; str:\n        \"\"\"Return the architecture name for this config.\"\"\"\n        return \"skip_transcoder\"\n\n    @classmethod\n    def from_dict(cls, config_dict: dict[str, Any]) -&gt; \"SkipTranscoderConfig\":\n        \"\"\"Create a SkipTranscoderConfig from a dictionary.\"\"\"\n        # Filter to only include valid dataclass fields\n        filtered_config_dict = filter_valid_dataclass_fields(config_dict, cls)\n\n        # Create the config instance\n        res = cls(**filtered_config_dict)\n\n        # Handle metadata if present\n        if \"metadata\" in config_dict:\n            res.metadata = SAEMetadata(**config_dict[\"metadata\"])\n\n        return res\n</code></pre>"},{"location":"api/#sae_lens.SkipTranscoderConfig.architecture","title":"<code>architecture()</code>  <code>classmethod</code>","text":"<p>Return the architecture name for this config.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>@classmethod\ndef architecture(cls) -&gt; str:\n    \"\"\"Return the architecture name for this config.\"\"\"\n    return \"skip_transcoder\"\n</code></pre>"},{"location":"api/#sae_lens.SkipTranscoderConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Create a SkipTranscoderConfig from a dictionary.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict[str, Any]) -&gt; \"SkipTranscoderConfig\":\n    \"\"\"Create a SkipTranscoderConfig from a dictionary.\"\"\"\n    # Filter to only include valid dataclass fields\n    filtered_config_dict = filter_valid_dataclass_fields(config_dict, cls)\n\n    # Create the config instance\n    res = cls(**filtered_config_dict)\n\n    # Handle metadata if present\n    if \"metadata\" in config_dict:\n        res.metadata = SAEMetadata(**config_dict[\"metadata\"])\n\n    return res\n</code></pre>"},{"location":"api/#sae_lens.StandardSAE","title":"<code>StandardSAE</code>","text":"<p>               Bases: <code>SAE[StandardSAEConfig]</code></p> <p>StandardSAE is an inference-only implementation of a Sparse Autoencoder (SAE) using a simple linear encoder and decoder.</p> It implements the required abstract methods from BaseSAE <ul> <li>initialize_weights: sets up simple parameter initializations for W_enc, b_enc, W_dec, and b_dec.</li> <li>encode: computes the feature activations from an input.</li> <li>decode: reconstructs the input from the feature activations.</li> </ul> <p>The BaseSAE.forward() method automatically calls encode and decode, including any error-term processing if configured.</p> Source code in <code>sae_lens/saes/standard_sae.py</code> <pre><code>class StandardSAE(SAE[StandardSAEConfig]):\n    \"\"\"\n    StandardSAE is an inference-only implementation of a Sparse Autoencoder (SAE)\n    using a simple linear encoder and decoder.\n\n    It implements the required abstract methods from BaseSAE:\n      - initialize_weights: sets up simple parameter initializations for W_enc, b_enc, W_dec, and b_dec.\n      - encode: computes the feature activations from an input.\n      - decode: reconstructs the input from the feature activations.\n\n    The BaseSAE.forward() method automatically calls encode and decode,\n    including any error-term processing if configured.\n    \"\"\"\n\n    b_enc: nn.Parameter\n\n    def __init__(self, cfg: StandardSAEConfig, use_error_term: bool = False):\n        super().__init__(cfg, use_error_term)\n\n    @override\n    def initialize_weights(self) -&gt; None:\n        # Initialize encoder weights and bias.\n        super().initialize_weights()\n        _init_weights_standard(self)\n\n    def encode(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n        \"\"\"\n        Encode the input tensor into the feature space.\n        \"\"\"\n        # Preprocess the SAE input (casting type, applying hooks, normalization)\n        sae_in = self.process_sae_in(x)\n        # Compute the pre-activation values\n        hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n        # Apply the activation function (e.g., ReLU, depending on config)\n        return self.hook_sae_acts_post(self.activation_fn(hidden_pre))\n\n    def decode(\n        self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n    ) -&gt; Float[torch.Tensor, \"... d_in\"]:\n        \"\"\"\n        Decode the feature activations back to the input space.\n        Now, if hook_z reshaping is turned on, we reverse the flattening.\n        \"\"\"\n        # 1) linear transform\n        sae_out_pre = feature_acts @ self.W_dec + self.b_dec\n        # 2) hook reconstruction\n        sae_out_pre = self.hook_sae_recons(sae_out_pre)\n        # 4) optional out-normalization (e.g. constant_norm_rescale)\n        sae_out_pre = self.run_time_activation_norm_fn_out(sae_out_pre)\n        # 5) if hook_z is enabled, rearrange back to (..., n_heads, d_head).\n        return self.reshape_fn_out(sae_out_pre, self.d_head)\n</code></pre>"},{"location":"api/#sae_lens.StandardSAE.decode","title":"<code>decode(feature_acts)</code>","text":"<p>Decode the feature activations back to the input space. Now, if hook_z reshaping is turned on, we reverse the flattening.</p> Source code in <code>sae_lens/saes/standard_sae.py</code> <pre><code>def decode(\n    self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n) -&gt; Float[torch.Tensor, \"... d_in\"]:\n    \"\"\"\n    Decode the feature activations back to the input space.\n    Now, if hook_z reshaping is turned on, we reverse the flattening.\n    \"\"\"\n    # 1) linear transform\n    sae_out_pre = feature_acts @ self.W_dec + self.b_dec\n    # 2) hook reconstruction\n    sae_out_pre = self.hook_sae_recons(sae_out_pre)\n    # 4) optional out-normalization (e.g. constant_norm_rescale)\n    sae_out_pre = self.run_time_activation_norm_fn_out(sae_out_pre)\n    # 5) if hook_z is enabled, rearrange back to (..., n_heads, d_head).\n    return self.reshape_fn_out(sae_out_pre, self.d_head)\n</code></pre>"},{"location":"api/#sae_lens.StandardSAE.encode","title":"<code>encode(x)</code>","text":"<p>Encode the input tensor into the feature space.</p> Source code in <code>sae_lens/saes/standard_sae.py</code> <pre><code>def encode(\n    self, x: Float[torch.Tensor, \"... d_in\"]\n) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n    \"\"\"\n    Encode the input tensor into the feature space.\n    \"\"\"\n    # Preprocess the SAE input (casting type, applying hooks, normalization)\n    sae_in = self.process_sae_in(x)\n    # Compute the pre-activation values\n    hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n    # Apply the activation function (e.g., ReLU, depending on config)\n    return self.hook_sae_acts_post(self.activation_fn(hidden_pre))\n</code></pre>"},{"location":"api/#sae_lens.StandardSAEConfig","title":"<code>StandardSAEConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SAEConfig</code></p> <p>Configuration class for a StandardSAE.</p> Source code in <code>sae_lens/saes/standard_sae.py</code> <pre><code>@dataclass\nclass StandardSAEConfig(SAEConfig):\n    \"\"\"\n    Configuration class for a StandardSAE.\n    \"\"\"\n\n    @override\n    @classmethod\n    def architecture(cls) -&gt; str:\n        return \"standard\"\n</code></pre>"},{"location":"api/#sae_lens.StandardTrainingSAE","title":"<code>StandardTrainingSAE</code>","text":"<p>               Bases: <code>TrainingSAE[StandardTrainingSAEConfig]</code></p> <p>StandardTrainingSAE is a concrete implementation of BaseTrainingSAE using the \"standard\" SAE architecture. It implements:   - initialize_weights: basic weight initialization for encoder/decoder.   - encode: inference encoding (invokes encode_with_hidden_pre).   - decode: a simple linear decoder.   - encode_with_hidden_pre: computes activations and pre-activations.   - calculate_aux_loss: computes a sparsity penalty based on the (optionally scaled) p-norm of feature activations.</p> Source code in <code>sae_lens/saes/standard_sae.py</code> <pre><code>class StandardTrainingSAE(TrainingSAE[StandardTrainingSAEConfig]):\n    \"\"\"\n    StandardTrainingSAE is a concrete implementation of BaseTrainingSAE using the \"standard\" SAE architecture.\n    It implements:\n      - initialize_weights: basic weight initialization for encoder/decoder.\n      - encode: inference encoding (invokes encode_with_hidden_pre).\n      - decode: a simple linear decoder.\n      - encode_with_hidden_pre: computes activations and pre-activations.\n      - calculate_aux_loss: computes a sparsity penalty based on the (optionally scaled) p-norm of feature activations.\n    \"\"\"\n\n    b_enc: nn.Parameter\n\n    def initialize_weights(self) -&gt; None:\n        super().initialize_weights()\n        _init_weights_standard(self)\n\n    @override\n    def get_coefficients(self) -&gt; dict[str, float | TrainCoefficientConfig]:\n        return {\n            \"l1\": TrainCoefficientConfig(\n                value=self.cfg.l1_coefficient,\n                warm_up_steps=self.cfg.l1_warm_up_steps,\n            ),\n        }\n\n    def encode_with_hidden_pre(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; tuple[Float[torch.Tensor, \"... d_sae\"], Float[torch.Tensor, \"... d_sae\"]]:\n        # Process the input (including dtype conversion, hook call, and any activation normalization)\n        sae_in = self.process_sae_in(x)\n        # Compute the pre-activation (and allow for a hook if desired)\n        hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)  # type: ignore\n        # Apply the activation function (and any post-activation hook)\n        feature_acts = self.hook_sae_acts_post(self.activation_fn(hidden_pre))\n        return feature_acts, hidden_pre\n\n    def calculate_aux_loss(\n        self,\n        step_input: TrainStepInput,\n        feature_acts: torch.Tensor,\n        hidden_pre: torch.Tensor,\n        sae_out: torch.Tensor,\n    ) -&gt; dict[str, torch.Tensor]:\n        # The \"standard\" auxiliary loss is a sparsity penalty on the feature activations\n        weighted_feature_acts = feature_acts * self.W_dec.norm(dim=1)\n\n        # Compute the p-norm (set by cfg.lp_norm) over the feature dimension\n        sparsity = weighted_feature_acts.norm(p=self.cfg.lp_norm, dim=-1)\n        l1_loss = (step_input.coefficients[\"l1\"] * sparsity).mean()\n\n        return {\"l1_loss\": l1_loss}\n\n    def log_histograms(self) -&gt; dict[str, NDArray[np.generic]]:\n        \"\"\"Log histograms of the weights and biases.\"\"\"\n        b_e_dist = self.b_enc.detach().float().cpu().numpy()\n        return {\n            **super().log_histograms(),\n            \"weights/b_e\": b_e_dist,\n        }\n</code></pre>"},{"location":"api/#sae_lens.StandardTrainingSAE.log_histograms","title":"<code>log_histograms()</code>","text":"<p>Log histograms of the weights and biases.</p> Source code in <code>sae_lens/saes/standard_sae.py</code> <pre><code>def log_histograms(self) -&gt; dict[str, NDArray[np.generic]]:\n    \"\"\"Log histograms of the weights and biases.\"\"\"\n    b_e_dist = self.b_enc.detach().float().cpu().numpy()\n    return {\n        **super().log_histograms(),\n        \"weights/b_e\": b_e_dist,\n    }\n</code></pre>"},{"location":"api/#sae_lens.StandardTrainingSAEConfig","title":"<code>StandardTrainingSAEConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingSAEConfig</code></p> <p>Configuration class for training a StandardTrainingSAE.</p> Source code in <code>sae_lens/saes/standard_sae.py</code> <pre><code>@dataclass\nclass StandardTrainingSAEConfig(TrainingSAEConfig):\n    \"\"\"\n    Configuration class for training a StandardTrainingSAE.\n    \"\"\"\n\n    l1_coefficient: float = 1.0\n    lp_norm: float = 1.0\n    l1_warm_up_steps: int = 0\n\n    @override\n    @classmethod\n    def architecture(cls) -&gt; str:\n        return \"standard\"\n</code></pre>"},{"location":"api/#sae_lens.TopKSAE","title":"<code>TopKSAE</code>","text":"<p>               Bases: <code>SAE[TopKSAEConfig]</code></p> <p>An inference-only sparse autoencoder using a \"topk\" activation function. It uses linear encoder and decoder layers, applying the TopK activation to the hidden pre-activation in its encode step.</p> Source code in <code>sae_lens/saes/topk_sae.py</code> <pre><code>class TopKSAE(SAE[TopKSAEConfig]):\n    \"\"\"\n    An inference-only sparse autoencoder using a \"topk\" activation function.\n    It uses linear encoder and decoder layers, applying the TopK activation\n    to the hidden pre-activation in its encode step.\n    \"\"\"\n\n    b_enc: nn.Parameter\n\n    def __init__(self, cfg: TopKSAEConfig, use_error_term: bool = False):\n        \"\"\"\n        Args:\n            cfg: SAEConfig defining model size and behavior.\n            use_error_term: Whether to apply the error-term approach in the forward pass.\n        \"\"\"\n        super().__init__(cfg, use_error_term)\n\n    @override\n    def initialize_weights(self) -&gt; None:\n        # Initialize encoder weights and bias.\n        super().initialize_weights()\n        _init_weights_topk(self)\n\n    def encode(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n        \"\"\"\n        Converts input x into feature activations.\n        Uses topk activation under the hood.\n        \"\"\"\n        sae_in = self.process_sae_in(x)\n        hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n        # The BaseSAE already sets self.activation_fn to TopK(...) if config requests topk.\n        return self.hook_sae_acts_post(self.activation_fn(hidden_pre))\n\n    def decode(\n        self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n    ) -&gt; Float[torch.Tensor, \"... d_in\"]:\n        \"\"\"\n        Reconstructs the input from topk feature activations.\n        Applies optional finetuning scaling, hooking to recons, out normalization,\n        and optional head reshaping.\n        \"\"\"\n        sae_out_pre = feature_acts @ self.W_dec + self.b_dec\n        sae_out_pre = self.hook_sae_recons(sae_out_pre)\n        sae_out_pre = self.run_time_activation_norm_fn_out(sae_out_pre)\n        return self.reshape_fn_out(sae_out_pre, self.d_head)\n\n    @override\n    def get_activation_fn(self) -&gt; Callable[[torch.Tensor], torch.Tensor]:\n        return TopK(self.cfg.k)\n\n    @override\n    @torch.no_grad()\n    def fold_W_dec_norm(self) -&gt; None:\n        raise NotImplementedError(\n            \"Folding W_dec_norm is not safe for TopKSAEs, as this may change the topk activations\"\n        )\n</code></pre>"},{"location":"api/#sae_lens.TopKSAE.__init__","title":"<code>__init__(cfg, use_error_term=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>TopKSAEConfig</code> <p>SAEConfig defining model size and behavior.</p> required <code>use_error_term</code> <code>bool</code> <p>Whether to apply the error-term approach in the forward pass.</p> <code>False</code> Source code in <code>sae_lens/saes/topk_sae.py</code> <pre><code>def __init__(self, cfg: TopKSAEConfig, use_error_term: bool = False):\n    \"\"\"\n    Args:\n        cfg: SAEConfig defining model size and behavior.\n        use_error_term: Whether to apply the error-term approach in the forward pass.\n    \"\"\"\n    super().__init__(cfg, use_error_term)\n</code></pre>"},{"location":"api/#sae_lens.TopKSAE.decode","title":"<code>decode(feature_acts)</code>","text":"<p>Reconstructs the input from topk feature activations. Applies optional finetuning scaling, hooking to recons, out normalization, and optional head reshaping.</p> Source code in <code>sae_lens/saes/topk_sae.py</code> <pre><code>def decode(\n    self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n) -&gt; Float[torch.Tensor, \"... d_in\"]:\n    \"\"\"\n    Reconstructs the input from topk feature activations.\n    Applies optional finetuning scaling, hooking to recons, out normalization,\n    and optional head reshaping.\n    \"\"\"\n    sae_out_pre = feature_acts @ self.W_dec + self.b_dec\n    sae_out_pre = self.hook_sae_recons(sae_out_pre)\n    sae_out_pre = self.run_time_activation_norm_fn_out(sae_out_pre)\n    return self.reshape_fn_out(sae_out_pre, self.d_head)\n</code></pre>"},{"location":"api/#sae_lens.TopKSAE.encode","title":"<code>encode(x)</code>","text":"<p>Converts input x into feature activations. Uses topk activation under the hood.</p> Source code in <code>sae_lens/saes/topk_sae.py</code> <pre><code>def encode(\n    self, x: Float[torch.Tensor, \"... d_in\"]\n) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n    \"\"\"\n    Converts input x into feature activations.\n    Uses topk activation under the hood.\n    \"\"\"\n    sae_in = self.process_sae_in(x)\n    hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n    # The BaseSAE already sets self.activation_fn to TopK(...) if config requests topk.\n    return self.hook_sae_acts_post(self.activation_fn(hidden_pre))\n</code></pre>"},{"location":"api/#sae_lens.TopKSAEConfig","title":"<code>TopKSAEConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SAEConfig</code></p> <p>Configuration class for a TopKSAE.</p> Source code in <code>sae_lens/saes/topk_sae.py</code> <pre><code>@dataclass\nclass TopKSAEConfig(SAEConfig):\n    \"\"\"\n    Configuration class for a TopKSAE.\n    \"\"\"\n\n    k: int = 100\n\n    @override\n    @classmethod\n    def architecture(cls) -&gt; str:\n        return \"topk\"\n</code></pre>"},{"location":"api/#sae_lens.TopKTrainingSAE","title":"<code>TopKTrainingSAE</code>","text":"<p>               Bases: <code>TrainingSAE[TopKTrainingSAEConfig]</code></p> <p>TopK variant with training functionality. Calculates a topk-related auxiliary loss, etc.</p> Source code in <code>sae_lens/saes/topk_sae.py</code> <pre><code>class TopKTrainingSAE(TrainingSAE[TopKTrainingSAEConfig]):\n    \"\"\"\n    TopK variant with training functionality. Calculates a topk-related auxiliary loss, etc.\n    \"\"\"\n\n    b_enc: nn.Parameter\n\n    def __init__(self, cfg: TopKTrainingSAEConfig, use_error_term: bool = False):\n        super().__init__(cfg, use_error_term)\n\n    @override\n    def initialize_weights(self) -&gt; None:\n        super().initialize_weights()\n        _init_weights_topk(self)\n\n    def encode_with_hidden_pre(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; tuple[Float[torch.Tensor, \"... d_sae\"], Float[torch.Tensor, \"... d_sae\"]]:\n        \"\"\"\n        Similar to the base training method: calculate pre-activations, then apply TopK.\n        \"\"\"\n        sae_in = self.process_sae_in(x)\n        hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n\n        # Apply the TopK activation function (already set in self.activation_fn if config is \"topk\")\n        feature_acts = self.hook_sae_acts_post(self.activation_fn(hidden_pre))\n        return feature_acts, hidden_pre\n\n    @override\n    def calculate_aux_loss(\n        self,\n        step_input: TrainStepInput,\n        feature_acts: torch.Tensor,\n        hidden_pre: torch.Tensor,\n        sae_out: torch.Tensor,\n    ) -&gt; dict[str, torch.Tensor]:\n        # Calculate the auxiliary loss for dead neurons\n        topk_loss = self.calculate_topk_aux_loss(\n            sae_in=step_input.sae_in,\n            sae_out=sae_out,\n            hidden_pre=hidden_pre,\n            dead_neuron_mask=step_input.dead_neuron_mask,\n        )\n        return {\"auxiliary_reconstruction_loss\": topk_loss}\n\n    @override\n    @torch.no_grad()\n    def fold_W_dec_norm(self) -&gt; None:\n        raise NotImplementedError(\n            \"Folding W_dec_norm is not safe for TopKSAEs, as this may change the topk activations\"\n        )\n\n    @override\n    def get_activation_fn(self) -&gt; Callable[[torch.Tensor], torch.Tensor]:\n        return TopK(self.cfg.k)\n\n    @override\n    def get_coefficients(self) -&gt; dict[str, TrainCoefficientConfig | float]:\n        return {}\n\n    def calculate_topk_aux_loss(\n        self,\n        sae_in: torch.Tensor,\n        sae_out: torch.Tensor,\n        hidden_pre: torch.Tensor,\n        dead_neuron_mask: torch.Tensor | None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Calculate TopK auxiliary loss.\n\n        This auxiliary loss encourages dead neurons to learn useful features by having\n        them reconstruct the residual error from the live neurons. It's a key part of\n        preventing neuron death in TopK SAEs.\n        \"\"\"\n        # Mostly taken from https://github.com/EleutherAI/sae/blob/main/sae/sae.py, except without variance normalization\n        # NOTE: checking the number of dead neurons will force a GPU sync, so performance can likely be improved here\n        if dead_neuron_mask is None or (num_dead := int(dead_neuron_mask.sum())) == 0:\n            return sae_out.new_tensor(0.0)\n        residual = (sae_in - sae_out).detach()\n\n        # Heuristic from Appendix B.1 in the paper\n        k_aux = sae_in.shape[-1] // 2\n\n        # Reduce the scale of the loss if there are a small number of dead latents\n        scale = min(num_dead / k_aux, 1.0)\n        k_aux = min(k_aux, num_dead)\n\n        auxk_acts = _calculate_topk_aux_acts(\n            k_aux=k_aux,\n            hidden_pre=hidden_pre,\n            dead_neuron_mask=dead_neuron_mask,\n        )\n\n        # Encourage the top ~50% of dead latents to predict the residual of the\n        # top k living latents\n        recons = self.decode(auxk_acts)\n        auxk_loss = (recons - residual).pow(2).sum(dim=-1).mean()\n        return self.cfg.aux_loss_coefficient * scale * auxk_loss\n</code></pre>"},{"location":"api/#sae_lens.TopKTrainingSAE.calculate_topk_aux_loss","title":"<code>calculate_topk_aux_loss(sae_in, sae_out, hidden_pre, dead_neuron_mask)</code>","text":"<p>Calculate TopK auxiliary loss.</p> <p>This auxiliary loss encourages dead neurons to learn useful features by having them reconstruct the residual error from the live neurons. It's a key part of preventing neuron death in TopK SAEs.</p> Source code in <code>sae_lens/saes/topk_sae.py</code> <pre><code>def calculate_topk_aux_loss(\n    self,\n    sae_in: torch.Tensor,\n    sae_out: torch.Tensor,\n    hidden_pre: torch.Tensor,\n    dead_neuron_mask: torch.Tensor | None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Calculate TopK auxiliary loss.\n\n    This auxiliary loss encourages dead neurons to learn useful features by having\n    them reconstruct the residual error from the live neurons. It's a key part of\n    preventing neuron death in TopK SAEs.\n    \"\"\"\n    # Mostly taken from https://github.com/EleutherAI/sae/blob/main/sae/sae.py, except without variance normalization\n    # NOTE: checking the number of dead neurons will force a GPU sync, so performance can likely be improved here\n    if dead_neuron_mask is None or (num_dead := int(dead_neuron_mask.sum())) == 0:\n        return sae_out.new_tensor(0.0)\n    residual = (sae_in - sae_out).detach()\n\n    # Heuristic from Appendix B.1 in the paper\n    k_aux = sae_in.shape[-1] // 2\n\n    # Reduce the scale of the loss if there are a small number of dead latents\n    scale = min(num_dead / k_aux, 1.0)\n    k_aux = min(k_aux, num_dead)\n\n    auxk_acts = _calculate_topk_aux_acts(\n        k_aux=k_aux,\n        hidden_pre=hidden_pre,\n        dead_neuron_mask=dead_neuron_mask,\n    )\n\n    # Encourage the top ~50% of dead latents to predict the residual of the\n    # top k living latents\n    recons = self.decode(auxk_acts)\n    auxk_loss = (recons - residual).pow(2).sum(dim=-1).mean()\n    return self.cfg.aux_loss_coefficient * scale * auxk_loss\n</code></pre>"},{"location":"api/#sae_lens.TopKTrainingSAE.encode_with_hidden_pre","title":"<code>encode_with_hidden_pre(x)</code>","text":"<p>Similar to the base training method: calculate pre-activations, then apply TopK.</p> Source code in <code>sae_lens/saes/topk_sae.py</code> <pre><code>def encode_with_hidden_pre(\n    self, x: Float[torch.Tensor, \"... d_in\"]\n) -&gt; tuple[Float[torch.Tensor, \"... d_sae\"], Float[torch.Tensor, \"... d_sae\"]]:\n    \"\"\"\n    Similar to the base training method: calculate pre-activations, then apply TopK.\n    \"\"\"\n    sae_in = self.process_sae_in(x)\n    hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n\n    # Apply the TopK activation function (already set in self.activation_fn if config is \"topk\")\n    feature_acts = self.hook_sae_acts_post(self.activation_fn(hidden_pre))\n    return feature_acts, hidden_pre\n</code></pre>"},{"location":"api/#sae_lens.TopKTrainingSAEConfig","title":"<code>TopKTrainingSAEConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingSAEConfig</code></p> <p>Configuration class for training a TopKTrainingSAE.</p> Source code in <code>sae_lens/saes/topk_sae.py</code> <pre><code>@dataclass\nclass TopKTrainingSAEConfig(TrainingSAEConfig):\n    \"\"\"\n    Configuration class for training a TopKTrainingSAE.\n    \"\"\"\n\n    k: int = 100\n    aux_loss_coefficient: float = 1.0\n\n    @override\n    @classmethod\n    def architecture(cls) -&gt; str:\n        return \"topk\"\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE","title":"<code>TrainingSAE</code>","text":"<p>               Bases: <code>SAE[T_TRAINING_SAE_CONFIG]</code>, <code>ABC</code></p> <p>Abstract base class for training versions of SAEs.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>class TrainingSAE(SAE[T_TRAINING_SAE_CONFIG], ABC):\n    \"\"\"Abstract base class for training versions of SAEs.\"\"\"\n\n    def __init__(self, cfg: T_TRAINING_SAE_CONFIG, use_error_term: bool = False):\n        super().__init__(cfg, use_error_term)\n\n        # Turn off hook_z reshaping for training mode - the activation store\n        # is expected to handle reshaping before passing data to the SAE\n        self.turn_off_forward_pass_hook_z_reshaping()\n        self.mse_loss_fn = mse_loss\n\n    @abstractmethod\n    def get_coefficients(self) -&gt; dict[str, float | TrainCoefficientConfig]: ...\n\n    @abstractmethod\n    def encode_with_hidden_pre(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; tuple[Float[torch.Tensor, \"... d_sae\"], Float[torch.Tensor, \"... d_sae\"]]:\n        \"\"\"Encode with access to pre-activation values for training.\"\"\"\n        ...\n\n    def encode(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n        \"\"\"\n        For inference, just encode without returning hidden_pre.\n        (training_forward_pass calls encode_with_hidden_pre).\n        \"\"\"\n        feature_acts, _ = self.encode_with_hidden_pre(x)\n        return feature_acts\n\n    def decode(\n        self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n    ) -&gt; Float[torch.Tensor, \"... d_in\"]:\n        \"\"\"\n        Decodes feature activations back into input space,\n        applying optional finetuning scale, hooking, out normalization, etc.\n        \"\"\"\n        sae_out_pre = feature_acts @ self.W_dec + self.b_dec\n        sae_out_pre = self.hook_sae_recons(sae_out_pre)\n        sae_out_pre = self.run_time_activation_norm_fn_out(sae_out_pre)\n        return self.reshape_fn_out(sae_out_pre, self.d_head)\n\n    @override\n    def initialize_weights(self):\n        super().initialize_weights()\n        if self.cfg.decoder_init_norm is not None:\n            with torch.no_grad():\n                self.W_dec.data /= self.W_dec.norm(dim=-1, keepdim=True)\n                self.W_dec.data *= self.cfg.decoder_init_norm\n            self.W_enc.data = self.W_dec.data.T.clone().detach().contiguous()\n\n    @abstractmethod\n    def calculate_aux_loss(\n        self,\n        step_input: TrainStepInput,\n        feature_acts: torch.Tensor,\n        hidden_pre: torch.Tensor,\n        sae_out: torch.Tensor,\n    ) -&gt; torch.Tensor | dict[str, torch.Tensor]:\n        \"\"\"Calculate architecture-specific auxiliary loss terms.\"\"\"\n        ...\n\n    def training_forward_pass(\n        self,\n        step_input: TrainStepInput,\n    ) -&gt; TrainStepOutput:\n        \"\"\"Forward pass during training.\"\"\"\n        feature_acts, hidden_pre = self.encode_with_hidden_pre(step_input.sae_in)\n        sae_out = self.decode(feature_acts)\n\n        # Calculate MSE loss\n        per_item_mse_loss = self.mse_loss_fn(sae_out, step_input.sae_in)\n        mse_loss = per_item_mse_loss.sum(dim=-1).mean()\n\n        # Calculate architecture-specific auxiliary losses\n        aux_losses = self.calculate_aux_loss(\n            step_input=step_input,\n            feature_acts=feature_acts,\n            hidden_pre=hidden_pre,\n            sae_out=sae_out,\n        )\n\n        # Total loss is MSE plus all auxiliary losses\n        total_loss = mse_loss\n\n        # Create losses dictionary with mse_loss\n        losses = {\"mse_loss\": mse_loss}\n\n        # Add architecture-specific losses to the dictionary\n        # Make sure aux_losses is a dictionary with string keys and tensor values\n        if isinstance(aux_losses, dict):\n            losses.update(aux_losses)\n\n        # Sum all losses for total_loss\n        if isinstance(aux_losses, dict):\n            for loss_value in aux_losses.values():\n                total_loss = total_loss + loss_value\n        else:\n            # Handle case where aux_losses is a tensor\n            total_loss = total_loss + aux_losses\n\n        return TrainStepOutput(\n            sae_in=step_input.sae_in,\n            sae_out=sae_out,\n            feature_acts=feature_acts,\n            hidden_pre=hidden_pre,\n            loss=total_loss,\n            losses=losses,\n        )\n\n    def save_inference_model(self, path: str | Path) -&gt; tuple[Path, Path]:\n        \"\"\"Save inference version of model weights and config to disk.\"\"\"\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Generate the weights\n        state_dict = self.state_dict()  # Use internal SAE state dict\n        self.process_state_dict_for_saving_inference(state_dict)\n        model_weights_path = path / SAE_WEIGHTS_FILENAME\n        save_file(state_dict, model_weights_path)\n\n        # Save the config\n        config = self.cfg.get_inference_sae_cfg_dict()\n        cfg_path = path / SAE_CFG_FILENAME\n        with open(cfg_path, \"w\") as f:\n            json.dump(config, f)\n\n        return model_weights_path, cfg_path\n\n    def process_state_dict_for_saving_inference(\n        self, state_dict: dict[str, Any]\n    ) -&gt; None:\n        \"\"\"\n        Process the state dict for saving the inference model.\n        This is a hook that can be overridden to change how the state dict is processed for the inference model.\n        \"\"\"\n        return self.process_state_dict_for_saving(state_dict)\n\n    @torch.no_grad()\n    def log_histograms(self) -&gt; dict[str, NDArray[Any]]:\n        \"\"\"Log histograms of the weights and biases.\"\"\"\n        W_dec_norm_dist = self.W_dec.detach().float().norm(dim=1).cpu().numpy()\n        return {\n            \"weights/W_dec_norms\": W_dec_norm_dist,\n        }\n\n    @classmethod\n    def get_sae_class_for_architecture(\n        cls: Type[T_TRAINING_SAE], architecture: str\n    ) -&gt; Type[T_TRAINING_SAE]:\n        \"\"\"Get the SAE class for a given architecture.\"\"\"\n        sae_cls, _ = get_sae_training_class(architecture)\n        if not issubclass(sae_cls, cls):\n            raise ValueError(\n                f\"Loaded SAE is not of type {cls.__name__}. Use {sae_cls.__name__} instead\"\n            )\n        return sae_cls\n\n    # in the future, this can be used to load different config classes for different architectures\n    @classmethod\n    def get_sae_config_class_for_architecture(\n        cls,\n        architecture: str,  # noqa: ARG003\n    ) -&gt; type[TrainingSAEConfig]:\n        return get_sae_training_class(architecture)[1]\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.calculate_aux_loss","title":"<code>calculate_aux_loss(step_input, feature_acts, hidden_pre, sae_out)</code>  <code>abstractmethod</code>","text":"<p>Calculate architecture-specific auxiliary loss terms.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@abstractmethod\ndef calculate_aux_loss(\n    self,\n    step_input: TrainStepInput,\n    feature_acts: torch.Tensor,\n    hidden_pre: torch.Tensor,\n    sae_out: torch.Tensor,\n) -&gt; torch.Tensor | dict[str, torch.Tensor]:\n    \"\"\"Calculate architecture-specific auxiliary loss terms.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.decode","title":"<code>decode(feature_acts)</code>","text":"<p>Decodes feature activations back into input space, applying optional finetuning scale, hooking, out normalization, etc.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def decode(\n    self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n) -&gt; Float[torch.Tensor, \"... d_in\"]:\n    \"\"\"\n    Decodes feature activations back into input space,\n    applying optional finetuning scale, hooking, out normalization, etc.\n    \"\"\"\n    sae_out_pre = feature_acts @ self.W_dec + self.b_dec\n    sae_out_pre = self.hook_sae_recons(sae_out_pre)\n    sae_out_pre = self.run_time_activation_norm_fn_out(sae_out_pre)\n    return self.reshape_fn_out(sae_out_pre, self.d_head)\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.encode","title":"<code>encode(x)</code>","text":"<p>For inference, just encode without returning hidden_pre. (training_forward_pass calls encode_with_hidden_pre).</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def encode(\n    self, x: Float[torch.Tensor, \"... d_in\"]\n) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n    \"\"\"\n    For inference, just encode without returning hidden_pre.\n    (training_forward_pass calls encode_with_hidden_pre).\n    \"\"\"\n    feature_acts, _ = self.encode_with_hidden_pre(x)\n    return feature_acts\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.encode_with_hidden_pre","title":"<code>encode_with_hidden_pre(x)</code>  <code>abstractmethod</code>","text":"<p>Encode with access to pre-activation values for training.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@abstractmethod\ndef encode_with_hidden_pre(\n    self, x: Float[torch.Tensor, \"... d_in\"]\n) -&gt; tuple[Float[torch.Tensor, \"... d_sae\"], Float[torch.Tensor, \"... d_sae\"]]:\n    \"\"\"Encode with access to pre-activation values for training.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.get_sae_class_for_architecture","title":"<code>get_sae_class_for_architecture(architecture)</code>  <code>classmethod</code>","text":"<p>Get the SAE class for a given architecture.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@classmethod\ndef get_sae_class_for_architecture(\n    cls: Type[T_TRAINING_SAE], architecture: str\n) -&gt; Type[T_TRAINING_SAE]:\n    \"\"\"Get the SAE class for a given architecture.\"\"\"\n    sae_cls, _ = get_sae_training_class(architecture)\n    if not issubclass(sae_cls, cls):\n        raise ValueError(\n            f\"Loaded SAE is not of type {cls.__name__}. Use {sae_cls.__name__} instead\"\n        )\n    return sae_cls\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.log_histograms","title":"<code>log_histograms()</code>","text":"<p>Log histograms of the weights and biases.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@torch.no_grad()\ndef log_histograms(self) -&gt; dict[str, NDArray[Any]]:\n    \"\"\"Log histograms of the weights and biases.\"\"\"\n    W_dec_norm_dist = self.W_dec.detach().float().norm(dim=1).cpu().numpy()\n    return {\n        \"weights/W_dec_norms\": W_dec_norm_dist,\n    }\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.process_state_dict_for_saving_inference","title":"<code>process_state_dict_for_saving_inference(state_dict)</code>","text":"<p>Process the state dict for saving the inference model. This is a hook that can be overridden to change how the state dict is processed for the inference model.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def process_state_dict_for_saving_inference(\n    self, state_dict: dict[str, Any]\n) -&gt; None:\n    \"\"\"\n    Process the state dict for saving the inference model.\n    This is a hook that can be overridden to change how the state dict is processed for the inference model.\n    \"\"\"\n    return self.process_state_dict_for_saving(state_dict)\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.save_inference_model","title":"<code>save_inference_model(path)</code>","text":"<p>Save inference version of model weights and config to disk.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def save_inference_model(self, path: str | Path) -&gt; tuple[Path, Path]:\n    \"\"\"Save inference version of model weights and config to disk.\"\"\"\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Generate the weights\n    state_dict = self.state_dict()  # Use internal SAE state dict\n    self.process_state_dict_for_saving_inference(state_dict)\n    model_weights_path = path / SAE_WEIGHTS_FILENAME\n    save_file(state_dict, model_weights_path)\n\n    # Save the config\n    config = self.cfg.get_inference_sae_cfg_dict()\n    cfg_path = path / SAE_CFG_FILENAME\n    with open(cfg_path, \"w\") as f:\n        json.dump(config, f)\n\n    return model_weights_path, cfg_path\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.training_forward_pass","title":"<code>training_forward_pass(step_input)</code>","text":"<p>Forward pass during training.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def training_forward_pass(\n    self,\n    step_input: TrainStepInput,\n) -&gt; TrainStepOutput:\n    \"\"\"Forward pass during training.\"\"\"\n    feature_acts, hidden_pre = self.encode_with_hidden_pre(step_input.sae_in)\n    sae_out = self.decode(feature_acts)\n\n    # Calculate MSE loss\n    per_item_mse_loss = self.mse_loss_fn(sae_out, step_input.sae_in)\n    mse_loss = per_item_mse_loss.sum(dim=-1).mean()\n\n    # Calculate architecture-specific auxiliary losses\n    aux_losses = self.calculate_aux_loss(\n        step_input=step_input,\n        feature_acts=feature_acts,\n        hidden_pre=hidden_pre,\n        sae_out=sae_out,\n    )\n\n    # Total loss is MSE plus all auxiliary losses\n    total_loss = mse_loss\n\n    # Create losses dictionary with mse_loss\n    losses = {\"mse_loss\": mse_loss}\n\n    # Add architecture-specific losses to the dictionary\n    # Make sure aux_losses is a dictionary with string keys and tensor values\n    if isinstance(aux_losses, dict):\n        losses.update(aux_losses)\n\n    # Sum all losses for total_loss\n    if isinstance(aux_losses, dict):\n        for loss_value in aux_losses.values():\n            total_loss = total_loss + loss_value\n    else:\n        # Handle case where aux_losses is a tensor\n        total_loss = total_loss + aux_losses\n\n    return TrainStepOutput(\n        sae_in=step_input.sae_in,\n        sae_out=sae_out,\n        feature_acts=feature_acts,\n        hidden_pre=hidden_pre,\n        loss=total_loss,\n        losses=losses,\n    )\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAEConfig","title":"<code>TrainingSAEConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SAEConfig</code>, <code>ABC</code></p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>@dataclass(kw_only=True)\nclass TrainingSAEConfig(SAEConfig, ABC):\n    # https://transformer-circuits.pub/2024/april-update/index.html#training-saes\n    # 0.1 corresponds to the \"heuristic\" initialization, use None to disable\n    decoder_init_norm: float | None = 0.1\n\n    @classmethod\n    @abstractmethod\n    def architecture(cls) -&gt; str: ...\n\n    @classmethod\n    def from_sae_runner_config(\n        cls: type[T_TRAINING_SAE_CONFIG],\n        cfg: \"LanguageModelSAERunnerConfig[T_TRAINING_SAE_CONFIG]\",\n    ) -&gt; T_TRAINING_SAE_CONFIG:\n        metadata = SAEMetadata(\n            model_name=cfg.model_name,\n            hook_name=cfg.hook_name,\n            hook_head_index=cfg.hook_head_index,\n            context_size=cfg.context_size,\n            prepend_bos=cfg.prepend_bos,\n            seqpos_slice=cfg.seqpos_slice,\n            model_from_pretrained_kwargs=cfg.model_from_pretrained_kwargs or {},\n        )\n        if not isinstance(cfg.sae, cls):\n            raise ValueError(\n                f\"SAE config class {cls} does not match SAE runner config class {type(cfg.sae)}\"\n            )\n        return replace(cfg.sae, metadata=metadata)\n\n    @classmethod\n    def from_dict(\n        cls: type[T_TRAINING_SAE_CONFIG], config_dict: dict[str, Any]\n    ) -&gt; T_TRAINING_SAE_CONFIG:\n        cfg_class = cls\n        if \"architecture\" in config_dict:\n            cfg_class = get_sae_training_class(config_dict[\"architecture\"])[1]\n        if not issubclass(cfg_class, cls):\n            raise ValueError(\n                f\"SAE config class {cls} does not match dict config class {type(cfg_class)}\"\n            )\n        # remove any keys that are not in the dataclass\n        # since we sometimes enhance the config with the whole LM runner config\n        valid_config_dict = filter_valid_dataclass_fields(config_dict, cfg_class)\n        if \"metadata\" in config_dict:\n            valid_config_dict[\"metadata\"] = SAEMetadata(**config_dict[\"metadata\"])\n        return cfg_class(**valid_config_dict)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        return {\n            **super().to_dict(),\n            **asdict(self),\n            \"metadata\": self.metadata.to_dict(),\n            \"architecture\": self.architecture(),\n        }\n\n    def get_inference_config_class(self) -&gt; type[SAEConfig]:\n        \"\"\"\n        Get the architecture for inference.\n        \"\"\"\n        return get_sae_class(self.architecture())[1]\n\n    # this needs to exist so we can initialize the parent sae cfg without the training specific\n    # parameters. Maybe there's a cleaner way to do this\n    def get_inference_sae_cfg_dict(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Creates a dictionary containing attributes corresponding to the fields\n        defined in the base SAEConfig class.\n        \"\"\"\n        base_sae_cfg_class = self.get_inference_config_class()\n        base_config_field_names = {f.name for f in fields(base_sae_cfg_class)}\n        result_dict = {\n            field_name: getattr(self, field_name)\n            for field_name in base_config_field_names\n        }\n        result_dict[\"architecture\"] = base_sae_cfg_class.architecture()\n        result_dict[\"metadata\"] = self.metadata.to_dict()\n        return result_dict\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAEConfig.get_inference_config_class","title":"<code>get_inference_config_class()</code>","text":"<p>Get the architecture for inference.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def get_inference_config_class(self) -&gt; type[SAEConfig]:\n    \"\"\"\n    Get the architecture for inference.\n    \"\"\"\n    return get_sae_class(self.architecture())[1]\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAEConfig.get_inference_sae_cfg_dict","title":"<code>get_inference_sae_cfg_dict()</code>","text":"<p>Creates a dictionary containing attributes corresponding to the fields defined in the base SAEConfig class.</p> Source code in <code>sae_lens/saes/sae.py</code> <pre><code>def get_inference_sae_cfg_dict(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Creates a dictionary containing attributes corresponding to the fields\n    defined in the base SAEConfig class.\n    \"\"\"\n    base_sae_cfg_class = self.get_inference_config_class()\n    base_config_field_names = {f.name for f in fields(base_sae_cfg_class)}\n    result_dict = {\n        field_name: getattr(self, field_name)\n        for field_name in base_config_field_names\n    }\n    result_dict[\"architecture\"] = base_sae_cfg_class.architecture()\n    result_dict[\"metadata\"] = self.metadata.to_dict()\n    return result_dict\n</code></pre>"},{"location":"api/#sae_lens.Transcoder","title":"<code>Transcoder</code>","text":"<p>               Bases: <code>SAE[TranscoderConfig]</code></p> <p>A transcoder maps activations from one hook point to another with potentially different dimensions. It extends the standard SAE but with a decoder that maps to a different output dimension.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>class Transcoder(SAE[TranscoderConfig]):\n    \"\"\"\n    A transcoder maps activations from one hook point to another with\n    potentially different dimensions. It extends the standard SAE but with a\n    decoder that maps to a different output dimension.\n    \"\"\"\n\n    cfg: TranscoderConfig\n    W_enc: nn.Parameter\n    b_enc: nn.Parameter\n    W_dec: nn.Parameter\n    b_dec: nn.Parameter\n\n    def __init__(self, cfg: TranscoderConfig):\n        super().__init__(cfg)\n        self.cfg = cfg\n\n    def initialize_weights(self):\n        \"\"\"Initialize transcoder weights with proper dimensions.\"\"\"\n        # Initialize b_dec with output dimension\n        self.b_dec = nn.Parameter(\n            torch.zeros(self.cfg.d_out, dtype=self.dtype, device=self.device)\n        )\n\n        # Initialize W_dec with shape [d_sae, d_out]\n        w_dec_data = torch.empty(\n            self.cfg.d_sae, self.cfg.d_out, dtype=self.dtype, device=self.device\n        )\n        nn.init.kaiming_uniform_(w_dec_data)\n        self.W_dec = nn.Parameter(w_dec_data)\n\n        # Initialize W_enc with shape [d_in, d_sae]\n        w_enc_data = torch.empty(\n            self.cfg.d_in, self.cfg.d_sae, dtype=self.dtype, device=self.device\n        )\n        nn.init.kaiming_uniform_(w_enc_data)\n        self.W_enc = nn.Parameter(w_enc_data)\n\n        # Initialize b_enc\n        self.b_enc = nn.Parameter(\n            torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)\n        )\n\n    def process_sae_in(self, sae_in: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Process input without applying decoder bias (which has wrong dimension\n        for transcoder).\n\n        Overrides the parent method to skip the bias subtraction since b_dec\n        has dimension d_out which doesn't match the input dimension d_in.\n        \"\"\"\n        # Don't apply b_dec since it has different dimension\n        # Just handle dtype conversion and hooks\n        sae_in = sae_in.to(self.dtype)\n        sae_in = self.hook_sae_input(sae_in)\n        return self.run_time_activation_norm_fn_in(sae_in)\n\n    def encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Encode the input tensor into the feature space.\n        \"\"\"\n        # Preprocess the SAE input (casting type, applying hooks, normalization)\n        sae_in = self.process_sae_in(x)\n        # Compute the pre-activation values\n        hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n        # Apply the activation function (e.g., ReLU)\n        return self.hook_sae_acts_post(self.activation_fn(hidden_pre))\n\n    def decode(self, feature_acts: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Decode to output dimension.\"\"\"\n        # W_dec has shape [d_sae, d_out], feature_acts has shape\n        # [batch, d_sae]\n        sae_out = feature_acts @ self.W_dec + self.b_dec\n        # Apply hooks\n        # Note: We don't apply run_time_activation_norm_fn_out since the\n        # output dimension is different from the input dimension\n        return self.hook_sae_recons(sae_out)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass for transcoder.\n\n        Args:\n            x: Input activations from the input hook point [batch, d_in]\n\n        Returns:\n            sae_out: Reconstructed activations for the output hook point\n            [batch, d_out]\n        \"\"\"\n        feature_acts = self.encode(x)\n        return self.decode(feature_acts)\n\n    def forward_with_activations(\n        self,\n        x: torch.Tensor,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass returning both output and feature activations.\n\n        Args:\n            x: Input activations from the input hook point [batch, d_in]\n\n        Returns:\n            sae_out: Reconstructed activations for the output hook point\n            [batch, d_out]\n            feature_acts: Hidden activations [batch, d_sae]\n        \"\"\"\n        feature_acts = self.encode(x)\n        sae_out = self.decode(feature_acts)\n        return sae_out, feature_acts\n\n    @property\n    def d_out(self) -&gt; int:\n        \"\"\"Output dimension of the transcoder.\"\"\"\n        return self.cfg.d_out\n\n    @classmethod\n    def from_dict(cls, config_dict: dict[str, Any]) -&gt; \"Transcoder\":\n        cfg = TranscoderConfig.from_dict(config_dict)\n        return cls(cfg)\n</code></pre>"},{"location":"api/#sae_lens.Transcoder.d_out","title":"<code>d_out: int</code>  <code>property</code>","text":"<p>Output dimension of the transcoder.</p>"},{"location":"api/#sae_lens.Transcoder.decode","title":"<code>decode(feature_acts)</code>","text":"<p>Decode to output dimension.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>def decode(self, feature_acts: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Decode to output dimension.\"\"\"\n    # W_dec has shape [d_sae, d_out], feature_acts has shape\n    # [batch, d_sae]\n    sae_out = feature_acts @ self.W_dec + self.b_dec\n    # Apply hooks\n    # Note: We don't apply run_time_activation_norm_fn_out since the\n    # output dimension is different from the input dimension\n    return self.hook_sae_recons(sae_out)\n</code></pre>"},{"location":"api/#sae_lens.Transcoder.encode","title":"<code>encode(x)</code>","text":"<p>Encode the input tensor into the feature space.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>def encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Encode the input tensor into the feature space.\n    \"\"\"\n    # Preprocess the SAE input (casting type, applying hooks, normalization)\n    sae_in = self.process_sae_in(x)\n    # Compute the pre-activation values\n    hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n    # Apply the activation function (e.g., ReLU)\n    return self.hook_sae_acts_post(self.activation_fn(hidden_pre))\n</code></pre>"},{"location":"api/#sae_lens.Transcoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass for transcoder.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input activations from the input hook point [batch, d_in]</p> required <p>Returns:</p> Name Type Description <code>sae_out</code> <code>Tensor</code> <p>Reconstructed activations for the output hook point</p> <code>Tensor</code> <p>[batch, d_out]</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for transcoder.\n\n    Args:\n        x: Input activations from the input hook point [batch, d_in]\n\n    Returns:\n        sae_out: Reconstructed activations for the output hook point\n        [batch, d_out]\n    \"\"\"\n    feature_acts = self.encode(x)\n    return self.decode(feature_acts)\n</code></pre>"},{"location":"api/#sae_lens.Transcoder.forward_with_activations","title":"<code>forward_with_activations(x)</code>","text":"<p>Forward pass returning both output and feature activations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input activations from the input hook point [batch, d_in]</p> required <p>Returns:</p> Name Type Description <code>sae_out</code> <code>Tensor</code> <p>Reconstructed activations for the output hook point</p> <code>Tensor</code> <p>[batch, d_out]</p> <code>feature_acts</code> <code>tuple[Tensor, Tensor]</code> <p>Hidden activations [batch, d_sae]</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>def forward_with_activations(\n    self,\n    x: torch.Tensor,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Forward pass returning both output and feature activations.\n\n    Args:\n        x: Input activations from the input hook point [batch, d_in]\n\n    Returns:\n        sae_out: Reconstructed activations for the output hook point\n        [batch, d_out]\n        feature_acts: Hidden activations [batch, d_sae]\n    \"\"\"\n    feature_acts = self.encode(x)\n    sae_out = self.decode(feature_acts)\n    return sae_out, feature_acts\n</code></pre>"},{"location":"api/#sae_lens.Transcoder.initialize_weights","title":"<code>initialize_weights()</code>","text":"<p>Initialize transcoder weights with proper dimensions.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>def initialize_weights(self):\n    \"\"\"Initialize transcoder weights with proper dimensions.\"\"\"\n    # Initialize b_dec with output dimension\n    self.b_dec = nn.Parameter(\n        torch.zeros(self.cfg.d_out, dtype=self.dtype, device=self.device)\n    )\n\n    # Initialize W_dec with shape [d_sae, d_out]\n    w_dec_data = torch.empty(\n        self.cfg.d_sae, self.cfg.d_out, dtype=self.dtype, device=self.device\n    )\n    nn.init.kaiming_uniform_(w_dec_data)\n    self.W_dec = nn.Parameter(w_dec_data)\n\n    # Initialize W_enc with shape [d_in, d_sae]\n    w_enc_data = torch.empty(\n        self.cfg.d_in, self.cfg.d_sae, dtype=self.dtype, device=self.device\n    )\n    nn.init.kaiming_uniform_(w_enc_data)\n    self.W_enc = nn.Parameter(w_enc_data)\n\n    # Initialize b_enc\n    self.b_enc = nn.Parameter(\n        torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)\n    )\n</code></pre>"},{"location":"api/#sae_lens.Transcoder.process_sae_in","title":"<code>process_sae_in(sae_in)</code>","text":"<p>Process input without applying decoder bias (which has wrong dimension for transcoder).</p> <p>Overrides the parent method to skip the bias subtraction since b_dec has dimension d_out which doesn't match the input dimension d_in.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>def process_sae_in(self, sae_in: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Process input without applying decoder bias (which has wrong dimension\n    for transcoder).\n\n    Overrides the parent method to skip the bias subtraction since b_dec\n    has dimension d_out which doesn't match the input dimension d_in.\n    \"\"\"\n    # Don't apply b_dec since it has different dimension\n    # Just handle dtype conversion and hooks\n    sae_in = sae_in.to(self.dtype)\n    sae_in = self.hook_sae_input(sae_in)\n    return self.run_time_activation_norm_fn_in(sae_in)\n</code></pre>"},{"location":"api/#sae_lens.TranscoderConfig","title":"<code>TranscoderConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SAEConfig</code></p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>@dataclass\nclass TranscoderConfig(SAEConfig):\n    # Output dimension fields\n    d_out: int = 768\n    # hook_name_out: str = \"\"\n    # hook_layer_out: int = 0\n    # hook_head_index_out: int | None = None\n\n    @classmethod\n    def architecture(cls) -&gt; str:\n        \"\"\"Return the architecture name for this config.\"\"\"\n        return \"transcoder\"\n\n    @classmethod\n    def from_dict(cls, config_dict: dict[str, Any]) -&gt; \"TranscoderConfig\":\n        \"\"\"Create a TranscoderConfig from a dictionary.\"\"\"\n        # Filter to only include valid dataclass fields\n        filtered_config_dict = filter_valid_dataclass_fields(config_dict, cls)\n\n        # Create the config instance\n        res = cls(**filtered_config_dict)\n\n        # Handle metadata if present\n        if \"metadata\" in config_dict:\n            res.metadata = SAEMetadata(**config_dict[\"metadata\"])\n\n        return res\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert to dictionary, including parent fields.\"\"\"\n        # Get the base dictionary from parent\n        res = super().to_dict()\n\n        # Add transcoder-specific fields\n        res.update({\"d_out\": self.d_out})\n\n        return res\n\n    def __post_init__(self):\n        if self.apply_b_dec_to_input:\n            raise ValueError(\"apply_b_dec_to_input is not supported for transcoders\")\n        return super().__post_init__()\n</code></pre>"},{"location":"api/#sae_lens.TranscoderConfig.architecture","title":"<code>architecture()</code>  <code>classmethod</code>","text":"<p>Return the architecture name for this config.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>@classmethod\ndef architecture(cls) -&gt; str:\n    \"\"\"Return the architecture name for this config.\"\"\"\n    return \"transcoder\"\n</code></pre>"},{"location":"api/#sae_lens.TranscoderConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Create a TranscoderConfig from a dictionary.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict[str, Any]) -&gt; \"TranscoderConfig\":\n    \"\"\"Create a TranscoderConfig from a dictionary.\"\"\"\n    # Filter to only include valid dataclass fields\n    filtered_config_dict = filter_valid_dataclass_fields(config_dict, cls)\n\n    # Create the config instance\n    res = cls(**filtered_config_dict)\n\n    # Handle metadata if present\n    if \"metadata\" in config_dict:\n        res.metadata = SAEMetadata(**config_dict[\"metadata\"])\n\n    return res\n</code></pre>"},{"location":"api/#sae_lens.TranscoderConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary, including parent fields.</p> Source code in <code>sae_lens/saes/transcoder.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary, including parent fields.\"\"\"\n    # Get the base dictionary from parent\n    res = super().to_dict()\n\n    # Add transcoder-specific fields\n    res.update({\"d_out\": self.d_out})\n\n    return res\n</code></pre>"},{"location":"citation/","title":"Citation","text":"<pre><code>@misc{bloom2024saetrainingcodebase,\n   title = {SAELens},\n   author = {Bloom, Joseph and Tigges, Curt and Duong, Anthony and Chanin, David},\n   year = {2024},\n   howpublished = {\\url{https://github.com/jbloomAus/SAELens}},\n}}\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! To get setup for development, follow the instructions below.</p>"},{"location":"contributing/#setup","title":"Setup","text":"<p>Make sure you have poetry installed, clone the repository, and install dependencies with:</p> <pre><code>git clone https://github.com/jbloomAus/SAELens.git # we recommend you make a fork for submitting PR's and clone that!\npoetry lock # can take a while.\npoetry install \nmake check-ci # validate the install\n</code></pre>"},{"location":"contributing/#testing-linting-and-formatting","title":"Testing, Linting, and Formatting","text":"<p>This project uses pytest for testing, pyright for type-checking, and Ruff for formatting and linting.</p> <p>If you add new code, it would be greatly appreciated if you could add tests in the <code>tests</code> directory. You can run the tests with:</p> <pre><code>make test\n</code></pre> <p>Before commiting, make sure you format the code with:</p> <pre><code>make format\n</code></pre> <p>Finally, run all CI checks locally with:</p> <pre><code>make check-ci\n</code></pre> <p>If these pass, you're good to go! Open a pull request with your changes.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>This project uses mkdocs for documentation. You can see the docs locally with:</p> <p><pre><code>make docs-serve\n</code></pre> If you make changes to code which requires updating documentation, it would be greatly appreciated if you could update the docs as well.</p>"},{"location":"migrating/","title":"Migrating to SAELens v6","text":"<p>SAELens v6 is a major update with a number of breaking changes that will require updating your code. The goal of this update was the following:</p> <ul> <li>Simplify SAE configs, removing legacy cruft and making it easy to train SAEs</li> <li>Separate SAE architectures into their own classes with separate options (e.g. TopK doesn't have the same options as JumpReLU)</li> <li>Make it easy to extend SAELens with your own custom SAEs</li> <li>Simplify loading and using SAEs</li> <li>Decouple the LLM-specific training code from SAE training, so it's possible to train an SAE on any type of model (e.g. vision models).</li> </ul> <p>We apologize for any inconvenience this causes! These changes should put SAELens in a good place going forward though, making it possible to easily extend the library as new SAE architectures get released, and make it easier than ever to train and share customized SAEs. Expect lots of exciting updates building on this to come soon!</p>"},{"location":"migrating/#changes-to-saefrom_pretrained","title":"Changes to SAE.from_pretrained()","text":"<p>We expect that most users will use SAELens to load SAEs rather than training SAEs, so we expect this section to be the most important for most users. Previously, <code>SAE.from_pretrained()</code> returned a tuple of SAE, a cfg dict, and a feature sparsity tensor. Now, <code>SAE.from_pretrained()</code> returns just the SAE to be consistent with how <code>from_pretrained()</code> functions in Huggingface Transformers and TransformerLens.</p> <p>Old code of the form <code>sae, cfg_dict, sparsity = SAE.from_pretrained(...)</code> should still continue to work, but will give a deprecation warning.</p> <p>The old functionality also exists via <code>SAE.load_from_pretrained_with_cfg_and_sparsity()</code> - although we expect this will not be needed by most users.</p>"},{"location":"migrating/#sae-training-config-changes","title":"SAE Training config changes","text":"<p>The LLM SAE training runner config now follows a new nested structure, with SAE-specific options specified in a nested <code>sae</code> section, and logging options specified in a nested <code>logger</code> section. This looks like the following:</p> <pre><code>from sae_lens import LanguageModelSAERunnerConfig, StandardTrainingSAEConfig, LoggingConfig\n\ncfg = LanguageModelSAERunnerConfig(\n    # SAE Parameters\n    sae=StandardTrainingSAEConfig(\n        d_in=1024,\n        d_sae=16384,\n        apply_b_dec_to_input=True,\n        normalize_activations=\"expected_average_only_in\",\n        l1_coefficient=5,\n    ),\n    # Data Generating Function (Model + Training Distibuion)\n    model_name=\"tiny-stories-1L-21M\",\n    hook_name=\"blocks.0.hook_mlp_out\",\n    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",\n    is_dataset_tokenized=True,\n    streaming=True,\n    # Training Parameters\n    lr=5e-5,\n    train_batch_size_tokens=4096,\n    context_size=512,\n    n_batches_in_buffer=64,\n    training_tokens=100_000_000,\n    store_batch_size_prompts=16,\n    # WANDB\n    logger=LoggingConfig(\n        log_to_wandb=True,\n        wandb_project=\"sae_lens_tutorial\",\n    ),\n    # Misc\n    device=device,\n    seed=42,\n    n_checkpoints=0,\n    checkpoint_path=\"checkpoints\",\n    dtype=\"float32\",\n)\n</code></pre> <p>There are corresponding config classes for <code>GatedTrainingSAEConfig</code>, <code>JumpReLUTrainingSAEConfig</code>, and <code>TopKTrainingSAEConfig</code> depending on the type of SAE you'd like to train.</p>"},{"location":"migrating/#removed-legacy-training-options","title":"Removed legacy training options","text":"<p>We removed a number of legacy config options from the training config, as we found that having so many options was both confusing and daunting to new users of the library. SAE training best practices have changed rapidly, so we took this opportunity to remove rarely used and legacy options from the config.</p> <p>The removed options include:</p> <ul> <li>expansion_factor (this was redundant given the <code>d_sae</code> option to set SAE width)</li> <li>hook_layer (this was redundant - we already know the layer from <code>hook_name</code>)</li> <li>ghost grads</li> <li>b_dec init options (b_dec is always init to zero now)</li> <li>decoder init options (we always use the heuristic init from Anthropic's April 2024 update)</li> <li>MSE loss normalization</li> <li>decoder normalization (we always scale L1 loss by decoder norm now, this is always the right thing to do)</li> <li>finetuning_tokens / finetuning_method</li> <li>noise_scale</li> <li>activation_fn / activation_fn_kwargs</li> </ul>"},{"location":"migrating/#config-option-renaming-changed-defaults","title":"Config option renaming / changed defaults","text":"<ul> <li>The JumpReLU L0 coefficient is now called <code>l0_coefficient</code></li> <li><code>k</code> is now set explicitly for TopK SAEs rather than being in <code>activation_fn_kwargs</code></li> <li>Default JumpReLU bandwidth has been increased to 0.05 from 0.001 to make JumpReLU SAEs more responsive during training</li> <li>Default JumpReLU starting threshold has been increase to 0.01 from 0.001</li> </ul>"},{"location":"migrating/#other-breaking-changes","title":"Other breaking changes","text":"<p><code>SAETrainingRunner</code> has been renamed to <code>LanguageModelSAETrainingRunner</code>, although the <code>SAETrainingRunner</code> name will still keep working for now. This change was made to allow other types of SAETrainingRunners to be added in the future (e.g. for vision models).</p> <p>Running SAE training from the CLI now requires running the script: <code>python -m sae_lens.llm_sae_training_runner</code> to reflect this change.</p> <p><code>SAE.cfg</code> now only contains config keys that are essential for running the SAE. Everything else, such as <code>prepend_bos</code>, has been moved to <code>SAE.cfg.metadata</code>.</p>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#motivation","title":"Motivation","text":"<ul> <li>Accelerate SAE Research: Support fast experimentation to understand SAEs and improve SAE training so we can train SAEs on larger and more diverse models.</li> <li>Make Research like Play: Support research into language model internals via SAEs. Good tooling can make research tremendously exciting and enjoyable. Balancing modifiability and reliability with ease of understanding / access is the name of the game here.</li> <li>Build an awesome community: Mechanistic Interpretability already has an awesome community but as that community grows, it makes sense that there will be niches. I'd love to build a great community around Sparse Autoencoders.</li> </ul>"},{"location":"roadmap/#goals","title":"Goals","text":""},{"location":"roadmap/#sae-training","title":"SAE Training","text":"<p>SAE Training features will fit into a number of categories including:</p> <ul> <li>Making it easy to train SAEs: Training SAEs is hard for a number of reasons and so making it easy for people to train SAEs with relatively little expertise seems like the main way this codebase will create value. </li> <li>Training SAEs on more models: Supporting training of SAEs on more models, architectures, different activations within those models.</li> <li>Being better at training SAEs: Enabling methodological changes which may improve SAE performance as measured by reconstruction loss, Cross Entropy Loss when using reconstructed activation, L1 loss, L0 and interpretability of features as well as improving speed of training or reducing the compute resources required to train SAEs. </li> <li>Being better at measuring SAE Performance: How do we know when SAEs are doing what we want them to? Improving training metrics should allow better decisions about which methods to use and which hyperparameters choices we make.</li> <li>Training SAE variants: People are already training \u201cTranscoders\u201d which map from one activation to another (such as before / after an MLP layer). These can be easily supported with a few changes. Other variants will come in time and </li> </ul>"},{"location":"roadmap/#analysis-with-saes","title":"Analysis with SAEs","text":"<p>Using SAEs to understand neural network internals is an exciting, but complicated task.</p> <ul> <li>Feature-wise Interpretability: This looks something like \"for each feature, have as much knowledge about it as possible\". Part of this will feature dashboard improvements, or supporting better integrations with Neuronpedia.</li> <li>Mechanistic Interpretability: This comprises the more traditional kinds of Mechanistic Interpretability which TransformerLens supports and should be supported by this codebase. Making it easy to patch, ablate or otherwise intervene on features so as to find circuits will likely speed up lots of researchers.</li> </ul>"},{"location":"roadmap/#other-stuff","title":"Other Stuff","text":"<p>I think there are lots of other types of analysis that could be done in the future with SAE features. I've already explored many different types of statistical tests which can reveal interesting properties of features. There are also things like saliency mapping and attribution techniques which it would be nice to support.</p> <ul> <li>Accessibility and Code Quality: The codebase won\u2019t be used if it doesn\u2019t work and it also won\u2019t get used if it\u2019s too hard to understand, modify or read.  Making the code accessible: This involves tasks like turning the code base into a python package.</li> <li>Knowing how the code is supposed to work: Is the code well-documented? This will require docstrings, tutorials and links to related work and publications. Getting aligned on what the code does is critical to sharing a resource like this. </li> <li>Knowing the code works as intended: All code should be tested.</li> <li>Knowing the code is actually performant: This will ensure code works as intended. However deep learning introduces lots of complexity which makes actually running benchmarks essential to having confidence in the code. </li> </ul>"},{"location":"sae_table/","title":"Pretrained SAEs","text":"<p>This is a list of SAEs importable from the SAELens package. Click on each link for more details.</p> <p>This file contains the contents of <code>sae_lens/pretrained_saes.yaml</code> in Markdown</p>"},{"location":"sae_table/#deepseek-r1-distill-llama-8b-qresearch","title":"deepseek-r1-distill-llama-8b-qresearch","text":"<ul> <li>Huggingface Repo: qresearch/DeepSeek-R1-Distill-Llama-8B-SAE-l19</li> <li>model: deepseek-ai/DeepSeek-R1-Distill-Llama-8B</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.19.hook_resid_postLoad this SAE standard deepseek-r1-distill-llama-8b/19-qresearch-res-65k blocks.19.hook_resid_post 65536 none"},{"location":"sae_table/#gemma-2b-it-res-jb","title":"gemma-2b-it-res-jb","text":"<ul> <li>Huggingface Repo: jbloom/Gemma-2b-IT-Residual-Stream-SAEs</li> <li>model: gemma-2b-it</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.12.hook_resid_postLoad this SAE standard gemma-2b-it/12-res-jb blocks.12.hook_resid_post 16384 none"},{"location":"sae_table/#gemma-2b-res-jb","title":"gemma-2b-res-jb","text":"<ul> <li>Huggingface Repo: jbloom/Gemma-2b-Residual-Stream-SAEs</li> <li>model: gemma-2b</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_resid_postLoad this SAE standard gemma-2b/0-res-jb blocks.0.hook_resid_post 16384 none blocks.6.hook_resid_postLoad this SAE standard gemma-2b/6-res-jb blocks.6.hook_resid_post 16384 none blocks.10.hook_resid_postLoad this SAE standard gemma-2b/10-res-jb blocks.10.hook_resid_post 16384 none blocks.12.hook_resid_postLoad this SAE standard gemma-2b/12-res-jb blocks.12.hook_resid_post 16384 expected_average_only_in blocks.17.hook_resid_postLoad this SAE standard blocks.17.hook_resid_post 16384 none"},{"location":"sae_table/#gemma-scope-27b-pt-res","title":"gemma-scope-27b-pt-res","text":"<ul> <li>Huggingface Repo: google/gemma-scope-27b-pt-res</li> <li>model: gemma-2-27b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_10/width_131k/average_l0_106Load this SAE jumprelu blocks.10.hook_resid_post 131072 none layer_10/width_131k/average_l0_15Load this SAE jumprelu blocks.10.hook_resid_post 131072 none layer_10/width_131k/average_l0_200Load this SAE jumprelu blocks.10.hook_resid_post 131072 none layer_10/width_131k/average_l0_24Load this SAE jumprelu blocks.10.hook_resid_post 131072 none layer_10/width_131k/average_l0_37Load this SAE jumprelu blocks.10.hook_resid_post 131072 none layer_10/width_131k/average_l0_64Load this SAE jumprelu blocks.10.hook_resid_post 131072 none layer_22/width_131k/average_l0_150Load this SAE jumprelu blocks.22.hook_resid_post 131072 none layer_22/width_131k/average_l0_20Load this SAE jumprelu blocks.22.hook_resid_post 131072 none layer_22/width_131k/average_l0_290Load this SAE jumprelu blocks.22.hook_resid_post 131072 none layer_22/width_131k/average_l0_31Load this SAE jumprelu blocks.22.hook_resid_post 131072 none layer_22/width_131k/average_l0_48Load this SAE jumprelu blocks.22.hook_resid_post 131072 none layer_22/width_131k/average_l0_82Load this SAE jumprelu blocks.22.hook_resid_post 131072 none layer_34/width_131k/average_l0_155Load this SAE jumprelu blocks.34.hook_resid_post 131072 none layer_34/width_131k/average_l0_21Load this SAE jumprelu blocks.34.hook_resid_post 131072 none layer_34/width_131k/average_l0_333Load this SAE jumprelu blocks.34.hook_resid_post 131072 none layer_34/width_131k/average_l0_38Load this SAE jumprelu blocks.34.hook_resid_post 131072 none layer_34/width_131k/average_l0_72Load this SAE jumprelu blocks.34.hook_resid_post 131072 none layer_34/width_131k/average_l0_785Load this SAE jumprelu blocks.34.hook_resid_post 131072 none"},{"location":"sae_table/#gemma-scope-27b-pt-res-canonical","title":"gemma-scope-27b-pt-res-canonical","text":"<ul> <li>Huggingface Repo: google/gemma-scope-27b-pt-res</li> <li>model: gemma-2-27b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_10/width_131k/canonicalLoad this SAE jumprelu gemma-2-27b/10-gemmascope-res-131k blocks.10.hook_resid_post 131072 none layer_22/width_131k/canonicalLoad this SAE jumprelu gemma-2-27b/22-gemmascope-res-131k blocks.22.hook_resid_post 131072 none layer_34/width_131k/canonicalLoad this SAE jumprelu gemma-2-27b/34-gemmascope-res-131k blocks.34.hook_resid_post 131072 none"},{"location":"sae_table/#gemma-scope-2b-pt-att","title":"gemma-scope-2b-pt-att","text":"<ul> <li>Huggingface Repo: google/gemma-scope-2b-pt-att</li> <li>model: gemma-2-2b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0/width_16k/average_l0_104Load this SAE jumprelu blocks.0.attn.hook_z 16384 none layer_0/width_16k/average_l0_12Load this SAE jumprelu blocks.0.attn.hook_z 16384 none layer_0/width_16k/average_l0_18Load this SAE jumprelu blocks.0.attn.hook_z 16384 none layer_0/width_16k/average_l0_30Load this SAE jumprelu blocks.0.attn.hook_z 16384 none layer_0/width_16k/average_l0_57Load this SAE jumprelu blocks.0.attn.hook_z 16384 none layer_1/width_16k/average_l0_146Load this SAE jumprelu blocks.1.attn.hook_z 16384 none layer_1/width_16k/average_l0_20Load this SAE jumprelu blocks.1.attn.hook_z 16384 none layer_1/width_16k/average_l0_251Load this SAE jumprelu blocks.1.attn.hook_z 16384 none layer_1/width_16k/average_l0_40Load this SAE jumprelu blocks.1.attn.hook_z 16384 none layer_1/width_16k/average_l0_79Load this SAE jumprelu blocks.1.attn.hook_z 16384 none layer_2/width_16k/average_l0_174Load this SAE jumprelu blocks.2.attn.hook_z 16384 none layer_2/width_16k/average_l0_19Load this SAE jumprelu blocks.2.attn.hook_z 16384 none layer_2/width_16k/average_l0_297Load this SAE jumprelu blocks.2.attn.hook_z 16384 none layer_2/width_16k/average_l0_43Load this SAE jumprelu blocks.2.attn.hook_z 16384 none layer_2/width_16k/average_l0_93Load this SAE jumprelu blocks.2.attn.hook_z 16384 none layer_3/width_16k/average_l0_117Load this SAE jumprelu blocks.3.attn.hook_z 16384 none layer_3/width_16k/average_l0_219Load this SAE jumprelu blocks.3.attn.hook_z 16384 none layer_3/width_16k/average_l0_24Load this SAE jumprelu blocks.3.attn.hook_z 16384 none layer_3/width_16k/average_l0_386Load this SAE jumprelu blocks.3.attn.hook_z 16384 none layer_3/width_16k/average_l0_55Load this SAE jumprelu blocks.3.attn.hook_z 16384 none layer_4/width_16k/average_l0_116Load this SAE jumprelu blocks.4.attn.hook_z 16384 none layer_4/width_16k/average_l0_249Load this SAE jumprelu blocks.4.attn.hook_z 16384 none layer_4/width_16k/average_l0_26Load this SAE jumprelu blocks.4.attn.hook_z 16384 none layer_4/width_16k/average_l0_454Load this SAE jumprelu blocks.4.attn.hook_z 16384 none layer_4/width_16k/average_l0_53Load this SAE jumprelu blocks.4.attn.hook_z 16384 none layer_5/width_16k/average_l0_135Load this SAE jumprelu blocks.5.attn.hook_z 16384 none layer_5/width_16k/average_l0_268Load this SAE jumprelu blocks.5.attn.hook_z 16384 none layer_5/width_16k/average_l0_30Load this SAE jumprelu blocks.5.attn.hook_z 16384 none layer_5/width_16k/average_l0_449Load this SAE jumprelu blocks.5.attn.hook_z 16384 none layer_5/width_16k/average_l0_59Load this SAE jumprelu blocks.5.attn.hook_z 16384 none layer_6/width_16k/average_l0_143Load this SAE jumprelu blocks.6.attn.hook_z 16384 none layer_6/width_16k/average_l0_292Load this SAE jumprelu blocks.6.attn.hook_z 16384 none layer_6/width_16k/average_l0_30Load this SAE jumprelu blocks.6.attn.hook_z 16384 none layer_6/width_16k/average_l0_479Load this SAE jumprelu blocks.6.attn.hook_z 16384 none layer_6/width_16k/average_l0_61Load this SAE jumprelu blocks.6.attn.hook_z 16384 none layer_7/width_16k/average_l0_184Load this SAE jumprelu blocks.7.attn.hook_z 16384 none layer_7/width_16k/average_l0_331Load this SAE jumprelu blocks.7.attn.hook_z 16384 none layer_7/width_16k/average_l0_46Load this SAE jumprelu blocks.7.attn.hook_z 16384 none layer_7/width_16k/average_l0_537Load this SAE jumprelu blocks.7.attn.hook_z 16384 none layer_7/width_16k/average_l0_99Load this SAE jumprelu blocks.7.attn.hook_z 16384 none layer_8/width_16k/average_l0_129Load this SAE jumprelu blocks.8.attn.hook_z 16384 none layer_8/width_16k/average_l0_282Load this SAE jumprelu blocks.8.attn.hook_z 16384 none layer_8/width_16k/average_l0_32Load this SAE jumprelu blocks.8.attn.hook_z 16384 none layer_8/width_16k/average_l0_482Load this SAE jumprelu blocks.8.attn.hook_z 16384 none layer_8/width_16k/average_l0_64Load this SAE jumprelu blocks.8.attn.hook_z 16384 none layer_9/width_16k/average_l0_127Load this SAE jumprelu blocks.9.attn.hook_z 16384 none layer_9/width_16k/average_l0_270Load this SAE jumprelu blocks.9.attn.hook_z 16384 none layer_9/width_16k/average_l0_34Load this SAE jumprelu blocks.9.attn.hook_z 16384 none layer_9/width_16k/average_l0_499Load this SAE jumprelu blocks.9.attn.hook_z 16384 none layer_9/width_16k/average_l0_64Load this SAE jumprelu blocks.9.attn.hook_z 16384 none layer_10/width_16k/average_l0_148Load this SAE jumprelu blocks.10.attn.hook_z 16384 none layer_10/width_16k/average_l0_307Load this SAE jumprelu blocks.10.attn.hook_z 16384 none layer_10/width_16k/average_l0_36Load this SAE jumprelu blocks.10.attn.hook_z 16384 none layer_10/width_16k/average_l0_541Load this SAE jumprelu blocks.10.attn.hook_z 16384 none layer_10/width_16k/average_l0_70Load this SAE jumprelu blocks.10.attn.hook_z 16384 none layer_11/width_16k/average_l0_170Load this SAE jumprelu blocks.11.attn.hook_z 16384 none layer_11/width_16k/average_l0_350Load this SAE jumprelu blocks.11.attn.hook_z 16384 none layer_11/width_16k/average_l0_41Load this SAE jumprelu blocks.11.attn.hook_z 16384 none layer_11/width_16k/average_l0_593Load this SAE jumprelu blocks.11.attn.hook_z 16384 none layer_11/width_16k/average_l0_80Load this SAE jumprelu blocks.11.attn.hook_z 16384 none layer_12/width_16k/average_l0_184Load this SAE jumprelu blocks.12.attn.hook_z 16384 none layer_12/width_16k/average_l0_328Load this SAE jumprelu blocks.12.attn.hook_z 16384 none layer_12/width_16k/average_l0_41Load this SAE jumprelu blocks.12.attn.hook_z 16384 none layer_12/width_16k/average_l0_514Load this SAE jumprelu blocks.12.attn.hook_z 16384 none layer_12/width_16k/average_l0_85Load this SAE jumprelu blocks.12.attn.hook_z 16384 none layer_13/width_16k/average_l0_203Load this SAE jumprelu blocks.13.attn.hook_z 16384 none layer_13/width_16k/average_l0_372Load this SAE jumprelu blocks.13.attn.hook_z 16384 none layer_13/width_16k/average_l0_43Load this SAE jumprelu blocks.13.attn.hook_z 16384 none layer_13/width_16k/average_l0_570Load this SAE jumprelu blocks.13.attn.hook_z 16384 none layer_13/width_16k/average_l0_92Load this SAE jumprelu blocks.13.attn.hook_z 16384 none layer_14/width_16k/average_l0_161Load this SAE jumprelu blocks.14.attn.hook_z 16384 none layer_14/width_16k/average_l0_298Load this SAE jumprelu blocks.14.attn.hook_z 16384 none layer_14/width_16k/average_l0_37Load this SAE jumprelu blocks.14.attn.hook_z 16384 none layer_14/width_16k/average_l0_468Load this SAE jumprelu blocks.14.attn.hook_z 16384 none layer_14/width_16k/average_l0_71Load this SAE jumprelu blocks.14.attn.hook_z 16384 none layer_15/width_16k/average_l0_195Load this SAE jumprelu blocks.15.attn.hook_z 16384 none layer_15/width_16k/average_l0_342Load this SAE jumprelu blocks.15.attn.hook_z 16384 none layer_15/width_16k/average_l0_44Load this SAE jumprelu blocks.15.attn.hook_z 16384 none layer_15/width_16k/average_l0_535Load this SAE jumprelu blocks.15.attn.hook_z 16384 none layer_15/width_16k/average_l0_98Load this SAE jumprelu blocks.15.attn.hook_z 16384 none layer_16/width_16k/average_l0_144Load this SAE jumprelu blocks.16.attn.hook_z 16384 none layer_16/width_16k/average_l0_293Load this SAE jumprelu blocks.16.attn.hook_z 16384 none layer_16/width_16k/average_l0_37Load this SAE jumprelu blocks.16.attn.hook_z 16384 none layer_16/width_16k/average_l0_527Load this SAE jumprelu blocks.16.attn.hook_z 16384 none layer_16/width_16k/average_l0_71Load this SAE jumprelu blocks.16.attn.hook_z 16384 none layer_17/width_16k/average_l0_176Load this SAE jumprelu blocks.17.attn.hook_z 16384 none layer_17/width_16k/average_l0_316Load this SAE jumprelu blocks.17.attn.hook_z 16384 none layer_17/width_16k/average_l0_38Load this SAE jumprelu blocks.17.attn.hook_z 16384 none layer_17/width_16k/average_l0_509Load this SAE jumprelu blocks.17.attn.hook_z 16384 none layer_17/width_16k/average_l0_79Load this SAE jumprelu blocks.17.attn.hook_z 16384 none layer_18/width_16k/average_l0_144Load this SAE jumprelu blocks.18.attn.hook_z 16384 none layer_18/width_16k/average_l0_292Load this SAE jumprelu blocks.18.attn.hook_z 16384 none layer_18/width_16k/average_l0_34Load this SAE jumprelu blocks.18.attn.hook_z 16384 none layer_18/width_16k/average_l0_491Load this SAE jumprelu blocks.18.attn.hook_z 16384 none layer_18/width_16k/average_l0_72Load this SAE jumprelu blocks.18.attn.hook_z 16384 none layer_19/width_16k/average_l0_122Load this SAE jumprelu blocks.19.attn.hook_z 16384 none layer_19/width_16k/average_l0_249Load this SAE jumprelu blocks.19.attn.hook_z 16384 none layer_19/width_16k/average_l0_28Load this SAE jumprelu blocks.19.attn.hook_z 16384 none layer_19/width_16k/average_l0_423Load this SAE jumprelu blocks.19.attn.hook_z 16384 none layer_19/width_16k/average_l0_56Load this SAE jumprelu blocks.19.attn.hook_z 16384 none layer_20/width_16k/average_l0_141Load this SAE jumprelu blocks.20.attn.hook_z 16384 none layer_20/width_16k/average_l0_274Load this SAE jumprelu blocks.20.attn.hook_z 16384 none layer_20/width_16k/average_l0_31Load this SAE jumprelu blocks.20.attn.hook_z 16384 none layer_20/width_16k/average_l0_446Load this SAE jumprelu blocks.20.attn.hook_z 16384 none layer_20/width_16k/average_l0_62Load this SAE jumprelu blocks.20.attn.hook_z 16384 none layer_21/width_16k/average_l0_142Load this SAE jumprelu blocks.21.attn.hook_z 16384 none layer_21/width_16k/average_l0_301Load this SAE jumprelu blocks.21.attn.hook_z 16384 none layer_21/width_16k/average_l0_32Load this SAE jumprelu blocks.21.attn.hook_z 16384 none layer_21/width_16k/average_l0_505Load this SAE jumprelu blocks.21.attn.hook_z 16384 none layer_21/width_16k/average_l0_65Load this SAE jumprelu blocks.21.attn.hook_z 16384 none layer_22/width_16k/average_l0_106Load this SAE jumprelu blocks.22.attn.hook_z 16384 none layer_22/width_16k/average_l0_215Load this SAE jumprelu blocks.22.attn.hook_z 16384 none layer_22/width_16k/average_l0_22Load this SAE jumprelu blocks.22.attn.hook_z 16384 none layer_22/width_16k/average_l0_373Load this SAE jumprelu blocks.22.attn.hook_z 16384 none layer_22/width_16k/average_l0_47Load this SAE jumprelu blocks.22.attn.hook_z 16384 none layer_23/width_16k/average_l0_161Load this SAE jumprelu blocks.23.attn.hook_z 16384 none layer_23/width_16k/average_l0_30Load this SAE jumprelu blocks.23.attn.hook_z 16384 none layer_23/width_16k/average_l0_300Load this SAE jumprelu blocks.23.attn.hook_z 16384 none layer_23/width_16k/average_l0_474Load this SAE jumprelu blocks.23.attn.hook_z 16384 none layer_23/width_16k/average_l0_73Load this SAE jumprelu blocks.23.attn.hook_z 16384 none layer_24/width_16k/average_l0_212Load this SAE jumprelu blocks.24.attn.hook_z 16384 none layer_24/width_16k/average_l0_372Load this SAE jumprelu blocks.24.attn.hook_z 16384 none layer_24/width_16k/average_l0_39Load this SAE jumprelu blocks.24.attn.hook_z 16384 none layer_24/width_16k/average_l0_558Load this SAE jumprelu blocks.24.attn.hook_z 16384 none layer_24/width_16k/average_l0_96Load this SAE jumprelu blocks.24.attn.hook_z 16384 none layer_25/width_16k/average_l0_177Load this SAE jumprelu blocks.25.attn.hook_z 16384 none layer_25/width_16k/average_l0_313Load this SAE jumprelu blocks.25.attn.hook_z 16384 none layer_25/width_16k/average_l0_35Load this SAE jumprelu blocks.25.attn.hook_z 16384 none layer_25/width_16k/average_l0_492Load this SAE jumprelu blocks.25.attn.hook_z 16384 none layer_25/width_16k/average_l0_77Load this SAE jumprelu blocks.25.attn.hook_z 16384 none layer_0/width_65k/average_l0_10Load this SAE jumprelu blocks.0.attn.hook_z 65536 none layer_0/width_65k/average_l0_16Load this SAE jumprelu blocks.0.attn.hook_z 65536 none layer_0/width_65k/average_l0_24Load this SAE jumprelu blocks.0.attn.hook_z 65536 none layer_0/width_65k/average_l0_43Load this SAE jumprelu blocks.0.attn.hook_z 65536 none layer_0/width_65k/average_l0_75Load this SAE jumprelu blocks.0.attn.hook_z 65536 none layer_1/width_65k/average_l0_15Load this SAE jumprelu blocks.1.attn.hook_z 65536 none layer_1/width_65k/average_l0_181Load this SAE jumprelu blocks.1.attn.hook_z 65536 none layer_1/width_65k/average_l0_28Load this SAE jumprelu blocks.1.attn.hook_z 65536 none layer_1/width_65k/average_l0_55Load this SAE jumprelu blocks.1.attn.hook_z 65536 none layer_1/width_65k/average_l0_98Load this SAE jumprelu blocks.1.attn.hook_z 65536 none layer_2/width_65k/average_l0_125Load this SAE jumprelu blocks.2.attn.hook_z 65536 none layer_2/width_65k/average_l0_14Load this SAE jumprelu blocks.2.attn.hook_z 65536 none layer_2/width_65k/average_l0_228Load this SAE jumprelu blocks.2.attn.hook_z 65536 none layer_2/width_65k/average_l0_28Load this SAE jumprelu blocks.2.attn.hook_z 65536 none layer_2/width_65k/average_l0_59Load this SAE jumprelu blocks.2.attn.hook_z 65536 none layer_3/width_65k/average_l0_174Load this SAE jumprelu blocks.3.attn.hook_z 65536 none layer_3/width_65k/average_l0_19Load this SAE jumprelu blocks.3.attn.hook_z 65536 none layer_3/width_65k/average_l0_320Load this SAE jumprelu blocks.3.attn.hook_z 65536 none layer_3/width_65k/average_l0_39Load this SAE jumprelu blocks.3.attn.hook_z 65536 none layer_3/width_65k/average_l0_83Load this SAE jumprelu blocks.3.attn.hook_z 65536 none layer_4/width_65k/average_l0_188Load this SAE jumprelu blocks.4.attn.hook_z 65536 none layer_4/width_65k/average_l0_22Load this SAE jumprelu blocks.4.attn.hook_z 65536 none layer_4/width_65k/average_l0_382Load this SAE jumprelu blocks.4.attn.hook_z 65536 none layer_4/width_65k/average_l0_43Load this SAE jumprelu blocks.4.attn.hook_z 65536 none layer_4/width_65k/average_l0_87Load this SAE jumprelu blocks.4.attn.hook_z 65536 none layer_5/width_65k/average_l0_227Load this SAE jumprelu blocks.5.attn.hook_z 65536 none layer_5/width_65k/average_l0_28Load this SAE jumprelu blocks.5.attn.hook_z 65536 none layer_5/width_65k/average_l0_400Load this SAE jumprelu blocks.5.attn.hook_z 65536 none layer_5/width_65k/average_l0_51Load this SAE jumprelu blocks.5.attn.hook_z 65536 none layer_5/width_65k/average_l0_99Load this SAE jumprelu blocks.5.attn.hook_z 65536 none layer_6/width_65k/average_l0_112Load this SAE jumprelu blocks.6.attn.hook_z 65536 none layer_6/width_65k/average_l0_261Load this SAE jumprelu blocks.6.attn.hook_z 65536 none layer_6/width_65k/average_l0_30Load this SAE jumprelu blocks.6.attn.hook_z 65536 none layer_6/width_65k/average_l0_449Load this SAE jumprelu blocks.6.attn.hook_z 65536 none layer_6/width_65k/average_l0_55Load this SAE jumprelu blocks.6.attn.hook_z 65536 none layer_7/width_65k/average_l0_176Load this SAE jumprelu blocks.7.attn.hook_z 65536 none layer_7/width_65k/average_l0_311Load this SAE jumprelu blocks.7.attn.hook_z 65536 none layer_7/width_65k/average_l0_519Load this SAE jumprelu blocks.7.attn.hook_z 65536 none layer_7/width_65k/average_l0_52Load this SAE jumprelu blocks.7.attn.hook_z 65536 none layer_7/width_65k/average_l0_96Load this SAE jumprelu blocks.7.attn.hook_z 65536 none layer_8/width_65k/average_l0_112Load this SAE jumprelu blocks.8.attn.hook_z 65536 none layer_8/width_65k/average_l0_246Load this SAE jumprelu blocks.8.attn.hook_z 65536 none layer_8/width_65k/average_l0_35Load this SAE jumprelu blocks.8.attn.hook_z 65536 none layer_8/width_65k/average_l0_454Load this SAE jumprelu blocks.8.attn.hook_z 65536 none layer_8/width_65k/average_l0_56Load this SAE jumprelu blocks.8.attn.hook_z 65536 none layer_9/width_65k/average_l0_107Load this SAE jumprelu blocks.9.attn.hook_z 65536 none layer_9/width_65k/average_l0_231Load this SAE jumprelu blocks.9.attn.hook_z 65536 none layer_9/width_65k/average_l0_31Load this SAE jumprelu blocks.9.attn.hook_z 65536 none layer_9/width_65k/average_l0_454Load this SAE jumprelu blocks.9.attn.hook_z 65536 none layer_9/width_65k/average_l0_57Load this SAE jumprelu blocks.9.attn.hook_z 65536 none layer_10/width_65k/average_l0_134Load this SAE jumprelu blocks.10.attn.hook_z 65536 none layer_10/width_65k/average_l0_292Load this SAE jumprelu blocks.10.attn.hook_z 65536 none layer_10/width_65k/average_l0_35Load this SAE jumprelu blocks.10.attn.hook_z 65536 none layer_10/width_65k/average_l0_521Load this SAE jumprelu blocks.10.attn.hook_z 65536 none layer_10/width_65k/average_l0_67Load this SAE jumprelu blocks.10.attn.hook_z 65536 none layer_11/width_65k/average_l0_154Load this SAE jumprelu blocks.11.attn.hook_z 65536 none layer_11/width_65k/average_l0_330Load this SAE jumprelu blocks.11.attn.hook_z 65536 none layer_11/width_65k/average_l0_41Load this SAE jumprelu blocks.11.attn.hook_z 65536 none layer_11/width_65k/average_l0_576Load this SAE jumprelu blocks.11.attn.hook_z 65536 none layer_11/width_65k/average_l0_75Load this SAE jumprelu blocks.11.attn.hook_z 65536 none layer_12/width_65k/average_l0_172Load this SAE jumprelu blocks.12.attn.hook_z 65536 none layer_12/width_65k/average_l0_320Load this SAE jumprelu blocks.12.attn.hook_z 65536 none layer_12/width_65k/average_l0_39Load this SAE jumprelu blocks.12.attn.hook_z 65536 none layer_12/width_65k/average_l0_503Load this SAE jumprelu blocks.12.attn.hook_z 65536 none layer_12/width_65k/average_l0_79Load this SAE jumprelu blocks.12.attn.hook_z 65536 none layer_13/width_65k/average_l0_191Load this SAE jumprelu blocks.13.attn.hook_z 65536 none layer_13/width_65k/average_l0_363Load this SAE jumprelu blocks.13.attn.hook_z 65536 none layer_13/width_65k/average_l0_41Load this SAE jumprelu blocks.13.attn.hook_z 65536 none layer_13/width_65k/average_l0_556Load this SAE jumprelu blocks.13.attn.hook_z 65536 none layer_13/width_65k/average_l0_87Load this SAE jumprelu blocks.13.attn.hook_z 65536 none layer_14/width_65k/average_l0_138Load this SAE jumprelu blocks.14.attn.hook_z 65536 none layer_14/width_65k/average_l0_283Load this SAE jumprelu blocks.14.attn.hook_z 65536 none layer_14/width_65k/average_l0_37Load this SAE jumprelu blocks.14.attn.hook_z 65536 none layer_14/width_65k/average_l0_453Load this SAE jumprelu blocks.14.attn.hook_z 65536 none layer_14/width_65k/average_l0_66Load this SAE jumprelu blocks.14.attn.hook_z 65536 none layer_15/width_65k/average_l0_182Load this SAE jumprelu blocks.15.attn.hook_z 65536 none layer_15/width_65k/average_l0_327Load this SAE jumprelu blocks.15.attn.hook_z 65536 none layer_15/width_65k/average_l0_42Load this SAE jumprelu blocks.15.attn.hook_z 65536 none layer_15/width_65k/average_l0_517Load this SAE jumprelu blocks.15.attn.hook_z 65536 none layer_15/width_65k/average_l0_90Load this SAE jumprelu blocks.15.attn.hook_z 65536 none layer_16/width_65k/average_l0_129Load this SAE jumprelu blocks.16.attn.hook_z 65536 none layer_16/width_65k/average_l0_260Load this SAE jumprelu blocks.16.attn.hook_z 65536 none layer_16/width_65k/average_l0_35Load this SAE jumprelu blocks.16.attn.hook_z 65536 none layer_16/width_65k/average_l0_502Load this SAE jumprelu blocks.16.attn.hook_z 65536 none layer_16/width_65k/average_l0_64Load this SAE jumprelu blocks.16.attn.hook_z 65536 none layer_17/width_65k/average_l0_157Load this SAE jumprelu blocks.17.attn.hook_z 65536 none layer_17/width_65k/average_l0_293Load this SAE jumprelu blocks.17.attn.hook_z 65536 none layer_17/width_65k/average_l0_35Load this SAE jumprelu blocks.17.attn.hook_z 65536 none layer_17/width_65k/average_l0_489Load this SAE jumprelu blocks.17.attn.hook_z 65536 none layer_17/width_65k/average_l0_70Load this SAE jumprelu blocks.17.attn.hook_z 65536 none layer_18/width_65k/average_l0_123Load this SAE jumprelu blocks.18.attn.hook_z 65536 none layer_18/width_65k/average_l0_255Load this SAE jumprelu blocks.18.attn.hook_z 65536 none layer_18/width_65k/average_l0_29Load this SAE jumprelu blocks.18.attn.hook_z 65536 none layer_18/width_65k/average_l0_466Load this SAE jumprelu blocks.18.attn.hook_z 65536 none layer_18/width_65k/average_l0_58Load this SAE jumprelu blocks.18.attn.hook_z 65536 none layer_19/width_65k/average_l0_106Load this SAE jumprelu blocks.19.attn.hook_z 65536 none layer_19/width_65k/average_l0_220Load this SAE jumprelu blocks.19.attn.hook_z 65536 none layer_19/width_65k/average_l0_26Load this SAE jumprelu blocks.19.attn.hook_z 65536 none layer_19/width_65k/average_l0_411Load this SAE jumprelu blocks.19.attn.hook_z 65536 none layer_19/width_65k/average_l0_49Load this SAE jumprelu blocks.19.attn.hook_z 65536 none layer_20/width_65k/average_l0_102Load this SAE jumprelu blocks.20.attn.hook_z 65536 none layer_20/width_65k/average_l0_242Load this SAE jumprelu blocks.20.attn.hook_z 65536 none layer_20/width_65k/average_l0_26Load this SAE jumprelu blocks.20.attn.hook_z 65536 none layer_20/width_65k/average_l0_419Load this SAE jumprelu blocks.20.attn.hook_z 65536 none layer_20/width_65k/average_l0_49Load this SAE jumprelu blocks.20.attn.hook_z 65536 none layer_21/width_65k/average_l0_118Load this SAE jumprelu blocks.21.attn.hook_z 65536 none layer_21/width_65k/average_l0_266Load this SAE jumprelu blocks.21.attn.hook_z 65536 none layer_21/width_65k/average_l0_29Load this SAE jumprelu blocks.21.attn.hook_z 65536 none layer_21/width_65k/average_l0_474Load this SAE jumprelu blocks.21.attn.hook_z 65536 none layer_21/width_65k/average_l0_56Load this SAE jumprelu blocks.21.attn.hook_z 65536 none layer_22/width_65k/average_l0_112Load this SAE jumprelu blocks.22.attn.hook_z 65536 none layer_22/width_65k/average_l0_196Load this SAE jumprelu blocks.22.attn.hook_z 65536 none layer_22/width_65k/average_l0_20Load this SAE jumprelu blocks.22.attn.hook_z 65536 none layer_22/width_65k/average_l0_361Load this SAE jumprelu blocks.22.attn.hook_z 65536 none layer_22/width_65k/average_l0_37Load this SAE jumprelu blocks.22.attn.hook_z 65536 none layer_23/width_65k/average_l0_140Load this SAE jumprelu blocks.23.attn.hook_z 65536 none layer_23/width_65k/average_l0_27Load this SAE jumprelu blocks.23.attn.hook_z 65536 none layer_23/width_65k/average_l0_276Load this SAE jumprelu blocks.23.attn.hook_z 65536 none layer_23/width_65k/average_l0_457Load this SAE jumprelu blocks.23.attn.hook_z 65536 none layer_23/width_65k/average_l0_56Load this SAE jumprelu blocks.23.attn.hook_z 65536 none layer_24/width_65k/average_l0_186Load this SAE jumprelu blocks.24.attn.hook_z 65536 none layer_24/width_65k/average_l0_32Load this SAE jumprelu blocks.24.attn.hook_z 65536 none layer_24/width_65k/average_l0_347Load this SAE jumprelu blocks.24.attn.hook_z 65536 none layer_24/width_65k/average_l0_537Load this SAE jumprelu blocks.24.attn.hook_z 65536 none layer_24/width_65k/average_l0_77Load this SAE jumprelu blocks.24.attn.hook_z 65536 none layer_25/width_65k/average_l0_153Load this SAE jumprelu blocks.25.attn.hook_z 65536 none layer_25/width_65k/average_l0_290Load this SAE jumprelu blocks.25.attn.hook_z 65536 none layer_25/width_65k/average_l0_30Load this SAE jumprelu blocks.25.attn.hook_z 65536 none layer_25/width_65k/average_l0_465Load this SAE jumprelu blocks.25.attn.hook_z 65536 none layer_25/width_65k/average_l0_63Load this SAE jumprelu blocks.25.attn.hook_z 65536 none"},{"location":"sae_table/#gemma-scope-2b-pt-att-canonical","title":"gemma-scope-2b-pt-att-canonical","text":"<ul> <li>Huggingface Repo: google/gemma-scope-2b-pt-att</li> <li>model: gemma-2-2b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/0-gemmascope-att-16k blocks.0.attn.hook_z 16384 none layer_1/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/1-gemmascope-att-16k blocks.1.attn.hook_z 16384 none layer_2/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/2-gemmascope-att-16k blocks.2.attn.hook_z 16384 none layer_3/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/3-gemmascope-att-16k blocks.3.attn.hook_z 16384 none layer_4/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/4-gemmascope-att-16k blocks.4.attn.hook_z 16384 none layer_5/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/5-gemmascope-att-16k blocks.5.attn.hook_z 16384 none layer_6/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/6-gemmascope-att-16k blocks.6.attn.hook_z 16384 none layer_7/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/7-gemmascope-att-16k blocks.7.attn.hook_z 16384 none layer_8/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/8-gemmascope-att-16k blocks.8.attn.hook_z 16384 none layer_9/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/9-gemmascope-att-16k blocks.9.attn.hook_z 16384 none layer_10/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/10-gemmascope-att-16k blocks.10.attn.hook_z 16384 none layer_11/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/11-gemmascope-att-16k blocks.11.attn.hook_z 16384 none layer_12/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/12-gemmascope-att-16k blocks.12.attn.hook_z 16384 none layer_13/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/13-gemmascope-att-16k blocks.13.attn.hook_z 16384 none layer_14/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/14-gemmascope-att-16k blocks.14.attn.hook_z 16384 none layer_15/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/15-gemmascope-att-16k blocks.15.attn.hook_z 16384 none layer_16/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/16-gemmascope-att-16k blocks.16.attn.hook_z 16384 none layer_17/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/17-gemmascope-att-16k blocks.17.attn.hook_z 16384 none layer_18/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/18-gemmascope-att-16k blocks.18.attn.hook_z 16384 none layer_19/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/19-gemmascope-att-16k blocks.19.attn.hook_z 16384 none layer_20/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/20-gemmascope-att-16k blocks.20.attn.hook_z 16384 none layer_21/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/21-gemmascope-att-16k blocks.21.attn.hook_z 16384 none layer_22/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/22-gemmascope-att-16k blocks.22.attn.hook_z 16384 none layer_23/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/23-gemmascope-att-16k blocks.23.attn.hook_z 16384 none layer_24/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/24-gemmascope-att-16k blocks.24.attn.hook_z 16384 none layer_25/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/25-gemmascope-att-16k blocks.25.attn.hook_z 16384 none layer_0/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/0-gemmascope-att-65k blocks.0.attn.hook_z 65536 none layer_1/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/1-gemmascope-att-65k blocks.1.attn.hook_z 65536 none layer_2/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/2-gemmascope-att-65k blocks.2.attn.hook_z 65536 none layer_3/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/3-gemmascope-att-65k blocks.3.attn.hook_z 65536 none layer_4/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/4-gemmascope-att-65k blocks.4.attn.hook_z 65536 none layer_5/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/5-gemmascope-att-65k blocks.5.attn.hook_z 65536 none layer_6/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/6-gemmascope-att-65k blocks.6.attn.hook_z 65536 none layer_7/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/7-gemmascope-att-65k blocks.7.attn.hook_z 65536 none layer_8/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/8-gemmascope-att-65k blocks.8.attn.hook_z 65536 none layer_9/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/9-gemmascope-att-65k blocks.9.attn.hook_z 65536 none layer_10/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/10-gemmascope-att-65k blocks.10.attn.hook_z 65536 none layer_11/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/11-gemmascope-att-65k blocks.11.attn.hook_z 65536 none layer_12/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/12-gemmascope-att-65k blocks.12.attn.hook_z 65536 none layer_13/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/13-gemmascope-att-65k blocks.13.attn.hook_z 65536 none layer_14/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/14-gemmascope-att-65k blocks.14.attn.hook_z 65536 none layer_15/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/15-gemmascope-att-65k blocks.15.attn.hook_z 65536 none layer_16/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/16-gemmascope-att-65k blocks.16.attn.hook_z 65536 none layer_17/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/17-gemmascope-att-65k blocks.17.attn.hook_z 65536 none layer_18/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/18-gemmascope-att-65k blocks.18.attn.hook_z 65536 none layer_19/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/19-gemmascope-att-65k blocks.19.attn.hook_z 65536 none layer_20/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/20-gemmascope-att-65k blocks.20.attn.hook_z 65536 none layer_21/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/21-gemmascope-att-65k blocks.21.attn.hook_z 65536 none layer_22/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/22-gemmascope-att-65k blocks.22.attn.hook_z 65536 none layer_23/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/23-gemmascope-att-65k blocks.23.attn.hook_z 65536 none layer_24/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/24-gemmascope-att-65k blocks.24.attn.hook_z 65536 none layer_25/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/25-gemmascope-att-65k blocks.25.attn.hook_z 65536 none"},{"location":"sae_table/#gemma-scope-2b-pt-mlp","title":"gemma-scope-2b-pt-mlp","text":"<ul> <li>Huggingface Repo: google/gemma-scope-2b-pt-mlp</li> <li>model: gemma-2-2b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0/width_16k/average_l0_119Load this SAE jumprelu blocks.0.hook_mlp_out 16384 none layer_0/width_16k/average_l0_16Load this SAE jumprelu blocks.0.hook_mlp_out 16384 none layer_0/width_16k/average_l0_30Load this SAE jumprelu blocks.0.hook_mlp_out 16384 none layer_0/width_16k/average_l0_60Load this SAE jumprelu blocks.0.hook_mlp_out 16384 none layer_0/width_16k/average_l0_9Load this SAE jumprelu blocks.0.hook_mlp_out 16384 none layer_1/width_16k/average_l0_105Load this SAE jumprelu blocks.1.hook_mlp_out 16384 none layer_1/width_16k/average_l0_12Load this SAE jumprelu blocks.1.hook_mlp_out 16384 none layer_1/width_16k/average_l0_239Load this SAE jumprelu blocks.1.hook_mlp_out 16384 none layer_1/width_16k/average_l0_24Load this SAE jumprelu blocks.1.hook_mlp_out 16384 none layer_1/width_16k/average_l0_50Load this SAE jumprelu blocks.1.hook_mlp_out 16384 none layer_2/width_16k/average_l0_19Load this SAE jumprelu blocks.2.hook_mlp_out 16384 none layer_2/width_16k/average_l0_213Load this SAE jumprelu blocks.2.hook_mlp_out 16384 none layer_2/width_16k/average_l0_41Load this SAE jumprelu blocks.2.hook_mlp_out 16384 none layer_2/width_16k/average_l0_434Load this SAE jumprelu blocks.2.hook_mlp_out 16384 none layer_2/width_16k/average_l0_95Load this SAE jumprelu blocks.2.hook_mlp_out 16384 none layer_3/width_16k/average_l0_195Load this SAE jumprelu blocks.3.hook_mlp_out 16384 none layer_3/width_16k/average_l0_21Load this SAE jumprelu blocks.3.hook_mlp_out 16384 none layer_3/width_16k/average_l0_377Load this SAE jumprelu blocks.3.hook_mlp_out 16384 none layer_3/width_16k/average_l0_44Load this SAE jumprelu blocks.3.hook_mlp_out 16384 none layer_3/width_16k/average_l0_95Load this SAE jumprelu blocks.3.hook_mlp_out 16384 none layer_4/width_16k/average_l0_18Load this SAE jumprelu blocks.4.hook_mlp_out 16384 none layer_4/width_16k/average_l0_198Load this SAE jumprelu blocks.4.hook_mlp_out 16384 none layer_4/width_16k/average_l0_38Load this SAE jumprelu blocks.4.hook_mlp_out 16384 none layer_4/width_16k/average_l0_433Load this SAE jumprelu blocks.4.hook_mlp_out 16384 none layer_4/width_16k/average_l0_85Load this SAE jumprelu blocks.4.hook_mlp_out 16384 none layer_5/width_16k/average_l0_114Load this SAE jumprelu blocks.5.hook_mlp_out 16384 none layer_5/width_16k/average_l0_23Load this SAE jumprelu blocks.5.hook_mlp_out 16384 none layer_5/width_16k/average_l0_269Load this SAE jumprelu blocks.5.hook_mlp_out 16384 none layer_5/width_16k/average_l0_48Load this SAE jumprelu blocks.5.hook_mlp_out 16384 none layer_5/width_16k/average_l0_575Load this SAE jumprelu blocks.5.hook_mlp_out 16384 none layer_6/width_16k/average_l0_133Load this SAE jumprelu blocks.6.hook_mlp_out 16384 none layer_6/width_16k/average_l0_25Load this SAE jumprelu blocks.6.hook_mlp_out 16384 none layer_6/width_16k/average_l0_328Load this SAE jumprelu blocks.6.hook_mlp_out 16384 none layer_6/width_16k/average_l0_55Load this SAE jumprelu blocks.6.hook_mlp_out 16384 none layer_6/width_16k/average_l0_699Load this SAE jumprelu blocks.6.hook_mlp_out 16384 none layer_7/width_16k/average_l0_146Load this SAE jumprelu blocks.7.hook_mlp_out 16384 none layer_7/width_16k/average_l0_28Load this SAE jumprelu blocks.7.hook_mlp_out 16384 none layer_7/width_16k/average_l0_355Load this SAE jumprelu blocks.7.hook_mlp_out 16384 none layer_7/width_16k/average_l0_60Load this SAE jumprelu blocks.7.hook_mlp_out 16384 none layer_7/width_16k/average_l0_731Load this SAE jumprelu blocks.7.hook_mlp_out 16384 none layer_8/width_16k/average_l0_136Load this SAE jumprelu blocks.8.hook_mlp_out 16384 none layer_8/width_16k/average_l0_27Load this SAE jumprelu blocks.8.hook_mlp_out 16384 none layer_8/width_16k/average_l0_351Load this SAE jumprelu blocks.8.hook_mlp_out 16384 none layer_8/width_16k/average_l0_56Load this SAE jumprelu blocks.8.hook_mlp_out 16384 none layer_8/width_16k/average_l0_739Load this SAE jumprelu blocks.8.hook_mlp_out 16384 none layer_9/width_16k/average_l0_216Load this SAE jumprelu blocks.9.hook_mlp_out 16384 none layer_9/width_16k/average_l0_38Load this SAE jumprelu blocks.9.hook_mlp_out 16384 none layer_9/width_16k/average_l0_482Load this SAE jumprelu blocks.9.hook_mlp_out 16384 none layer_9/width_16k/average_l0_861Load this SAE jumprelu blocks.9.hook_mlp_out 16384 none layer_9/width_16k/average_l0_88Load this SAE jumprelu blocks.9.hook_mlp_out 16384 none layer_10/width_16k/average_l0_110Load this SAE jumprelu blocks.10.hook_mlp_out 16384 none layer_10/width_16k/average_l0_266Load this SAE jumprelu blocks.10.hook_mlp_out 16384 none layer_10/width_16k/average_l0_45Load this SAE jumprelu blocks.10.hook_mlp_out 16384 none layer_10/width_16k/average_l0_568Load this SAE jumprelu blocks.10.hook_mlp_out 16384 none layer_10/width_16k/average_l0_908Load this SAE jumprelu blocks.10.hook_mlp_out 16384 none layer_11/width_16k/average_l0_234Load this SAE jumprelu blocks.11.hook_mlp_out 16384 none layer_11/width_16k/average_l0_42Load this SAE jumprelu blocks.11.hook_mlp_out 16384 none layer_11/width_16k/average_l0_499Load this SAE jumprelu blocks.11.hook_mlp_out 16384 none layer_11/width_16k/average_l0_847Load this SAE jumprelu blocks.11.hook_mlp_out 16384 none layer_11/width_16k/average_l0_98Load this SAE jumprelu blocks.11.hook_mlp_out 16384 none layer_12/width_16k/average_l0_108Load this SAE jumprelu blocks.12.hook_mlp_out 16384 none layer_12/width_16k/average_l0_262Load this SAE jumprelu blocks.12.hook_mlp_out 16384 none layer_12/width_16k/average_l0_44Load this SAE jumprelu blocks.12.hook_mlp_out 16384 none layer_12/width_16k/average_l0_548Load this SAE jumprelu blocks.12.hook_mlp_out 16384 none layer_12/width_16k/average_l0_879Load this SAE jumprelu blocks.12.hook_mlp_out 16384 none layer_13/width_16k/average_l0_112Load this SAE jumprelu blocks.13.hook_mlp_out 16384 none layer_13/width_16k/average_l0_267Load this SAE jumprelu blocks.13.hook_mlp_out 16384 none layer_13/width_16k/average_l0_47Load this SAE jumprelu blocks.13.hook_mlp_out 16384 none layer_13/width_16k/average_l0_553Load this SAE jumprelu blocks.13.hook_mlp_out 16384 none layer_13/width_16k/average_l0_892Load this SAE jumprelu blocks.13.hook_mlp_out 16384 none layer_14/width_16k/average_l0_246Load this SAE jumprelu blocks.14.hook_mlp_out 16384 none layer_14/width_16k/average_l0_41Load this SAE jumprelu blocks.14.hook_mlp_out 16384 none layer_14/width_16k/average_l0_536Load this SAE jumprelu blocks.14.hook_mlp_out 16384 none layer_14/width_16k/average_l0_894Load this SAE jumprelu blocks.14.hook_mlp_out 16384 none layer_14/width_16k/average_l0_97Load this SAE jumprelu blocks.14.hook_mlp_out 16384 none layer_15/width_16k/average_l0_207Load this SAE jumprelu blocks.15.hook_mlp_out 16384 none layer_15/width_16k/average_l0_35Load this SAE jumprelu blocks.15.hook_mlp_out 16384 none layer_15/width_16k/average_l0_492Load this SAE jumprelu blocks.15.hook_mlp_out 16384 none layer_15/width_16k/average_l0_80Load this SAE jumprelu blocks.15.hook_mlp_out 16384 none layer_15/width_16k/average_l0_879Load this SAE jumprelu blocks.15.hook_mlp_out 16384 none layer_16/width_16k/average_l0_185Load this SAE jumprelu blocks.16.hook_mlp_out 16384 none layer_16/width_16k/average_l0_33Load this SAE jumprelu blocks.16.hook_mlp_out 16384 none layer_16/width_16k/average_l0_452Load this SAE jumprelu blocks.16.hook_mlp_out 16384 none layer_16/width_16k/average_l0_72Load this SAE jumprelu blocks.16.hook_mlp_out 16384 none layer_16/width_16k/average_l0_847Load this SAE jumprelu blocks.16.hook_mlp_out 16384 none layer_17/width_16k/average_l0_179Load this SAE jumprelu blocks.17.hook_mlp_out 16384 none layer_17/width_16k/average_l0_31Load this SAE jumprelu blocks.17.hook_mlp_out 16384 none layer_17/width_16k/average_l0_453Load this SAE jumprelu blocks.17.hook_mlp_out 16384 none layer_17/width_16k/average_l0_68Load this SAE jumprelu blocks.17.hook_mlp_out 16384 none layer_17/width_16k/average_l0_853Load this SAE jumprelu blocks.17.hook_mlp_out 16384 none layer_18/width_16k/average_l0_106Load this SAE jumprelu blocks.18.hook_mlp_out 16384 none layer_18/width_16k/average_l0_24Load this SAE jumprelu blocks.18.hook_mlp_out 16384 none layer_18/width_16k/average_l0_292Load this SAE jumprelu blocks.18.hook_mlp_out 16384 none layer_18/width_16k/average_l0_47Load this SAE jumprelu blocks.18.hook_mlp_out 16384 none layer_18/width_16k/average_l0_672Load this SAE jumprelu blocks.18.hook_mlp_out 16384 none layer_19/width_16k/average_l0_109Load this SAE jumprelu blocks.19.hook_mlp_out 16384 none layer_19/width_16k/average_l0_25Load this SAE jumprelu blocks.19.hook_mlp_out 16384 none layer_19/width_16k/average_l0_295Load this SAE jumprelu blocks.19.hook_mlp_out 16384 none layer_19/width_16k/average_l0_50Load this SAE jumprelu blocks.19.hook_mlp_out 16384 none layer_19/width_16k/average_l0_673Load this SAE jumprelu blocks.19.hook_mlp_out 16384 none layer_20/width_16k/average_l0_109Load this SAE jumprelu blocks.20.hook_mlp_out 16384 none layer_20/width_16k/average_l0_24Load this SAE jumprelu blocks.20.hook_mlp_out 16384 none layer_20/width_16k/average_l0_289Load this SAE jumprelu blocks.20.hook_mlp_out 16384 none layer_20/width_16k/average_l0_49Load this SAE jumprelu blocks.20.hook_mlp_out 16384 none layer_20/width_16k/average_l0_658Load this SAE jumprelu blocks.20.hook_mlp_out 16384 none layer_21/width_16k/average_l0_113Load this SAE jumprelu blocks.21.hook_mlp_out 16384 none layer_21/width_16k/average_l0_23Load this SAE jumprelu blocks.21.hook_mlp_out 16384 none layer_21/width_16k/average_l0_279Load this SAE jumprelu blocks.21.hook_mlp_out 16384 none layer_21/width_16k/average_l0_48Load this SAE jumprelu blocks.21.hook_mlp_out 16384 none layer_21/width_16k/average_l0_633Load this SAE jumprelu blocks.21.hook_mlp_out 16384 none layer_22/width_16k/average_l0_121Load this SAE jumprelu blocks.22.hook_mlp_out 16384 none layer_22/width_16k/average_l0_24Load this SAE jumprelu blocks.22.hook_mlp_out 16384 none layer_22/width_16k/average_l0_290Load this SAE jumprelu blocks.22.hook_mlp_out 16384 none layer_22/width_16k/average_l0_51Load this SAE jumprelu blocks.22.hook_mlp_out 16384 none layer_22/width_16k/average_l0_624Load this SAE jumprelu blocks.22.hook_mlp_out 16384 none layer_23/width_16k/average_l0_128Load this SAE jumprelu blocks.23.hook_mlp_out 16384 none layer_23/width_16k/average_l0_27Load this SAE jumprelu blocks.23.hook_mlp_out 16384 none layer_23/width_16k/average_l0_287Load this SAE jumprelu blocks.23.hook_mlp_out 16384 none layer_23/width_16k/average_l0_57Load this SAE jumprelu blocks.23.hook_mlp_out 16384 none layer_23/width_16k/average_l0_627Load this SAE jumprelu blocks.23.hook_mlp_out 16384 none layer_24/width_16k/average_l0_158Load this SAE jumprelu blocks.24.hook_mlp_out 16384 none layer_24/width_16k/average_l0_19Load this SAE jumprelu blocks.24.hook_mlp_out 16384 none layer_24/width_16k/average_l0_35Load this SAE jumprelu blocks.24.hook_mlp_out 16384 none layer_24/width_16k/average_l0_357Load this SAE jumprelu blocks.24.hook_mlp_out 16384 none layer_24/width_16k/average_l0_73Load this SAE jumprelu blocks.24.hook_mlp_out 16384 none layer_25/width_16k/average_l0_126Load this SAE jumprelu blocks.25.hook_mlp_out 16384 none layer_25/width_16k/average_l0_15Load this SAE jumprelu blocks.25.hook_mlp_out 16384 none layer_25/width_16k/average_l0_277Load this SAE jumprelu blocks.25.hook_mlp_out 16384 none layer_25/width_16k/average_l0_29Load this SAE jumprelu blocks.25.hook_mlp_out 16384 none layer_25/width_16k/average_l0_59Load this SAE jumprelu blocks.25.hook_mlp_out 16384 none layer_0/width_65k/average_l0_12Load this SAE jumprelu blocks.0.hook_mlp_out 65536 none layer_0/width_65k/average_l0_21Load this SAE jumprelu blocks.0.hook_mlp_out 65536 none layer_0/width_65k/average_l0_39Load this SAE jumprelu blocks.0.hook_mlp_out 65536 none layer_0/width_65k/average_l0_7Load this SAE jumprelu blocks.0.hook_mlp_out 65536 none layer_0/width_65k/average_l0_72Load this SAE jumprelu blocks.0.hook_mlp_out 65536 none layer_1/width_65k/average_l0_11Load this SAE jumprelu blocks.1.hook_mlp_out 65536 none layer_1/width_65k/average_l0_127Load this SAE jumprelu blocks.1.hook_mlp_out 65536 none layer_1/width_65k/average_l0_20Load this SAE jumprelu blocks.1.hook_mlp_out 65536 none layer_1/width_65k/average_l0_37Load this SAE jumprelu blocks.1.hook_mlp_out 65536 none layer_1/width_65k/average_l0_67Load this SAE jumprelu blocks.1.hook_mlp_out 65536 none layer_2/width_65k/average_l0_134Load this SAE jumprelu blocks.2.hook_mlp_out 65536 none layer_2/width_65k/average_l0_16Load this SAE jumprelu blocks.2.hook_mlp_out 65536 none layer_2/width_65k/average_l0_265Load this SAE jumprelu blocks.2.hook_mlp_out 65536 none layer_2/width_65k/average_l0_31Load this SAE jumprelu blocks.2.hook_mlp_out 65536 none layer_2/width_65k/average_l0_60Load this SAE jumprelu blocks.2.hook_mlp_out 65536 none layer_3/width_65k/average_l0_144Load this SAE jumprelu blocks.3.hook_mlp_out 65536 none layer_3/width_65k/average_l0_18Load this SAE jumprelu blocks.3.hook_mlp_out 65536 none layer_3/width_65k/average_l0_279Load this SAE jumprelu blocks.3.hook_mlp_out 65536 none layer_3/width_65k/average_l0_33Load this SAE jumprelu blocks.3.hook_mlp_out 65536 none layer_3/width_65k/average_l0_68Load this SAE jumprelu blocks.3.hook_mlp_out 65536 none layer_4/width_65k/average_l0_138Load this SAE jumprelu blocks.4.hook_mlp_out 65536 none layer_4/width_65k/average_l0_17Load this SAE jumprelu blocks.4.hook_mlp_out 65536 none layer_4/width_65k/average_l0_299Load this SAE jumprelu blocks.4.hook_mlp_out 65536 none layer_4/width_65k/average_l0_32Load this SAE jumprelu blocks.4.hook_mlp_out 65536 none layer_4/width_65k/average_l0_66Load this SAE jumprelu blocks.4.hook_mlp_out 65536 none layer_5/width_65k/average_l0_186Load this SAE jumprelu blocks.5.hook_mlp_out 65536 none layer_5/width_65k/average_l0_22Load this SAE jumprelu blocks.5.hook_mlp_out 65536 none layer_5/width_65k/average_l0_407Load this SAE jumprelu blocks.5.hook_mlp_out 65536 none layer_5/width_65k/average_l0_43Load this SAE jumprelu blocks.5.hook_mlp_out 65536 none layer_5/width_65k/average_l0_86Load this SAE jumprelu blocks.5.hook_mlp_out 65536 none layer_6/width_65k/average_l0_101Load this SAE jumprelu blocks.6.hook_mlp_out 65536 none layer_6/width_65k/average_l0_224Load this SAE jumprelu blocks.6.hook_mlp_out 65536 none layer_6/width_65k/average_l0_24Load this SAE jumprelu blocks.6.hook_mlp_out 65536 none layer_6/width_65k/average_l0_47Load this SAE jumprelu blocks.6.hook_mlp_out 65536 none layer_6/width_65k/average_l0_515Load this SAE jumprelu blocks.6.hook_mlp_out 65536 none layer_7/width_65k/average_l0_115Load this SAE jumprelu blocks.7.hook_mlp_out 65536 none layer_7/width_65k/average_l0_266Load this SAE jumprelu blocks.7.hook_mlp_out 65536 none layer_7/width_65k/average_l0_28Load this SAE jumprelu blocks.7.hook_mlp_out 65536 none layer_7/width_65k/average_l0_56Load this SAE jumprelu blocks.7.hook_mlp_out 65536 none layer_7/width_65k/average_l0_571Load this SAE jumprelu blocks.7.hook_mlp_out 65536 none layer_8/width_65k/average_l0_110Load this SAE jumprelu blocks.8.hook_mlp_out 65536 none layer_8/width_65k/average_l0_256Load this SAE jumprelu blocks.8.hook_mlp_out 65536 none layer_8/width_65k/average_l0_31Load this SAE jumprelu blocks.8.hook_mlp_out 65536 none layer_8/width_65k/average_l0_547Load this SAE jumprelu blocks.8.hook_mlp_out 65536 none layer_8/width_65k/average_l0_55Load this SAE jumprelu blocks.8.hook_mlp_out 65536 none layer_9/width_65k/average_l0_168Load this SAE jumprelu blocks.9.hook_mlp_out 65536 none layer_9/width_65k/average_l0_38Load this SAE jumprelu blocks.9.hook_mlp_out 65536 none layer_9/width_65k/average_l0_387Load this SAE jumprelu blocks.9.hook_mlp_out 65536 none layer_9/width_65k/average_l0_745Load this SAE jumprelu blocks.9.hook_mlp_out 65536 none layer_9/width_65k/average_l0_77Load this SAE jumprelu blocks.9.hook_mlp_out 65536 none layer_10/width_65k/average_l0_218Load this SAE jumprelu blocks.10.hook_mlp_out 65536 none layer_10/width_65k/average_l0_43Load this SAE jumprelu blocks.10.hook_mlp_out 65536 none layer_10/width_65k/average_l0_474Load this SAE jumprelu blocks.10.hook_mlp_out 65536 none layer_10/width_65k/average_l0_851Load this SAE jumprelu blocks.10.hook_mlp_out 65536 none layer_10/width_65k/average_l0_95Load this SAE jumprelu blocks.10.hook_mlp_out 65536 none layer_11/width_65k/average_l0_200Load this SAE jumprelu blocks.11.hook_mlp_out 65536 none layer_11/width_65k/average_l0_41Load this SAE jumprelu blocks.11.hook_mlp_out 65536 none layer_11/width_65k/average_l0_436Load this SAE jumprelu blocks.11.hook_mlp_out 65536 none layer_11/width_65k/average_l0_771Load this SAE jumprelu blocks.11.hook_mlp_out 65536 none layer_11/width_65k/average_l0_88Load this SAE jumprelu blocks.11.hook_mlp_out 65536 none layer_12/width_65k/average_l0_222Load this SAE jumprelu blocks.12.hook_mlp_out 65536 none layer_12/width_65k/average_l0_44Load this SAE jumprelu blocks.12.hook_mlp_out 65536 none layer_12/width_65k/average_l0_482Load this SAE jumprelu blocks.12.hook_mlp_out 65536 none layer_12/width_65k/average_l0_848Load this SAE jumprelu blocks.12.hook_mlp_out 65536 none layer_12/width_65k/average_l0_96Load this SAE jumprelu blocks.12.hook_mlp_out 65536 none layer_13/width_65k/average_l0_228Load this SAE jumprelu blocks.13.hook_mlp_out 65536 none layer_13/width_65k/average_l0_44Load this SAE jumprelu blocks.13.hook_mlp_out 65536 none layer_13/width_65k/average_l0_480Load this SAE jumprelu blocks.13.hook_mlp_out 65536 none layer_13/width_65k/average_l0_841Load this SAE jumprelu blocks.13.hook_mlp_out 65536 none layer_13/width_65k/average_l0_98Load this SAE jumprelu blocks.13.hook_mlp_out 65536 none layer_14/width_65k/average_l0_204Load this SAE jumprelu blocks.14.hook_mlp_out 65536 none layer_14/width_65k/average_l0_39Load this SAE jumprelu blocks.14.hook_mlp_out 65536 none layer_14/width_65k/average_l0_463Load this SAE jumprelu blocks.14.hook_mlp_out 65536 none layer_14/width_65k/average_l0_816Load this SAE jumprelu blocks.14.hook_mlp_out 65536 none layer_14/width_65k/average_l0_89Load this SAE jumprelu blocks.14.hook_mlp_out 65536 none layer_15/width_65k/average_l0_164Load this SAE jumprelu blocks.15.hook_mlp_out 65536 none layer_15/width_65k/average_l0_35Load this SAE jumprelu blocks.15.hook_mlp_out 65536 none layer_15/width_65k/average_l0_405Load this SAE jumprelu blocks.15.hook_mlp_out 65536 none layer_15/width_65k/average_l0_72Load this SAE jumprelu blocks.15.hook_mlp_out 65536 none layer_15/width_65k/average_l0_754Load this SAE jumprelu blocks.15.hook_mlp_out 65536 none layer_16/width_65k/average_l0_142Load this SAE jumprelu blocks.16.hook_mlp_out 65536 none layer_16/width_65k/average_l0_32Load this SAE jumprelu blocks.16.hook_mlp_out 65536 none layer_16/width_65k/average_l0_348Load this SAE jumprelu blocks.16.hook_mlp_out 65536 none layer_16/width_65k/average_l0_66Load this SAE jumprelu blocks.16.hook_mlp_out 65536 none layer_16/width_65k/average_l0_695Load this SAE jumprelu blocks.16.hook_mlp_out 65536 none layer_17/width_65k/average_l0_136Load this SAE jumprelu blocks.17.hook_mlp_out 65536 none layer_17/width_65k/average_l0_30Load this SAE jumprelu blocks.17.hook_mlp_out 65536 none layer_17/width_65k/average_l0_342Load this SAE jumprelu blocks.17.hook_mlp_out 65536 none layer_17/width_65k/average_l0_61Load this SAE jumprelu blocks.17.hook_mlp_out 65536 none layer_17/width_65k/average_l0_666Load this SAE jumprelu blocks.17.hook_mlp_out 65536 none layer_18/width_65k/average_l0_191Load this SAE jumprelu blocks.18.hook_mlp_out 65536 none layer_18/width_65k/average_l0_24Load this SAE jumprelu blocks.18.hook_mlp_out 65536 none layer_18/width_65k/average_l0_44Load this SAE jumprelu blocks.18.hook_mlp_out 65536 none layer_18/width_65k/average_l0_491Load this SAE jumprelu blocks.18.hook_mlp_out 65536 none layer_18/width_65k/average_l0_88Load this SAE jumprelu blocks.18.hook_mlp_out 65536 none layer_19/width_65k/average_l0_192Load this SAE jumprelu blocks.19.hook_mlp_out 65536 none layer_19/width_65k/average_l0_25Load this SAE jumprelu blocks.19.hook_mlp_out 65536 none layer_19/width_65k/average_l0_45Load this SAE jumprelu blocks.19.hook_mlp_out 65536 none layer_19/width_65k/average_l0_470Load this SAE jumprelu blocks.19.hook_mlp_out 65536 none layer_19/width_65k/average_l0_88Load this SAE jumprelu blocks.19.hook_mlp_out 65536 none layer_20/width_65k/average_l0_189Load this SAE jumprelu blocks.20.hook_mlp_out 65536 none layer_20/width_65k/average_l0_23Load this SAE jumprelu blocks.20.hook_mlp_out 65536 none layer_20/width_65k/average_l0_44Load this SAE jumprelu blocks.20.hook_mlp_out 65536 none layer_20/width_65k/average_l0_446Load this SAE jumprelu blocks.20.hook_mlp_out 65536 none layer_20/width_65k/average_l0_88Load this SAE jumprelu blocks.20.hook_mlp_out 65536 none layer_21/width_65k/average_l0_192Load this SAE jumprelu blocks.21.hook_mlp_out 65536 none layer_21/width_65k/average_l0_23Load this SAE jumprelu blocks.21.hook_mlp_out 65536 none layer_21/width_65k/average_l0_42Load this SAE jumprelu blocks.21.hook_mlp_out 65536 none layer_21/width_65k/average_l0_472Load this SAE jumprelu blocks.21.hook_mlp_out 65536 none layer_21/width_65k/average_l0_86Load this SAE jumprelu blocks.21.hook_mlp_out 65536 none layer_22/width_65k/average_l0_203Load this SAE jumprelu blocks.22.hook_mlp_out 65536 none layer_22/width_65k/average_l0_23Load this SAE jumprelu blocks.22.hook_mlp_out 65536 none layer_22/width_65k/average_l0_46Load this SAE jumprelu blocks.22.hook_mlp_out 65536 none layer_22/width_65k/average_l0_487Load this SAE jumprelu blocks.22.hook_mlp_out 65536 none layer_22/width_65k/average_l0_92Load this SAE jumprelu blocks.22.hook_mlp_out 65536 none layer_23/width_65k/average_l0_102Load this SAE jumprelu blocks.23.hook_mlp_out 65536 none layer_23/width_65k/average_l0_218Load this SAE jumprelu blocks.23.hook_mlp_out 65536 none layer_23/width_65k/average_l0_25Load this SAE jumprelu blocks.23.hook_mlp_out 65536 none layer_23/width_65k/average_l0_49Load this SAE jumprelu blocks.23.hook_mlp_out 65536 none layer_23/width_65k/average_l0_497Load this SAE jumprelu blocks.23.hook_mlp_out 65536 none layer_24/width_65k/average_l0_128Load this SAE jumprelu blocks.24.hook_mlp_out 65536 none layer_24/width_65k/average_l0_18Load this SAE jumprelu blocks.24.hook_mlp_out 65536 none layer_24/width_65k/average_l0_268Load this SAE jumprelu blocks.24.hook_mlp_out 65536 none layer_24/width_65k/average_l0_32Load this SAE jumprelu blocks.24.hook_mlp_out 65536 none layer_24/width_65k/average_l0_62Load this SAE jumprelu blocks.24.hook_mlp_out 65536 none layer_25/width_65k/average_l0_107Load this SAE jumprelu blocks.25.hook_mlp_out 65536 none layer_25/width_65k/average_l0_14Load this SAE jumprelu blocks.25.hook_mlp_out 65536 none layer_25/width_65k/average_l0_215Load this SAE jumprelu blocks.25.hook_mlp_out 65536 none layer_25/width_65k/average_l0_26Load this SAE jumprelu blocks.25.hook_mlp_out 65536 none layer_25/width_65k/average_l0_52Load this SAE jumprelu blocks.25.hook_mlp_out 65536 none"},{"location":"sae_table/#gemma-scope-2b-pt-mlp-canonical","title":"gemma-scope-2b-pt-mlp-canonical","text":"<ul> <li>Huggingface Repo: google/gemma-scope-2b-pt-mlp</li> <li>model: gemma-2-2b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/0-gemmascope-mlp-16k blocks.0.hook_mlp_out 16384 none layer_1/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/1-gemmascope-mlp-16k blocks.1.hook_mlp_out 16384 none layer_2/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/2-gemmascope-mlp-16k blocks.2.hook_mlp_out 16384 none layer_3/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/3-gemmascope-mlp-16k blocks.3.hook_mlp_out 16384 none layer_4/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/4-gemmascope-mlp-16k blocks.4.hook_mlp_out 16384 none layer_5/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/5-gemmascope-mlp-16k blocks.5.hook_mlp_out 16384 none layer_6/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/6-gemmascope-mlp-16k blocks.6.hook_mlp_out 16384 none layer_7/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/7-gemmascope-mlp-16k blocks.7.hook_mlp_out 16384 none layer_8/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/8-gemmascope-mlp-16k blocks.8.hook_mlp_out 16384 none layer_9/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/9-gemmascope-mlp-16k blocks.9.hook_mlp_out 16384 none layer_10/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/10-gemmascope-mlp-16k blocks.10.hook_mlp_out 16384 none layer_11/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/11-gemmascope-mlp-16k blocks.11.hook_mlp_out 16384 none layer_12/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/12-gemmascope-mlp-16k blocks.12.hook_mlp_out 16384 none layer_13/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/13-gemmascope-mlp-16k blocks.13.hook_mlp_out 16384 none layer_14/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/14-gemmascope-mlp-16k blocks.14.hook_mlp_out 16384 none layer_15/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/15-gemmascope-mlp-16k blocks.15.hook_mlp_out 16384 none layer_16/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/16-gemmascope-mlp-16k blocks.16.hook_mlp_out 16384 none layer_17/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/17-gemmascope-mlp-16k blocks.17.hook_mlp_out 16384 none layer_18/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/18-gemmascope-mlp-16k blocks.18.hook_mlp_out 16384 none layer_19/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/19-gemmascope-mlp-16k blocks.19.hook_mlp_out 16384 none layer_20/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/20-gemmascope-mlp-16k blocks.20.hook_mlp_out 16384 none layer_21/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/21-gemmascope-mlp-16k blocks.21.hook_mlp_out 16384 none layer_22/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/22-gemmascope-mlp-16k blocks.22.hook_mlp_out 16384 none layer_23/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/23-gemmascope-mlp-16k blocks.23.hook_mlp_out 16384 none layer_24/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/24-gemmascope-mlp-16k blocks.24.hook_mlp_out 16384 none layer_25/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/25-gemmascope-mlp-16k blocks.25.hook_mlp_out 16384 none layer_0/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/0-gemmascope-mlp-65k blocks.0.hook_mlp_out 65536 none layer_1/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/1-gemmascope-mlp-65k blocks.1.hook_mlp_out 65536 none layer_2/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/2-gemmascope-mlp-65k blocks.2.hook_mlp_out 65536 none layer_3/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/3-gemmascope-mlp-65k blocks.3.hook_mlp_out 65536 none layer_4/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/4-gemmascope-mlp-65k blocks.4.hook_mlp_out 65536 none layer_5/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/5-gemmascope-mlp-65k blocks.5.hook_mlp_out 65536 none layer_6/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/6-gemmascope-mlp-65k blocks.6.hook_mlp_out 65536 none layer_7/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/7-gemmascope-mlp-65k blocks.7.hook_mlp_out 65536 none layer_8/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/8-gemmascope-mlp-65k blocks.8.hook_mlp_out 65536 none layer_9/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/9-gemmascope-mlp-65k blocks.9.hook_mlp_out 65536 none layer_10/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/10-gemmascope-mlp-65k blocks.10.hook_mlp_out 65536 none layer_11/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/11-gemmascope-mlp-65k blocks.11.hook_mlp_out 65536 none layer_12/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/12-gemmascope-mlp-65k blocks.12.hook_mlp_out 65536 none layer_13/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/13-gemmascope-mlp-65k blocks.13.hook_mlp_out 65536 none layer_14/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/14-gemmascope-mlp-65k blocks.14.hook_mlp_out 65536 none layer_15/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/15-gemmascope-mlp-65k blocks.15.hook_mlp_out 65536 none layer_16/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/16-gemmascope-mlp-65k blocks.16.hook_mlp_out 65536 none layer_17/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/17-gemmascope-mlp-65k blocks.17.hook_mlp_out 65536 none layer_18/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/18-gemmascope-mlp-65k blocks.18.hook_mlp_out 65536 none layer_19/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/19-gemmascope-mlp-65k blocks.19.hook_mlp_out 65536 none layer_20/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/20-gemmascope-mlp-65k blocks.20.hook_mlp_out 65536 none layer_21/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/21-gemmascope-mlp-65k blocks.21.hook_mlp_out 65536 none layer_22/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/22-gemmascope-mlp-65k blocks.22.hook_mlp_out 65536 none layer_23/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/23-gemmascope-mlp-65k blocks.23.hook_mlp_out 65536 none layer_24/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/24-gemmascope-mlp-65k blocks.24.hook_mlp_out 65536 none layer_25/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/25-gemmascope-mlp-65k blocks.25.hook_mlp_out 65536 none"},{"location":"sae_table/#gemma-scope-2b-pt-res","title":"gemma-scope-2b-pt-res","text":"<ul> <li>Huggingface Repo: google/gemma-scope-2b-pt-res</li> <li>model: gemma-2-2b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations embedding/width_4k/average_l0_6Load this SAE jumprelu hook_embed 4096 none embedding/width_4k/average_l0_44Load this SAE jumprelu hook_embed 4096 none embedding/width_4k/average_l0_21Load this SAE jumprelu hook_embed 4096 none embedding/width_4k/average_l0_111Load this SAE jumprelu hook_embed 4096 none layer_0/width_16k/average_l0_105Load this SAE jumprelu blocks.0.hook_resid_post 16384 none layer_0/width_16k/average_l0_13Load this SAE jumprelu blocks.0.hook_resid_post 16384 none layer_0/width_16k/average_l0_226Load this SAE jumprelu blocks.0.hook_resid_post 16384 none layer_0/width_16k/average_l0_25Load this SAE jumprelu blocks.0.hook_resid_post 16384 none layer_0/width_16k/average_l0_46Load this SAE jumprelu blocks.0.hook_resid_post 16384 none layer_1/width_16k/average_l0_10Load this SAE jumprelu blocks.1.hook_resid_post 16384 none layer_1/width_16k/average_l0_102Load this SAE jumprelu blocks.1.hook_resid_post 16384 none layer_1/width_16k/average_l0_20Load this SAE jumprelu blocks.1.hook_resid_post 16384 none layer_1/width_16k/average_l0_250Load this SAE jumprelu blocks.1.hook_resid_post 16384 none layer_1/width_16k/average_l0_40Load this SAE jumprelu blocks.1.hook_resid_post 16384 none layer_2/width_16k/average_l0_13Load this SAE jumprelu blocks.2.hook_resid_post 16384 none layer_2/width_16k/average_l0_141Load this SAE jumprelu blocks.2.hook_resid_post 16384 none layer_2/width_16k/average_l0_142Load this SAE jumprelu blocks.2.hook_resid_post 16384 none layer_2/width_16k/average_l0_24Load this SAE jumprelu blocks.2.hook_resid_post 16384 none layer_2/width_16k/average_l0_304Load this SAE jumprelu blocks.2.hook_resid_post 16384 none layer_2/width_16k/average_l0_53Load this SAE jumprelu blocks.2.hook_resid_post 16384 none layer_3/width_16k/average_l0_14Load this SAE jumprelu blocks.3.hook_resid_post 16384 none layer_3/width_16k/average_l0_142Load this SAE jumprelu blocks.3.hook_resid_post 16384 none layer_3/width_16k/average_l0_28Load this SAE jumprelu blocks.3.hook_resid_post 16384 none layer_3/width_16k/average_l0_315Load this SAE jumprelu blocks.3.hook_resid_post 16384 none layer_3/width_16k/average_l0_59Load this SAE jumprelu blocks.3.hook_resid_post 16384 none layer_4/width_16k/average_l0_124Load this SAE jumprelu blocks.4.hook_resid_post 16384 none layer_4/width_16k/average_l0_125Load this SAE jumprelu blocks.4.hook_resid_post 16384 none layer_4/width_16k/average_l0_17Load this SAE jumprelu blocks.4.hook_resid_post 16384 none layer_4/width_16k/average_l0_281Load this SAE jumprelu blocks.4.hook_resid_post 16384 none layer_4/width_16k/average_l0_31Load this SAE jumprelu blocks.4.hook_resid_post 16384 none layer_4/width_16k/average_l0_60Load this SAE jumprelu blocks.4.hook_resid_post 16384 none layer_5/width_16k/average_l0_143Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-16k__l0-143 blocks.5.hook_resid_post 16384 none layer_5/width_16k/average_l0_18Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-16k__l0-18 blocks.5.hook_resid_post 16384 none layer_5/width_16k/average_l0_309Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-16k__l0-309 blocks.5.hook_resid_post 16384 none layer_5/width_16k/average_l0_34Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-16k__l0-34 blocks.5.hook_resid_post 16384 none layer_5/width_16k/average_l0_68Load this SAE jumprelu blocks.5.hook_resid_post 16384 none layer_6/width_16k/average_l0_144Load this SAE jumprelu blocks.6.hook_resid_post 16384 none layer_6/width_16k/average_l0_19Load this SAE jumprelu blocks.6.hook_resid_post 16384 none layer_6/width_16k/average_l0_301Load this SAE jumprelu blocks.6.hook_resid_post 16384 none layer_6/width_16k/average_l0_36Load this SAE jumprelu blocks.6.hook_resid_post 16384 none layer_6/width_16k/average_l0_70Load this SAE jumprelu blocks.6.hook_resid_post 16384 none layer_7/width_16k/average_l0_137Load this SAE jumprelu blocks.7.hook_resid_post 16384 none layer_7/width_16k/average_l0_20Load this SAE jumprelu blocks.7.hook_resid_post 16384 none layer_7/width_16k/average_l0_285Load this SAE jumprelu blocks.7.hook_resid_post 16384 none layer_7/width_16k/average_l0_36Load this SAE jumprelu blocks.7.hook_resid_post 16384 none layer_7/width_16k/average_l0_69Load this SAE jumprelu blocks.7.hook_resid_post 16384 none layer_8/width_16k/average_l0_142Load this SAE jumprelu blocks.8.hook_resid_post 16384 none layer_8/width_16k/average_l0_20Load this SAE jumprelu blocks.8.hook_resid_post 16384 none layer_8/width_16k/average_l0_301Load this SAE jumprelu blocks.8.hook_resid_post 16384 none layer_8/width_16k/average_l0_37Load this SAE jumprelu blocks.8.hook_resid_post 16384 none layer_8/width_16k/average_l0_71Load this SAE jumprelu blocks.8.hook_resid_post 16384 none layer_9/width_16k/average_l0_151Load this SAE jumprelu blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_21Load this SAE jumprelu blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_340Load this SAE jumprelu blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_37Load this SAE jumprelu blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_73Load this SAE jumprelu blocks.9.hook_resid_post 16384 none layer_10/width_16k/average_l0_166Load this SAE jumprelu blocks.10.hook_resid_post 16384 none layer_10/width_16k/average_l0_21Load this SAE jumprelu blocks.10.hook_resid_post 16384 none layer_10/width_16k/average_l0_39Load this SAE jumprelu blocks.10.hook_resid_post 16384 none layer_10/width_16k/average_l0_395Load this SAE jumprelu blocks.10.hook_resid_post 16384 none layer_10/width_16k/average_l0_77Load this SAE jumprelu blocks.10.hook_resid_post 16384 none layer_11/width_16k/average_l0_168Load this SAE jumprelu blocks.11.hook_resid_post 16384 none layer_11/width_16k/average_l0_22Load this SAE jumprelu blocks.11.hook_resid_post 16384 none layer_11/width_16k/average_l0_393Load this SAE jumprelu blocks.11.hook_resid_post 16384 none layer_11/width_16k/average_l0_41Load this SAE jumprelu blocks.11.hook_resid_post 16384 none layer_11/width_16k/average_l0_80Load this SAE jumprelu blocks.11.hook_resid_post 16384 none layer_12/width_16k/average_l0_176Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-16k__l0-176 blocks.12.hook_resid_post 16384 none layer_12/width_16k/average_l0_22Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-16k__l0-22 blocks.12.hook_resid_post 16384 none layer_12/width_16k/average_l0_41Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-16k__l0-41 blocks.12.hook_resid_post 16384 none layer_12/width_16k/average_l0_445Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-16k__l0-445 blocks.12.hook_resid_post 16384 none layer_12/width_16k/average_l0_82Load this SAE jumprelu blocks.12.hook_resid_post 16384 none layer_13/width_16k/average_l0_173Load this SAE jumprelu blocks.13.hook_resid_post 16384 none layer_13/width_16k/average_l0_23Load this SAE jumprelu blocks.13.hook_resid_post 16384 none layer_13/width_16k/average_l0_403Load this SAE jumprelu blocks.13.hook_resid_post 16384 none layer_13/width_16k/average_l0_43Load this SAE jumprelu blocks.13.hook_resid_post 16384 none layer_13/width_16k/average_l0_83Load this SAE jumprelu blocks.13.hook_resid_post 16384 none layer_13/width_16k/average_l0_84Load this SAE jumprelu blocks.13.hook_resid_post 16384 none layer_14/width_16k/average_l0_173Load this SAE jumprelu blocks.14.hook_resid_post 16384 none layer_14/width_16k/average_l0_23Load this SAE jumprelu blocks.14.hook_resid_post 16384 none layer_14/width_16k/average_l0_388Load this SAE jumprelu blocks.14.hook_resid_post 16384 none layer_14/width_16k/average_l0_43Load this SAE jumprelu blocks.14.hook_resid_post 16384 none layer_14/width_16k/average_l0_83Load this SAE jumprelu blocks.14.hook_resid_post 16384 none layer_14/width_16k/average_l0_84Load this SAE jumprelu blocks.14.hook_resid_post 16384 none layer_15/width_16k/average_l0_150Load this SAE jumprelu blocks.15.hook_resid_post 16384 none layer_15/width_16k/average_l0_23Load this SAE jumprelu blocks.15.hook_resid_post 16384 none layer_15/width_16k/average_l0_308Load this SAE jumprelu blocks.15.hook_resid_post 16384 none layer_15/width_16k/average_l0_41Load this SAE jumprelu blocks.15.hook_resid_post 16384 none layer_15/width_16k/average_l0_78Load this SAE jumprelu blocks.15.hook_resid_post 16384 none layer_16/width_16k/average_l0_154Load this SAE jumprelu blocks.16.hook_resid_post 16384 none layer_16/width_16k/average_l0_23Load this SAE jumprelu blocks.16.hook_resid_post 16384 none layer_16/width_16k/average_l0_335Load this SAE jumprelu blocks.16.hook_resid_post 16384 none layer_16/width_16k/average_l0_42Load this SAE jumprelu blocks.16.hook_resid_post 16384 none layer_16/width_16k/average_l0_78Load this SAE jumprelu blocks.16.hook_resid_post 16384 none layer_17/width_16k/average_l0_150Load this SAE jumprelu blocks.17.hook_resid_post 16384 none layer_17/width_16k/average_l0_23Load this SAE jumprelu blocks.17.hook_resid_post 16384 none layer_17/width_16k/average_l0_304Load this SAE jumprelu blocks.17.hook_resid_post 16384 none layer_17/width_16k/average_l0_42Load this SAE jumprelu blocks.17.hook_resid_post 16384 none layer_17/width_16k/average_l0_77Load this SAE jumprelu blocks.17.hook_resid_post 16384 none layer_18/width_16k/average_l0_138Load this SAE jumprelu blocks.18.hook_resid_post 16384 none layer_18/width_16k/average_l0_23Load this SAE jumprelu blocks.18.hook_resid_post 16384 none layer_18/width_16k/average_l0_280Load this SAE jumprelu blocks.18.hook_resid_post 16384 none layer_18/width_16k/average_l0_40Load this SAE jumprelu blocks.18.hook_resid_post 16384 none layer_18/width_16k/average_l0_74Load this SAE jumprelu blocks.18.hook_resid_post 16384 none layer_19/width_16k/average_l0_137Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-16k__l0-137 blocks.19.hook_resid_post 16384 none layer_19/width_16k/average_l0_23Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-16k__l0-23 blocks.19.hook_resid_post 16384 none layer_19/width_16k/average_l0_279Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-16k__l0-279 blocks.19.hook_resid_post 16384 none layer_19/width_16k/average_l0_40Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-16k__l0-40 blocks.19.hook_resid_post 16384 none layer_19/width_16k/average_l0_73Load this SAE jumprelu blocks.19.hook_resid_post 16384 none layer_20/width_16k/average_l0_139Load this SAE jumprelu blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_22Load this SAE jumprelu blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_294Load this SAE jumprelu blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_38Load this SAE jumprelu blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_71Load this SAE jumprelu blocks.20.hook_resid_post 16384 none layer_21/width_16k/average_l0_139Load this SAE jumprelu blocks.21.hook_resid_post 16384 none layer_21/width_16k/average_l0_22Load this SAE jumprelu blocks.21.hook_resid_post 16384 none layer_21/width_16k/average_l0_301Load this SAE jumprelu blocks.21.hook_resid_post 16384 none layer_21/width_16k/average_l0_38Load this SAE jumprelu blocks.21.hook_resid_post 16384 none layer_21/width_16k/average_l0_70Load this SAE jumprelu blocks.21.hook_resid_post 16384 none layer_22/width_16k/average_l0_147Load this SAE jumprelu blocks.22.hook_resid_post 16384 none layer_22/width_16k/average_l0_21Load this SAE jumprelu blocks.22.hook_resid_post 16384 none layer_22/width_16k/average_l0_349Load this SAE jumprelu blocks.22.hook_resid_post 16384 none layer_22/width_16k/average_l0_38Load this SAE jumprelu blocks.22.hook_resid_post 16384 none layer_22/width_16k/average_l0_72Load this SAE jumprelu blocks.22.hook_resid_post 16384 none layer_23/width_16k/average_l0_157Load this SAE jumprelu blocks.23.hook_resid_post 16384 none layer_23/width_16k/average_l0_21Load this SAE jumprelu blocks.23.hook_resid_post 16384 none layer_23/width_16k/average_l0_38Load this SAE jumprelu blocks.23.hook_resid_post 16384 none layer_23/width_16k/average_l0_404Load this SAE jumprelu blocks.23.hook_resid_post 16384 none layer_23/width_16k/average_l0_74Load this SAE jumprelu blocks.23.hook_resid_post 16384 none layer_23/width_16k/average_l0_75Load this SAE jumprelu blocks.23.hook_resid_post 16384 none layer_24/width_16k/average_l0_158Load this SAE jumprelu blocks.24.hook_resid_post 16384 none layer_24/width_16k/average_l0_20Load this SAE jumprelu blocks.24.hook_resid_post 16384 none layer_24/width_16k/average_l0_38Load this SAE jumprelu blocks.24.hook_resid_post 16384 none layer_24/width_16k/average_l0_457Load this SAE jumprelu blocks.24.hook_resid_post 16384 none layer_24/width_16k/average_l0_73Load this SAE jumprelu blocks.24.hook_resid_post 16384 none layer_25/width_16k/average_l0_116Load this SAE jumprelu blocks.25.hook_resid_post 16384 none layer_25/width_16k/average_l0_16Load this SAE jumprelu blocks.25.hook_resid_post 16384 none layer_25/width_16k/average_l0_28Load this SAE jumprelu blocks.25.hook_resid_post 16384 none layer_25/width_16k/average_l0_285Load this SAE jumprelu blocks.25.hook_resid_post 16384 none layer_25/width_16k/average_l0_55Load this SAE jumprelu blocks.25.hook_resid_post 16384 none layer_5/width_1m/average_l0_114Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-1m__l0-114 blocks.5.hook_resid_post 1048576 none layer_5/width_1m/average_l0_13Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-1m__l0-13 blocks.5.hook_resid_post 1048576 none layer_5/width_1m/average_l0_21Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-1m__l0-21 blocks.5.hook_resid_post 1048576 none layer_5/width_1m/average_l0_36Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-1m__l0-36 blocks.5.hook_resid_post 1048576 none layer_5/width_1m/average_l0_63Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-1m__l0-63 blocks.5.hook_resid_post 1048576 none layer_5/width_1m/average_l0_9Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-1m__l0-9 blocks.5.hook_resid_post 1048576 none layer_12/width_1m/average_l0_107Load this SAE jumprelu blocks.12.hook_resid_post 1048576 none layer_12/width_1m/average_l0_19Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-1m__l0-19 blocks.12.hook_resid_post 1048576 none layer_12/width_1m/average_l0_207Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-1m__l0-207 blocks.12.hook_resid_post 1048576 none layer_12/width_1m/average_l0_26Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-1m__l0-26 blocks.12.hook_resid_post 1048576 none layer_12/width_1m/average_l0_58Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-1m__l0-58 blocks.12.hook_resid_post 1048576 none layer_12/width_1m/average_l0_73Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-1m__l0-73 blocks.12.hook_resid_post 1048576 none layer_19/width_1m/average_l0_157Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-1m__l0-157 blocks.19.hook_resid_post 1048576 none layer_19/width_1m/average_l0_16Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-1m__l0-16 blocks.19.hook_resid_post 1048576 none layer_19/width_1m/average_l0_18Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-1m__l0-18 blocks.19.hook_resid_post 1048576 none layer_19/width_1m/average_l0_29Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-1m__l0-29 blocks.19.hook_resid_post 1048576 none layer_19/width_1m/average_l0_50Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-1m__l0-50 blocks.19.hook_resid_post 1048576 none layer_19/width_1m/average_l0_88Load this SAE jumprelu blocks.19.hook_resid_post 1048576 none layer_12/width_262k/average_l0_11Load this SAE jumprelu blocks.12.hook_resid_post 262144 none layer_12/width_262k/average_l0_121Load this SAE jumprelu blocks.12.hook_resid_post 262144 none layer_12/width_262k/average_l0_21Load this SAE jumprelu blocks.12.hook_resid_post 262144 none layer_12/width_262k/average_l0_243Load this SAE jumprelu blocks.12.hook_resid_post 262144 none layer_12/width_262k/average_l0_36Load this SAE jumprelu blocks.12.hook_resid_post 262144 none layer_12/width_262k/average_l0_67Load this SAE jumprelu blocks.12.hook_resid_post 262144 none layer_12/width_32k/average_l0_12Load this SAE jumprelu blocks.12.hook_resid_post 32768 none layer_12/width_32k/average_l0_155Load this SAE jumprelu blocks.12.hook_resid_post 32768 none layer_12/width_32k/average_l0_22Load this SAE jumprelu blocks.12.hook_resid_post 32768 none layer_12/width_32k/average_l0_360Load this SAE jumprelu blocks.12.hook_resid_post 32768 none layer_12/width_32k/average_l0_40Load this SAE jumprelu blocks.12.hook_resid_post 32768 none layer_12/width_32k/average_l0_76Load this SAE jumprelu blocks.12.hook_resid_post 32768 none layer_12/width_524k/average_l0_115Load this SAE jumprelu blocks.12.hook_resid_post 524288 none layer_12/width_524k/average_l0_22Load this SAE jumprelu blocks.12.hook_resid_post 524288 none layer_12/width_524k/average_l0_227Load this SAE jumprelu blocks.12.hook_resid_post 524288 none layer_12/width_524k/average_l0_29Load this SAE jumprelu blocks.12.hook_resid_post 524288 none layer_12/width_524k/average_l0_46Load this SAE jumprelu blocks.12.hook_resid_post 524288 none layer_12/width_524k/average_l0_65Load this SAE jumprelu blocks.12.hook_resid_post 524288 none layer_12/width_131k/average_l0_12Load this SAE jumprelu blocks.12.hook_resid_post 131072 none layer_12/width_131k/average_l0_129Load this SAE jumprelu blocks.12.hook_resid_post 131072 none layer_12/width_131k/average_l0_20Load this SAE jumprelu blocks.12.hook_resid_post 131072 none layer_12/width_131k/average_l0_264Load this SAE jumprelu blocks.12.hook_resid_post 131072 none layer_12/width_131k/average_l0_36Load this SAE jumprelu blocks.12.hook_resid_post 131072 none layer_12/width_131k/average_l0_67Load this SAE jumprelu blocks.12.hook_resid_post 131072 none layer_0/width_65k/average_l0_11Load this SAE jumprelu blocks.0.hook_resid_post 65536 none layer_0/width_65k/average_l0_17Load this SAE jumprelu blocks.0.hook_resid_post 65536 none layer_0/width_65k/average_l0_27Load this SAE jumprelu blocks.0.hook_resid_post 65536 none layer_0/width_65k/average_l0_43Load this SAE jumprelu blocks.0.hook_resid_post 65536 none layer_0/width_65k/average_l0_73Load this SAE jumprelu blocks.0.hook_resid_post 65536 none layer_1/width_65k/average_l0_121Load this SAE jumprelu blocks.1.hook_resid_post 65536 none layer_1/width_65k/average_l0_16Load this SAE jumprelu blocks.1.hook_resid_post 65536 none layer_1/width_65k/average_l0_30Load this SAE jumprelu blocks.1.hook_resid_post 65536 none layer_1/width_65k/average_l0_54Load this SAE jumprelu blocks.1.hook_resid_post 65536 none layer_1/width_65k/average_l0_9Load this SAE jumprelu blocks.1.hook_resid_post 65536 none layer_2/width_65k/average_l0_11Load this SAE jumprelu blocks.2.hook_resid_post 65536 none layer_2/width_65k/average_l0_169Load this SAE jumprelu blocks.2.hook_resid_post 65536 none layer_2/width_65k/average_l0_20Load this SAE jumprelu blocks.2.hook_resid_post 65536 none layer_2/width_65k/average_l0_37Load this SAE jumprelu blocks.2.hook_resid_post 65536 none layer_2/width_65k/average_l0_77Load this SAE jumprelu blocks.2.hook_resid_post 65536 none layer_3/width_65k/average_l0_13Load this SAE jumprelu blocks.3.hook_resid_post 65536 none layer_3/width_65k/average_l0_193Load this SAE jumprelu blocks.3.hook_resid_post 65536 none layer_3/width_65k/average_l0_23Load this SAE jumprelu blocks.3.hook_resid_post 65536 none layer_3/width_65k/average_l0_42Load this SAE jumprelu blocks.3.hook_resid_post 65536 none layer_3/width_65k/average_l0_89Load this SAE jumprelu blocks.3.hook_resid_post 65536 none layer_4/width_65k/average_l0_14Load this SAE jumprelu blocks.4.hook_resid_post 65536 none layer_4/width_65k/average_l0_177Load this SAE jumprelu blocks.4.hook_resid_post 65536 none layer_4/width_65k/average_l0_25Load this SAE jumprelu blocks.4.hook_resid_post 65536 none layer_4/width_65k/average_l0_46Load this SAE jumprelu blocks.4.hook_resid_post 65536 none layer_4/width_65k/average_l0_89Load this SAE jumprelu blocks.4.hook_resid_post 65536 none layer_5/width_65k/average_l0_105Load this SAE jumprelu blocks.5.hook_resid_post 65536 none layer_5/width_65k/average_l0_17Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-65k__l0-17 blocks.5.hook_resid_post 65536 none layer_5/width_65k/average_l0_211Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-65k__l0-211 blocks.5.hook_resid_post 65536 none layer_5/width_65k/average_l0_29Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-65k__l0-29 blocks.5.hook_resid_post 65536 none layer_5/width_65k/average_l0_53Load this SAE jumprelu gemma-2-2b/5-gemmascope-res-65k__l0-53 blocks.5.hook_resid_post 65536 none layer_6/width_65k/average_l0_107Load this SAE jumprelu blocks.6.hook_resid_post 65536 none layer_6/width_65k/average_l0_17Load this SAE jumprelu blocks.6.hook_resid_post 65536 none layer_6/width_65k/average_l0_208Load this SAE jumprelu blocks.6.hook_resid_post 65536 none layer_6/width_65k/average_l0_30Load this SAE jumprelu blocks.6.hook_resid_post 65536 none layer_6/width_65k/average_l0_56Load this SAE jumprelu blocks.6.hook_resid_post 65536 none layer_7/width_65k/average_l0_107Load this SAE jumprelu blocks.7.hook_resid_post 65536 none layer_7/width_65k/average_l0_18Load this SAE jumprelu blocks.7.hook_resid_post 65536 none layer_7/width_65k/average_l0_203Load this SAE jumprelu blocks.7.hook_resid_post 65536 none layer_7/width_65k/average_l0_31Load this SAE jumprelu blocks.7.hook_resid_post 65536 none layer_7/width_65k/average_l0_57Load this SAE jumprelu blocks.7.hook_resid_post 65536 none layer_8/width_65k/average_l0_111Load this SAE jumprelu blocks.8.hook_resid_post 65536 none layer_8/width_65k/average_l0_19Load this SAE jumprelu blocks.8.hook_resid_post 65536 none layer_8/width_65k/average_l0_213Load this SAE jumprelu blocks.8.hook_resid_post 65536 none layer_8/width_65k/average_l0_33Load this SAE jumprelu blocks.8.hook_resid_post 65536 none layer_8/width_65k/average_l0_59Load this SAE jumprelu blocks.8.hook_resid_post 65536 none layer_9/width_65k/average_l0_118Load this SAE jumprelu blocks.9.hook_resid_post 65536 none layer_9/width_65k/average_l0_19Load this SAE jumprelu blocks.9.hook_resid_post 65536 none layer_9/width_65k/average_l0_240Load this SAE jumprelu blocks.9.hook_resid_post 65536 none layer_9/width_65k/average_l0_34Load this SAE jumprelu blocks.9.hook_resid_post 65536 none layer_9/width_65k/average_l0_61Load this SAE jumprelu blocks.9.hook_resid_post 65536 none layer_10/width_65k/average_l0_128Load this SAE jumprelu blocks.10.hook_resid_post 65536 none layer_10/width_65k/average_l0_20Load this SAE jumprelu blocks.10.hook_resid_post 65536 none layer_10/width_65k/average_l0_265Load this SAE jumprelu blocks.10.hook_resid_post 65536 none layer_10/width_65k/average_l0_36Load this SAE jumprelu blocks.10.hook_resid_post 65536 none layer_10/width_65k/average_l0_66Load this SAE jumprelu blocks.10.hook_resid_post 65536 none layer_11/width_65k/average_l0_134Load this SAE jumprelu blocks.11.hook_resid_post 65536 none layer_11/width_65k/average_l0_21Load this SAE jumprelu blocks.11.hook_resid_post 65536 none layer_11/width_65k/average_l0_273Load this SAE jumprelu blocks.11.hook_resid_post 65536 none layer_11/width_65k/average_l0_37Load this SAE jumprelu blocks.11.hook_resid_post 65536 none layer_11/width_65k/average_l0_70Load this SAE jumprelu blocks.11.hook_resid_post 65536 none layer_12/width_65k/average_l0_141Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-65k__l0-141 blocks.12.hook_resid_post 65536 none layer_12/width_65k/average_l0_21Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-65k__l0-21 blocks.12.hook_resid_post 65536 none layer_12/width_65k/average_l0_297Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-65k__l0-297 blocks.12.hook_resid_post 65536 none layer_12/width_65k/average_l0_38Load this SAE jumprelu gemma-2-2b/12-gemmascope-res-65k__l0-38 blocks.12.hook_resid_post 65536 none layer_12/width_65k/average_l0_72Load this SAE jumprelu blocks.12.hook_resid_post 65536 none layer_13/width_65k/average_l0_142Load this SAE jumprelu blocks.13.hook_resid_post 65536 none layer_13/width_65k/average_l0_22Load this SAE jumprelu blocks.13.hook_resid_post 65536 none layer_13/width_65k/average_l0_288Load this SAE jumprelu blocks.13.hook_resid_post 65536 none layer_13/width_65k/average_l0_40Load this SAE jumprelu blocks.13.hook_resid_post 65536 none layer_13/width_65k/average_l0_74Load this SAE jumprelu blocks.13.hook_resid_post 65536 none layer_13/width_65k/average_l0_75Load this SAE jumprelu blocks.13.hook_resid_post 65536 none layer_14/width_65k/average_l0_144Load this SAE jumprelu blocks.14.hook_resid_post 65536 none layer_14/width_65k/average_l0_21Load this SAE jumprelu blocks.14.hook_resid_post 65536 none layer_14/width_65k/average_l0_284Load this SAE jumprelu blocks.14.hook_resid_post 65536 none layer_14/width_65k/average_l0_40Load this SAE jumprelu blocks.14.hook_resid_post 65536 none layer_14/width_65k/average_l0_73Load this SAE jumprelu blocks.14.hook_resid_post 65536 none layer_15/width_65k/average_l0_127Load this SAE jumprelu blocks.15.hook_resid_post 65536 none layer_15/width_65k/average_l0_21Load this SAE jumprelu blocks.15.hook_resid_post 65536 none layer_15/width_65k/average_l0_240Load this SAE jumprelu blocks.15.hook_resid_post 65536 none layer_15/width_65k/average_l0_38Load this SAE jumprelu blocks.15.hook_resid_post 65536 none layer_15/width_65k/average_l0_68Load this SAE jumprelu blocks.15.hook_resid_post 65536 none layer_16/width_65k/average_l0_128Load this SAE jumprelu blocks.16.hook_resid_post 65536 none layer_16/width_65k/average_l0_21Load this SAE jumprelu blocks.16.hook_resid_post 65536 none layer_16/width_65k/average_l0_244Load this SAE jumprelu blocks.16.hook_resid_post 65536 none layer_16/width_65k/average_l0_38Load this SAE jumprelu blocks.16.hook_resid_post 65536 none layer_16/width_65k/average_l0_69Load this SAE jumprelu blocks.16.hook_resid_post 65536 none layer_17/width_65k/average_l0_125Load this SAE jumprelu blocks.17.hook_resid_post 65536 none layer_17/width_65k/average_l0_21Load this SAE jumprelu blocks.17.hook_resid_post 65536 none layer_17/width_65k/average_l0_233Load this SAE jumprelu blocks.17.hook_resid_post 65536 none layer_17/width_65k/average_l0_38Load this SAE jumprelu blocks.17.hook_resid_post 65536 none layer_17/width_65k/average_l0_68Load this SAE jumprelu blocks.17.hook_resid_post 65536 none layer_18/width_65k/average_l0_116Load this SAE jumprelu blocks.18.hook_resid_post 65536 none layer_18/width_65k/average_l0_117Load this SAE jumprelu blocks.18.hook_resid_post 65536 none layer_18/width_65k/average_l0_21Load this SAE jumprelu blocks.18.hook_resid_post 65536 none layer_18/width_65k/average_l0_216Load this SAE jumprelu blocks.18.hook_resid_post 65536 none layer_18/width_65k/average_l0_36Load this SAE jumprelu blocks.18.hook_resid_post 65536 none layer_18/width_65k/average_l0_64Load this SAE jumprelu blocks.18.hook_resid_post 65536 none layer_19/width_65k/average_l0_115Load this SAE jumprelu blocks.19.hook_resid_post 65536 none layer_19/width_65k/average_l0_21Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-65k__l0-21 blocks.19.hook_resid_post 65536 none layer_19/width_65k/average_l0_216Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-65k__l0-216 blocks.19.hook_resid_post 65536 none layer_19/width_65k/average_l0_35Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-65k__l0-35 blocks.19.hook_resid_post 65536 none layer_19/width_65k/average_l0_63Load this SAE jumprelu gemma-2-2b/19-gemmascope-res-65k__l0-63 blocks.19.hook_resid_post 65536 none layer_20/width_65k/average_l0_114Load this SAE jumprelu blocks.20.hook_resid_post 65536 none layer_20/width_65k/average_l0_20Load this SAE jumprelu blocks.20.hook_resid_post 65536 none layer_20/width_65k/average_l0_221Load this SAE jumprelu blocks.20.hook_resid_post 65536 none layer_20/width_65k/average_l0_34Load this SAE jumprelu blocks.20.hook_resid_post 65536 none layer_20/width_65k/average_l0_61Load this SAE jumprelu blocks.20.hook_resid_post 65536 none layer_21/width_65k/average_l0_111Load this SAE jumprelu blocks.21.hook_resid_post 65536 none layer_21/width_65k/average_l0_112Load this SAE jumprelu blocks.21.hook_resid_post 65536 none layer_21/width_65k/average_l0_20Load this SAE jumprelu blocks.21.hook_resid_post 65536 none layer_21/width_65k/average_l0_225Load this SAE jumprelu blocks.21.hook_resid_post 65536 none layer_21/width_65k/average_l0_33Load this SAE jumprelu blocks.21.hook_resid_post 65536 none layer_21/width_65k/average_l0_61Load this SAE jumprelu blocks.21.hook_resid_post 65536 none layer_22/width_65k/average_l0_116Load this SAE jumprelu blocks.22.hook_resid_post 65536 none layer_22/width_65k/average_l0_117Load this SAE jumprelu blocks.22.hook_resid_post 65536 none layer_22/width_65k/average_l0_20Load this SAE jumprelu blocks.22.hook_resid_post 65536 none layer_22/width_65k/average_l0_248Load this SAE jumprelu blocks.22.hook_resid_post 65536 none layer_22/width_65k/average_l0_33Load this SAE jumprelu blocks.22.hook_resid_post 65536 none layer_22/width_65k/average_l0_62Load this SAE jumprelu blocks.22.hook_resid_post 65536 none layer_23/width_65k/average_l0_123Load this SAE jumprelu blocks.23.hook_resid_post 65536 none layer_23/width_65k/average_l0_124Load this SAE jumprelu blocks.23.hook_resid_post 65536 none layer_23/width_65k/average_l0_20Load this SAE jumprelu blocks.23.hook_resid_post 65536 none layer_23/width_65k/average_l0_272Load this SAE jumprelu blocks.23.hook_resid_post 65536 none layer_23/width_65k/average_l0_35Load this SAE jumprelu blocks.23.hook_resid_post 65536 none layer_23/width_65k/average_l0_64Load this SAE jumprelu blocks.23.hook_resid_post 65536 none layer_24/width_65k/average_l0_124Load this SAE jumprelu blocks.24.hook_resid_post 65536 none layer_24/width_65k/average_l0_19Load this SAE jumprelu blocks.24.hook_resid_post 65536 none layer_24/width_65k/average_l0_273Load this SAE jumprelu blocks.24.hook_resid_post 65536 none layer_24/width_65k/average_l0_34Load this SAE jumprelu blocks.24.hook_resid_post 65536 none layer_24/width_65k/average_l0_63Load this SAE jumprelu blocks.24.hook_resid_post 65536 none layer_25/width_65k/average_l0_15Load this SAE jumprelu blocks.25.hook_resid_post 65536 none layer_25/width_65k/average_l0_197Load this SAE jumprelu blocks.25.hook_resid_post 65536 none layer_25/width_65k/average_l0_26Load this SAE jumprelu blocks.25.hook_resid_post 65536 none layer_25/width_65k/average_l0_48Load this SAE jumprelu blocks.25.hook_resid_post 65536 none layer_25/width_65k/average_l0_93Load this SAE jumprelu blocks.25.hook_resid_post 65536 none"},{"location":"sae_table/#gemma-scope-2b-pt-res-canonical","title":"gemma-scope-2b-pt-res-canonical","text":"<ul> <li>Huggingface Repo: google/gemma-scope-2b-pt-res</li> <li>model: gemma-2-2b</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> <li>Publication</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/0-gemmascope-res-16k blocks.0.hook_resid_post 16384 none layer_1/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/1-gemmascope-res-16k blocks.1.hook_resid_post 16384 none layer_2/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/2-gemmascope-res-16k blocks.2.hook_resid_post 16384 none layer_3/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/3-gemmascope-res-16k blocks.3.hook_resid_post 16384 none layer_4/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/4-gemmascope-res-16k blocks.4.hook_resid_post 16384 none layer_5/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/5-gemmascope-res-16k blocks.5.hook_resid_post 16384 none layer_6/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/6-gemmascope-res-16k blocks.6.hook_resid_post 16384 none layer_7/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/7-gemmascope-res-16k blocks.7.hook_resid_post 16384 none layer_8/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/8-gemmascope-res-16k blocks.8.hook_resid_post 16384 none layer_9/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/9-gemmascope-res-16k blocks.9.hook_resid_post 16384 none layer_10/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/10-gemmascope-res-16k blocks.10.hook_resid_post 16384 none layer_11/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/11-gemmascope-res-16k blocks.11.hook_resid_post 16384 none layer_12/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/12-gemmascope-res-16k blocks.12.hook_resid_post 16384 none layer_13/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/13-gemmascope-res-16k blocks.13.hook_resid_post 16384 none layer_14/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/14-gemmascope-res-16k blocks.14.hook_resid_post 16384 none layer_15/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/15-gemmascope-res-16k blocks.15.hook_resid_post 16384 none layer_16/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/16-gemmascope-res-16k blocks.16.hook_resid_post 16384 none layer_17/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/17-gemmascope-res-16k blocks.17.hook_resid_post 16384 none layer_18/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/18-gemmascope-res-16k blocks.18.hook_resid_post 16384 none layer_19/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/19-gemmascope-res-16k blocks.19.hook_resid_post 16384 none layer_20/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/20-gemmascope-res-16k blocks.20.hook_resid_post 16384 none layer_21/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/21-gemmascope-res-16k blocks.21.hook_resid_post 16384 none layer_22/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/22-gemmascope-res-16k blocks.22.hook_resid_post 16384 none layer_23/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/23-gemmascope-res-16k blocks.23.hook_resid_post 16384 none layer_24/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/24-gemmascope-res-16k blocks.24.hook_resid_post 16384 none layer_25/width_16k/canonicalLoad this SAE jumprelu gemma-2-2b/25-gemmascope-res-16k blocks.25.hook_resid_post 16384 none layer_5/width_1m/canonicalLoad this SAE jumprelu gemma-2-2b/5-gemmascope-res-1m blocks.5.hook_resid_post 1048576 none layer_12/width_1m/canonicalLoad this SAE jumprelu gemma-2-2b/12-gemmascope-res-1m blocks.12.hook_resid_post 1048576 none layer_19/width_1m/canonicalLoad this SAE jumprelu gemma-2-2b/19-gemmascope-res-1m blocks.19.hook_resid_post 1048576 none layer_12/width_262k/canonicalLoad this SAE jumprelu gemma-2-2b/12-gemmascope-res-262k blocks.12.hook_resid_post 262144 none layer_12/width_32k/canonicalLoad this SAE jumprelu gemma-2-2b/12-gemmascope-res-32k blocks.12.hook_resid_post 32768 none layer_12/width_524k/canonicalLoad this SAE jumprelu gemma-2-2b/12-gemmascope-res-524k blocks.12.hook_resid_post 524288 none layer_0/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/0-gemmascope-res-65k blocks.0.hook_resid_post 65536 none layer_1/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/1-gemmascope-res-65k blocks.1.hook_resid_post 65536 none layer_2/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/2-gemmascope-res-65k blocks.2.hook_resid_post 65536 none layer_3/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/3-gemmascope-res-65k blocks.3.hook_resid_post 65536 none layer_4/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/4-gemmascope-res-65k blocks.4.hook_resid_post 65536 none layer_5/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/5-gemmascope-res-65k blocks.5.hook_resid_post 65536 none layer_6/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/6-gemmascope-res-65k blocks.6.hook_resid_post 65536 none layer_7/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/7-gemmascope-res-65k blocks.7.hook_resid_post 65536 none layer_8/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/8-gemmascope-res-65k blocks.8.hook_resid_post 65536 none layer_9/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/9-gemmascope-res-65k blocks.9.hook_resid_post 65536 none layer_10/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/10-gemmascope-res-65k blocks.10.hook_resid_post 65536 none layer_11/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/11-gemmascope-res-65k blocks.11.hook_resid_post 65536 none layer_12/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/12-gemmascope-res-65k blocks.12.hook_resid_post 65536 none layer_13/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/13-gemmascope-res-65k blocks.13.hook_resid_post 65536 none layer_14/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/14-gemmascope-res-65k blocks.14.hook_resid_post 65536 none layer_15/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/15-gemmascope-res-65k blocks.15.hook_resid_post 65536 none layer_16/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/16-gemmascope-res-65k blocks.16.hook_resid_post 65536 none layer_17/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/17-gemmascope-res-65k blocks.17.hook_resid_post 65536 none layer_18/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/18-gemmascope-res-65k blocks.18.hook_resid_post 65536 none layer_19/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/19-gemmascope-res-65k blocks.19.hook_resid_post 65536 none layer_20/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/20-gemmascope-res-65k blocks.20.hook_resid_post 65536 none layer_21/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/21-gemmascope-res-65k blocks.21.hook_resid_post 65536 none layer_22/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/22-gemmascope-res-65k blocks.22.hook_resid_post 65536 none layer_23/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/23-gemmascope-res-65k blocks.23.hook_resid_post 65536 none layer_24/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/24-gemmascope-res-65k blocks.24.hook_resid_post 65536 none layer_25/width_65k/canonicalLoad this SAE jumprelu gemma-2-2b/25-gemmascope-res-65k blocks.25.hook_resid_post 65536 none"},{"location":"sae_table/#gemma-scope-9b-it-res","title":"gemma-scope-9b-it-res","text":"<ul> <li>Huggingface Repo: google/gemma-scope-9b-it-res</li> <li>model: gemma-2-9b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_20/width_131k/average_l0_13Load this SAE jumprelu blocks.20.hook_resid_post 131072 none layer_20/width_131k/average_l0_153Load this SAE jumprelu blocks.20.hook_resid_post 131072 none layer_20/width_131k/average_l0_24Load this SAE jumprelu blocks.20.hook_resid_post 131072 none layer_20/width_131k/average_l0_43Load this SAE jumprelu blocks.20.hook_resid_post 131072 none layer_20/width_131k/average_l0_81Load this SAE jumprelu blocks.20.hook_resid_post 131072 none layer_20/width_16k/average_l0_14Load this SAE jumprelu blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_189Load this SAE jumprelu blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_25Load this SAE jumprelu blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_47Load this SAE jumprelu blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_91Load this SAE jumprelu blocks.20.hook_resid_post 16384 none layer_31/width_131k/average_l0_109Load this SAE jumprelu blocks.31.hook_resid_post 131072 none layer_31/width_131k/average_l0_13Load this SAE jumprelu blocks.31.hook_resid_post 131072 none layer_31/width_131k/average_l0_22Load this SAE jumprelu blocks.31.hook_resid_post 131072 none layer_31/width_131k/average_l0_37Load this SAE jumprelu blocks.31.hook_resid_post 131072 none layer_31/width_131k/average_l0_63Load this SAE jumprelu blocks.31.hook_resid_post 131072 none layer_31/width_16k/average_l0_14Load this SAE jumprelu blocks.31.hook_resid_post 16384 none layer_31/width_16k/average_l0_142Load this SAE jumprelu blocks.31.hook_resid_post 16384 none layer_31/width_16k/average_l0_24Load this SAE jumprelu blocks.31.hook_resid_post 16384 none layer_31/width_16k/average_l0_43Load this SAE jumprelu blocks.31.hook_resid_post 16384 none layer_31/width_16k/average_l0_76Load this SAE jumprelu blocks.31.hook_resid_post 16384 none layer_9/width_131k/average_l0_121Load this SAE jumprelu blocks.9.hook_resid_post 131072 none layer_9/width_131k/average_l0_13Load this SAE jumprelu blocks.9.hook_resid_post 131072 none layer_9/width_131k/average_l0_22Load this SAE jumprelu blocks.9.hook_resid_post 131072 none layer_9/width_131k/average_l0_39Load this SAE jumprelu blocks.9.hook_resid_post 131072 none layer_9/width_131k/average_l0_67Load this SAE jumprelu blocks.9.hook_resid_post 131072 none layer_9/width_16k/average_l0_14Load this SAE jumprelu blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_186Load this SAE jumprelu blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_26Load this SAE jumprelu blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_47Load this SAE jumprelu blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_88Load this SAE jumprelu blocks.9.hook_resid_post 16384 none"},{"location":"sae_table/#gemma-scope-9b-it-res-canonical","title":"gemma-scope-9b-it-res-canonical","text":"<ul> <li>Huggingface Repo: google/gemma-scope-9b-it-res</li> <li>model: gemma-2-9b-it</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_9/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b-it/9-gemmascope-res-16k blocks.9.hook_resid_post 16384 none layer_20/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b-it/20-gemmascope-res-16k blocks.20.hook_resid_post 16384 none layer_31/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b-it/31-gemmascope-res-16k blocks.31.hook_resid_post 16384 none layer_9/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b-it/9-gemmascope-res-131k blocks.9.hook_resid_post 131072 none layer_20/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b-it/20-gemmascope-res-131k blocks.20.hook_resid_post 131072 none layer_31/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b-it/31-gemmascope-res-131k blocks.31.hook_resid_post 131072 none"},{"location":"sae_table/#gemma-scope-9b-pt-att","title":"gemma-scope-9b-pt-att","text":"<ul> <li>Huggingface Repo: google/gemma-scope-9b-pt-att</li> <li>model: gemma-2-9b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0/width_16k/average_l0_12Load this SAE jumprelu blocks.0.attn.hook_z 16384 none layer_0/width_16k/average_l0_16Load this SAE jumprelu blocks.0.attn.hook_z 16384 none layer_0/width_16k/average_l0_25Load this SAE jumprelu blocks.0.attn.hook_z 16384 none layer_0/width_16k/average_l0_38Load this SAE jumprelu blocks.0.attn.hook_z 16384 none layer_0/width_16k/average_l0_61Load this SAE jumprelu blocks.0.attn.hook_z 16384 none layer_0/width_131k/average_l0_8Load this SAE jumprelu blocks.0.attn.hook_z 131072 none layer_0/width_131k/average_l0_11Load this SAE jumprelu blocks.0.attn.hook_z 131072 none layer_0/width_131k/average_l0_15Load this SAE jumprelu blocks.0.attn.hook_z 131072 none layer_0/width_131k/average_l0_22Load this SAE jumprelu blocks.0.attn.hook_z 131072 none layer_0/width_131k/average_l0_33Load this SAE jumprelu blocks.0.attn.hook_z 131072 none layer_0/width_131k/average_l0_55Load this SAE jumprelu blocks.0.attn.hook_z 131072 none layer_1/width_16k/average_l0_17Load this SAE jumprelu blocks.1.attn.hook_z 16384 none layer_1/width_16k/average_l0_34Load this SAE jumprelu blocks.1.attn.hook_z 16384 none layer_1/width_16k/average_l0_77Load this SAE jumprelu blocks.1.attn.hook_z 16384 none layer_1/width_16k/average_l0_147Load this SAE jumprelu blocks.1.attn.hook_z 16384 none layer_1/width_16k/average_l0_266Load this SAE jumprelu blocks.1.attn.hook_z 16384 none layer_1/width_131k/average_l0_8Load this SAE jumprelu blocks.1.attn.hook_z 131072 none layer_1/width_131k/average_l0_12Load this SAE jumprelu blocks.1.attn.hook_z 131072 none layer_1/width_131k/average_l0_18Load this SAE jumprelu blocks.1.attn.hook_z 131072 none layer_1/width_131k/average_l0_29Load this SAE jumprelu blocks.1.attn.hook_z 131072 none layer_1/width_131k/average_l0_63Load this SAE jumprelu blocks.1.attn.hook_z 131072 none layer_1/width_131k/average_l0_116Load this SAE jumprelu blocks.1.attn.hook_z 131072 none layer_10/width_16k/average_l0_18Load this SAE jumprelu blocks.10.attn.hook_z 16384 none layer_10/width_16k/average_l0_33Load this SAE jumprelu blocks.10.attn.hook_z 16384 none layer_10/width_16k/average_l0_63Load this SAE jumprelu blocks.10.attn.hook_z 16384 none layer_10/width_16k/average_l0_132Load this SAE jumprelu blocks.10.attn.hook_z 16384 none layer_10/width_16k/average_l0_301Load this SAE jumprelu blocks.10.attn.hook_z 16384 none layer_10/width_131k/average_l0_16Load this SAE jumprelu blocks.10.attn.hook_z 131072 none layer_10/width_131k/average_l0_28Load this SAE jumprelu blocks.10.attn.hook_z 131072 none layer_10/width_131k/average_l0_51Load this SAE jumprelu blocks.10.attn.hook_z 131072 none layer_10/width_131k/average_l0_97Load this SAE jumprelu blocks.10.attn.hook_z 131072 none layer_10/width_131k/average_l0_199Load this SAE jumprelu blocks.10.attn.hook_z 131072 none layer_10/width_131k/average_l0_440Load this SAE jumprelu blocks.10.attn.hook_z 131072 none layer_11/width_16k/average_l0_18Load this SAE jumprelu blocks.11.attn.hook_z 16384 none layer_11/width_16k/average_l0_34Load this SAE jumprelu blocks.11.attn.hook_z 16384 none layer_11/width_16k/average_l0_67Load this SAE jumprelu blocks.11.attn.hook_z 16384 none layer_11/width_16k/average_l0_153Load this SAE jumprelu blocks.11.attn.hook_z 16384 none layer_11/width_16k/average_l0_366Load this SAE jumprelu blocks.11.attn.hook_z 16384 none layer_11/width_131k/average_l0_17Load this SAE jumprelu blocks.11.attn.hook_z 131072 none layer_11/width_131k/average_l0_29Load this SAE jumprelu blocks.11.attn.hook_z 131072 none layer_11/width_131k/average_l0_54Load this SAE jumprelu blocks.11.attn.hook_z 131072 none layer_11/width_131k/average_l0_104Load this SAE jumprelu blocks.11.attn.hook_z 131072 none layer_11/width_131k/average_l0_241Load this SAE jumprelu blocks.11.attn.hook_z 131072 none layer_11/width_131k/average_l0_552Load this SAE jumprelu blocks.11.attn.hook_z 131072 none layer_12/width_16k/average_l0_19Load this SAE jumprelu blocks.12.attn.hook_z 16384 none layer_12/width_16k/average_l0_35Load this SAE jumprelu blocks.12.attn.hook_z 16384 none layer_12/width_16k/average_l0_68Load this SAE jumprelu blocks.12.attn.hook_z 16384 none layer_12/width_16k/average_l0_149Load this SAE jumprelu blocks.12.attn.hook_z 16384 none layer_12/width_16k/average_l0_337Load this SAE jumprelu blocks.12.attn.hook_z 16384 none layer_12/width_16k/average_l0_654Load this SAE jumprelu blocks.12.attn.hook_z 16384 none layer_12/width_131k/average_l0_17Load this SAE jumprelu blocks.12.attn.hook_z 131072 none layer_12/width_131k/average_l0_29Load this SAE jumprelu blocks.12.attn.hook_z 131072 none layer_12/width_131k/average_l0_55Load this SAE jumprelu blocks.12.attn.hook_z 131072 none layer_12/width_131k/average_l0_110Load this SAE jumprelu blocks.12.attn.hook_z 131072 none layer_12/width_131k/average_l0_237Load this SAE jumprelu blocks.12.attn.hook_z 131072 none layer_12/width_131k/average_l0_525Load this SAE jumprelu blocks.12.attn.hook_z 131072 none layer_13/width_16k/average_l0_20Load this SAE jumprelu blocks.13.attn.hook_z 16384 none layer_13/width_16k/average_l0_38Load this SAE jumprelu blocks.13.attn.hook_z 16384 none layer_13/width_16k/average_l0_77Load this SAE jumprelu blocks.13.attn.hook_z 16384 none layer_13/width_16k/average_l0_170Load this SAE jumprelu blocks.13.attn.hook_z 16384 none layer_13/width_16k/average_l0_380Load this SAE jumprelu blocks.13.attn.hook_z 16384 none layer_13/width_16k/average_l0_675Load this SAE jumprelu blocks.13.attn.hook_z 16384 none layer_13/width_131k/average_l0_18Load this SAE jumprelu blocks.13.attn.hook_z 131072 none layer_13/width_131k/average_l0_33Load this SAE jumprelu blocks.13.attn.hook_z 131072 none layer_13/width_131k/average_l0_64Load this SAE jumprelu blocks.13.attn.hook_z 131072 none layer_13/width_131k/average_l0_126Load this SAE jumprelu blocks.13.attn.hook_z 131072 none layer_13/width_131k/average_l0_283Load this SAE jumprelu blocks.13.attn.hook_z 131072 none layer_13/width_131k/average_l0_611Load this SAE jumprelu blocks.13.attn.hook_z 131072 none layer_14/width_16k/average_l0_21Load this SAE jumprelu blocks.14.attn.hook_z 16384 none layer_14/width_16k/average_l0_40Load this SAE jumprelu blocks.14.attn.hook_z 16384 none layer_14/width_16k/average_l0_81Load this SAE jumprelu blocks.14.attn.hook_z 16384 none layer_14/width_16k/average_l0_179Load this SAE jumprelu blocks.14.attn.hook_z 16384 none layer_14/width_16k/average_l0_411Load this SAE jumprelu blocks.14.attn.hook_z 16384 none layer_14/width_16k/average_l0_767Load this SAE jumprelu blocks.14.attn.hook_z 16384 none layer_14/width_131k/average_l0_18Load this SAE jumprelu blocks.14.attn.hook_z 131072 none layer_14/width_131k/average_l0_35Load this SAE jumprelu blocks.14.attn.hook_z 131072 none layer_14/width_131k/average_l0_67Load this SAE jumprelu blocks.14.attn.hook_z 131072 none layer_14/width_131k/average_l0_131Load this SAE jumprelu blocks.14.attn.hook_z 131072 none layer_14/width_131k/average_l0_306Load this SAE jumprelu blocks.14.attn.hook_z 131072 none layer_14/width_131k/average_l0_650Load this SAE jumprelu blocks.14.attn.hook_z 131072 none layer_15/width_16k/average_l0_21Load this SAE jumprelu blocks.15.attn.hook_z 16384 none layer_15/width_16k/average_l0_40Load this SAE jumprelu blocks.15.attn.hook_z 16384 none layer_15/width_16k/average_l0_79Load this SAE jumprelu blocks.15.attn.hook_z 16384 none layer_15/width_16k/average_l0_168Load this SAE jumprelu blocks.15.attn.hook_z 16384 none layer_15/width_16k/average_l0_344Load this SAE jumprelu blocks.15.attn.hook_z 16384 none layer_15/width_16k/average_l0_638Load this SAE jumprelu blocks.15.attn.hook_z 16384 none layer_15/width_131k/average_l0_19Load this SAE jumprelu blocks.15.attn.hook_z 131072 none layer_15/width_131k/average_l0_35Load this SAE jumprelu blocks.15.attn.hook_z 131072 none layer_15/width_131k/average_l0_67Load this SAE jumprelu blocks.15.attn.hook_z 131072 none layer_15/width_131k/average_l0_130Load this SAE jumprelu blocks.15.attn.hook_z 131072 none layer_15/width_131k/average_l0_283Load this SAE jumprelu blocks.15.attn.hook_z 131072 none layer_15/width_131k/average_l0_540Load this SAE jumprelu blocks.15.attn.hook_z 131072 none layer_16/width_16k/average_l0_21Load this SAE jumprelu blocks.16.attn.hook_z 16384 none layer_16/width_16k/average_l0_40Load this SAE jumprelu blocks.16.attn.hook_z 16384 none layer_16/width_16k/average_l0_81Load this SAE jumprelu blocks.16.attn.hook_z 16384 none layer_16/width_16k/average_l0_172Load this SAE jumprelu blocks.16.attn.hook_z 16384 none layer_16/width_16k/average_l0_376Load this SAE jumprelu blocks.16.attn.hook_z 16384 none layer_16/width_16k/average_l0_705Load this SAE jumprelu blocks.16.attn.hook_z 16384 none layer_16/width_131k/average_l0_19Load this SAE jumprelu blocks.16.attn.hook_z 131072 none layer_16/width_131k/average_l0_36Load this SAE jumprelu blocks.16.attn.hook_z 131072 none layer_16/width_131k/average_l0_71Load this SAE jumprelu blocks.16.attn.hook_z 131072 none layer_16/width_131k/average_l0_140Load this SAE jumprelu blocks.16.attn.hook_z 131072 none layer_16/width_131k/average_l0_298Load this SAE jumprelu blocks.16.attn.hook_z 131072 none layer_16/width_131k/average_l0_621Load this SAE jumprelu blocks.16.attn.hook_z 131072 none layer_17/width_16k/average_l0_24Load this SAE jumprelu blocks.17.attn.hook_z 16384 none layer_17/width_16k/average_l0_50Load this SAE jumprelu blocks.17.attn.hook_z 16384 none layer_17/width_16k/average_l0_110Load this SAE jumprelu blocks.17.attn.hook_z 16384 none layer_17/width_16k/average_l0_227Load this SAE jumprelu blocks.17.attn.hook_z 16384 none layer_17/width_16k/average_l0_435Load this SAE jumprelu blocks.17.attn.hook_z 16384 none layer_17/width_16k/average_l0_719Load this SAE jumprelu blocks.17.attn.hook_z 16384 none layer_17/width_131k/average_l0_22Load this SAE jumprelu blocks.17.attn.hook_z 131072 none layer_17/width_131k/average_l0_44Load this SAE jumprelu blocks.17.attn.hook_z 131072 none layer_17/width_131k/average_l0_90Load this SAE jumprelu blocks.17.attn.hook_z 131072 none layer_17/width_131k/average_l0_191Load this SAE jumprelu blocks.17.attn.hook_z 131072 none layer_17/width_131k/average_l0_380Load this SAE jumprelu blocks.17.attn.hook_z 131072 none layer_17/width_131k/average_l0_672Load this SAE jumprelu blocks.17.attn.hook_z 131072 none layer_18/width_16k/average_l0_21Load this SAE jumprelu blocks.18.attn.hook_z 16384 none layer_18/width_16k/average_l0_40Load this SAE jumprelu blocks.18.attn.hook_z 16384 none layer_18/width_16k/average_l0_80Load this SAE jumprelu blocks.18.attn.hook_z 16384 none layer_18/width_16k/average_l0_171Load this SAE jumprelu blocks.18.attn.hook_z 16384 none layer_18/width_16k/average_l0_352Load this SAE jumprelu blocks.18.attn.hook_z 16384 none layer_18/width_16k/average_l0_646Load this SAE jumprelu blocks.18.attn.hook_z 16384 none layer_18/width_131k/average_l0_19Load this SAE jumprelu blocks.18.attn.hook_z 131072 none layer_18/width_131k/average_l0_35Load this SAE jumprelu blocks.18.attn.hook_z 131072 none layer_18/width_131k/average_l0_69Load this SAE jumprelu blocks.18.attn.hook_z 131072 none layer_18/width_131k/average_l0_133Load this SAE jumprelu blocks.18.attn.hook_z 131072 none layer_18/width_131k/average_l0_294Load this SAE jumprelu blocks.18.attn.hook_z 131072 none layer_18/width_131k/average_l0_557Load this SAE jumprelu blocks.18.attn.hook_z 131072 none layer_19/width_16k/average_l0_21Load this SAE jumprelu blocks.19.attn.hook_z 16384 none layer_19/width_16k/average_l0_41Load this SAE jumprelu blocks.19.attn.hook_z 16384 none layer_19/width_16k/average_l0_86Load this SAE jumprelu blocks.19.attn.hook_z 16384 none layer_19/width_16k/average_l0_186Load this SAE jumprelu blocks.19.attn.hook_z 16384 none layer_19/width_16k/average_l0_360Load this SAE jumprelu blocks.19.attn.hook_z 16384 none layer_19/width_16k/average_l0_661Load this SAE jumprelu blocks.19.attn.hook_z 16384 none layer_19/width_131k/average_l0_19Load this SAE jumprelu blocks.19.attn.hook_z 131072 none layer_19/width_131k/average_l0_38Load this SAE jumprelu blocks.19.attn.hook_z 131072 none layer_19/width_131k/average_l0_71Load this SAE jumprelu blocks.19.attn.hook_z 131072 none layer_19/width_131k/average_l0_152Load this SAE jumprelu blocks.19.attn.hook_z 131072 none layer_19/width_131k/average_l0_307Load this SAE jumprelu blocks.19.attn.hook_z 131072 none layer_19/width_131k/average_l0_571Load this SAE jumprelu blocks.19.attn.hook_z 131072 none layer_2/width_16k/average_l0_15Load this SAE jumprelu blocks.2.attn.hook_z 16384 none layer_2/width_16k/average_l0_30Load this SAE jumprelu blocks.2.attn.hook_z 16384 none layer_2/width_16k/average_l0_69Load this SAE jumprelu blocks.2.attn.hook_z 16384 none layer_2/width_16k/average_l0_180Load this SAE jumprelu blocks.2.attn.hook_z 16384 none layer_2/width_16k/average_l0_384Load this SAE jumprelu blocks.2.attn.hook_z 16384 none layer_2/width_131k/average_l0_7Load this SAE jumprelu blocks.2.attn.hook_z 131072 none layer_2/width_131k/average_l0_11Load this SAE jumprelu blocks.2.attn.hook_z 131072 none layer_2/width_131k/average_l0_18Load this SAE jumprelu blocks.2.attn.hook_z 131072 none layer_2/width_131k/average_l0_32Load this SAE jumprelu blocks.2.attn.hook_z 131072 none layer_2/width_131k/average_l0_62Load this SAE jumprelu blocks.2.attn.hook_z 131072 none layer_2/width_131k/average_l0_135Load this SAE jumprelu blocks.2.attn.hook_z 131072 none layer_20/width_16k/average_l0_20Load this SAE jumprelu blocks.20.attn.hook_z 16384 none layer_20/width_16k/average_l0_39Load this SAE jumprelu blocks.20.attn.hook_z 16384 none layer_20/width_16k/average_l0_76Load this SAE jumprelu blocks.20.attn.hook_z 16384 none layer_20/width_16k/average_l0_158Load this SAE jumprelu blocks.20.attn.hook_z 16384 none layer_20/width_16k/average_l0_342Load this SAE jumprelu blocks.20.attn.hook_z 16384 none layer_20/width_16k/average_l0_674Load this SAE jumprelu blocks.20.attn.hook_z 16384 none layer_20/width_131k/average_l0_18Load this SAE jumprelu blocks.20.attn.hook_z 131072 none layer_20/width_131k/average_l0_34Load this SAE jumprelu blocks.20.attn.hook_z 131072 none layer_20/width_131k/average_l0_65Load this SAE jumprelu blocks.20.attn.hook_z 131072 none layer_20/width_131k/average_l0_125Load this SAE jumprelu blocks.20.attn.hook_z 131072 none layer_20/width_131k/average_l0_270Load this SAE jumprelu blocks.20.attn.hook_z 131072 none layer_20/width_131k/average_l0_558Load this SAE jumprelu blocks.20.attn.hook_z 131072 none layer_21/width_16k/average_l0_21Load this SAE jumprelu blocks.21.attn.hook_z 16384 none layer_21/width_16k/average_l0_41Load this SAE jumprelu blocks.21.attn.hook_z 16384 none layer_21/width_16k/average_l0_86Load this SAE jumprelu blocks.21.attn.hook_z 16384 none layer_21/width_16k/average_l0_195Load this SAE jumprelu blocks.21.attn.hook_z 16384 none layer_21/width_16k/average_l0_420Load this SAE jumprelu blocks.21.attn.hook_z 16384 none layer_21/width_16k/average_l0_792Load this SAE jumprelu blocks.21.attn.hook_z 16384 none layer_21/width_131k/average_l0_18Load this SAE jumprelu blocks.21.attn.hook_z 131072 none layer_21/width_131k/average_l0_35Load this SAE jumprelu blocks.21.attn.hook_z 131072 none layer_21/width_131k/average_l0_71Load this SAE jumprelu blocks.21.attn.hook_z 131072 none layer_21/width_131k/average_l0_150Load this SAE jumprelu blocks.21.attn.hook_z 131072 none layer_21/width_131k/average_l0_333Load this SAE jumprelu blocks.21.attn.hook_z 131072 none layer_21/width_131k/average_l0_665Load this SAE jumprelu blocks.21.attn.hook_z 131072 none layer_22/width_16k/average_l0_20Load this SAE jumprelu blocks.22.attn.hook_z 16384 none layer_22/width_16k/average_l0_36Load this SAE jumprelu blocks.22.attn.hook_z 16384 none layer_22/width_16k/average_l0_70Load this SAE jumprelu blocks.22.attn.hook_z 16384 none layer_22/width_16k/average_l0_141Load this SAE jumprelu blocks.22.attn.hook_z 16384 none layer_22/width_16k/average_l0_289Load this SAE jumprelu blocks.22.attn.hook_z 16384 none layer_22/width_16k/average_l0_569Load this SAE jumprelu blocks.22.attn.hook_z 16384 none layer_22/width_131k/average_l0_17Load this SAE jumprelu blocks.22.attn.hook_z 131072 none layer_22/width_131k/average_l0_32Load this SAE jumprelu blocks.22.attn.hook_z 131072 none layer_22/width_131k/average_l0_59Load this SAE jumprelu blocks.22.attn.hook_z 131072 none layer_22/width_131k/average_l0_115Load this SAE jumprelu blocks.22.attn.hook_z 131072 none layer_22/width_131k/average_l0_231Load this SAE jumprelu blocks.22.attn.hook_z 131072 none layer_22/width_131k/average_l0_446Load this SAE jumprelu blocks.22.attn.hook_z 131072 none layer_23/width_16k/average_l0_21Load this SAE jumprelu blocks.23.attn.hook_z 16384 none layer_23/width_16k/average_l0_41Load this SAE jumprelu blocks.23.attn.hook_z 16384 none layer_23/width_16k/average_l0_80Load this SAE jumprelu blocks.23.attn.hook_z 16384 none layer_23/width_16k/average_l0_173Load this SAE jumprelu blocks.23.attn.hook_z 16384 none layer_23/width_16k/average_l0_356Load this SAE jumprelu blocks.23.attn.hook_z 16384 none layer_23/width_16k/average_l0_649Load this SAE jumprelu blocks.23.attn.hook_z 16384 none layer_23/width_131k/average_l0_19Load this SAE jumprelu blocks.23.attn.hook_z 131072 none layer_23/width_131k/average_l0_36Load this SAE jumprelu blocks.23.attn.hook_z 131072 none layer_23/width_131k/average_l0_66Load this SAE jumprelu blocks.23.attn.hook_z 131072 none layer_23/width_131k/average_l0_134Load this SAE jumprelu blocks.23.attn.hook_z 131072 none layer_23/width_131k/average_l0_288Load this SAE jumprelu blocks.23.attn.hook_z 131072 none layer_23/width_131k/average_l0_568Load this SAE jumprelu blocks.23.attn.hook_z 131072 none layer_24/width_16k/average_l0_21Load this SAE jumprelu blocks.24.attn.hook_z 16384 none layer_24/width_16k/average_l0_40Load this SAE jumprelu blocks.24.attn.hook_z 16384 none layer_24/width_16k/average_l0_79Load this SAE jumprelu blocks.24.attn.hook_z 16384 none layer_24/width_16k/average_l0_167Load this SAE jumprelu blocks.24.attn.hook_z 16384 none layer_24/width_16k/average_l0_348Load this SAE jumprelu blocks.24.attn.hook_z 16384 none layer_24/width_16k/average_l0_615Load this SAE jumprelu blocks.24.attn.hook_z 16384 none layer_24/width_131k/average_l0_19Load this SAE jumprelu blocks.24.attn.hook_z 131072 none layer_24/width_131k/average_l0_35Load this SAE jumprelu blocks.24.attn.hook_z 131072 none layer_24/width_131k/average_l0_66Load this SAE jumprelu blocks.24.attn.hook_z 131072 none layer_24/width_131k/average_l0_130Load this SAE jumprelu blocks.24.attn.hook_z 131072 none layer_24/width_131k/average_l0_273Load this SAE jumprelu blocks.24.attn.hook_z 131072 none layer_24/width_131k/average_l0_542Load this SAE jumprelu blocks.24.attn.hook_z 131072 none layer_25/width_16k/average_l0_19Load this SAE jumprelu blocks.25.attn.hook_z 16384 none layer_25/width_16k/average_l0_36Load this SAE jumprelu blocks.25.attn.hook_z 16384 none layer_25/width_16k/average_l0_73Load this SAE jumprelu blocks.25.attn.hook_z 16384 none layer_25/width_16k/average_l0_156Load this SAE jumprelu blocks.25.attn.hook_z 16384 none layer_25/width_16k/average_l0_371Load this SAE jumprelu blocks.25.attn.hook_z 16384 none layer_25/width_16k/average_l0_686Load this SAE jumprelu blocks.25.attn.hook_z 16384 none layer_25/width_131k/average_l0_17Load this SAE jumprelu blocks.25.attn.hook_z 131072 none layer_25/width_131k/average_l0_31Load this SAE jumprelu blocks.25.attn.hook_z 131072 none layer_25/width_131k/average_l0_59Load this SAE jumprelu blocks.25.attn.hook_z 131072 none layer_25/width_131k/average_l0_115Load this SAE jumprelu blocks.25.attn.hook_z 131072 none layer_25/width_131k/average_l0_261Load this SAE jumprelu blocks.25.attn.hook_z 131072 none layer_25/width_131k/average_l0_605Load this SAE jumprelu blocks.25.attn.hook_z 131072 none layer_26/width_16k/average_l0_20Load this SAE jumprelu blocks.26.attn.hook_z 16384 none layer_26/width_16k/average_l0_38Load this SAE jumprelu blocks.26.attn.hook_z 16384 none layer_26/width_16k/average_l0_75Load this SAE jumprelu blocks.26.attn.hook_z 16384 none layer_26/width_16k/average_l0_159Load this SAE jumprelu blocks.26.attn.hook_z 16384 none layer_26/width_16k/average_l0_336Load this SAE jumprelu blocks.26.attn.hook_z 16384 none layer_26/width_16k/average_l0_634Load this SAE jumprelu blocks.26.attn.hook_z 16384 none layer_26/width_131k/average_l0_18Load this SAE jumprelu blocks.26.attn.hook_z 131072 none layer_26/width_131k/average_l0_32Load this SAE jumprelu blocks.26.attn.hook_z 131072 none layer_26/width_131k/average_l0_62Load this SAE jumprelu blocks.26.attn.hook_z 131072 none layer_26/width_131k/average_l0_120Load this SAE jumprelu blocks.26.attn.hook_z 131072 none layer_26/width_131k/average_l0_267Load this SAE jumprelu blocks.26.attn.hook_z 131072 none layer_26/width_131k/average_l0_525Load this SAE jumprelu blocks.26.attn.hook_z 131072 none layer_27/width_16k/average_l0_18Load this SAE jumprelu blocks.27.attn.hook_z 16384 none layer_27/width_16k/average_l0_33Load this SAE jumprelu blocks.27.attn.hook_z 16384 none layer_27/width_16k/average_l0_64Load this SAE jumprelu blocks.27.attn.hook_z 16384 none layer_27/width_16k/average_l0_136Load this SAE jumprelu blocks.27.attn.hook_z 16384 none layer_27/width_16k/average_l0_306Load this SAE jumprelu blocks.27.attn.hook_z 16384 none layer_27/width_16k/average_l0_613Load this SAE jumprelu blocks.27.attn.hook_z 16384 none layer_27/width_131k/average_l0_16Load this SAE jumprelu blocks.27.attn.hook_z 131072 none layer_27/width_131k/average_l0_28Load this SAE jumprelu blocks.27.attn.hook_z 131072 none layer_27/width_131k/average_l0_53Load this SAE jumprelu blocks.27.attn.hook_z 131072 none layer_27/width_131k/average_l0_102Load this SAE jumprelu blocks.27.attn.hook_z 131072 none layer_27/width_131k/average_l0_211Load this SAE jumprelu blocks.27.attn.hook_z 131072 none layer_27/width_131k/average_l0_458Load this SAE jumprelu blocks.27.attn.hook_z 131072 none layer_28/width_16k/average_l0_20Load this SAE jumprelu blocks.28.attn.hook_z 16384 none layer_28/width_16k/average_l0_37Load this SAE jumprelu blocks.28.attn.hook_z 16384 none layer_28/width_16k/average_l0_71Load this SAE jumprelu blocks.28.attn.hook_z 16384 none layer_28/width_16k/average_l0_143Load this SAE jumprelu blocks.28.attn.hook_z 16384 none layer_28/width_16k/average_l0_279Load this SAE jumprelu blocks.28.attn.hook_z 16384 none layer_28/width_16k/average_l0_498Load this SAE jumprelu blocks.28.attn.hook_z 16384 none layer_28/width_131k/average_l0_19Load this SAE jumprelu blocks.28.attn.hook_z 131072 none layer_28/width_131k/average_l0_32Load this SAE jumprelu blocks.28.attn.hook_z 131072 none layer_28/width_131k/average_l0_59Load this SAE jumprelu blocks.28.attn.hook_z 131072 none layer_28/width_131k/average_l0_115Load this SAE jumprelu blocks.28.attn.hook_z 131072 none layer_28/width_131k/average_l0_230Load this SAE jumprelu blocks.28.attn.hook_z 131072 none layer_28/width_131k/average_l0_452Load this SAE jumprelu blocks.28.attn.hook_z 131072 none layer_29/width_16k/average_l0_18Load this SAE jumprelu blocks.29.attn.hook_z 16384 none layer_29/width_16k/average_l0_35Load this SAE jumprelu blocks.29.attn.hook_z 16384 none layer_29/width_16k/average_l0_76Load this SAE jumprelu blocks.29.attn.hook_z 16384 none layer_29/width_16k/average_l0_171Load this SAE jumprelu blocks.29.attn.hook_z 16384 none layer_29/width_16k/average_l0_308Load this SAE jumprelu blocks.29.attn.hook_z 16384 none layer_29/width_16k/average_l0_559Load this SAE jumprelu blocks.29.attn.hook_z 16384 none layer_29/width_131k/average_l0_17Load this SAE jumprelu blocks.29.attn.hook_z 131072 none layer_29/width_131k/average_l0_30Load this SAE jumprelu blocks.29.attn.hook_z 131072 none layer_29/width_131k/average_l0_57Load this SAE jumprelu blocks.29.attn.hook_z 131072 none layer_29/width_131k/average_l0_128Load this SAE jumprelu blocks.29.attn.hook_z 131072 none layer_29/width_131k/average_l0_265Load this SAE jumprelu blocks.29.attn.hook_z 131072 none layer_29/width_131k/average_l0_446Load this SAE jumprelu blocks.29.attn.hook_z 131072 none layer_3/width_16k/average_l0_23Load this SAE jumprelu blocks.3.attn.hook_z 16384 none layer_3/width_16k/average_l0_44Load this SAE jumprelu blocks.3.attn.hook_z 16384 none layer_3/width_16k/average_l0_102Load this SAE jumprelu blocks.3.attn.hook_z 16384 none layer_3/width_16k/average_l0_221Load this SAE jumprelu blocks.3.attn.hook_z 16384 none layer_3/width_16k/average_l0_435Load this SAE jumprelu blocks.3.attn.hook_z 16384 none layer_3/width_131k/average_l0_10Load this SAE jumprelu blocks.3.attn.hook_z 131072 none layer_3/width_131k/average_l0_17Load this SAE jumprelu blocks.3.attn.hook_z 131072 none layer_3/width_131k/average_l0_31Load this SAE jumprelu blocks.3.attn.hook_z 131072 none layer_3/width_131k/average_l0_58Load this SAE jumprelu blocks.3.attn.hook_z 131072 none layer_3/width_131k/average_l0_119Load this SAE jumprelu blocks.3.attn.hook_z 131072 none layer_3/width_131k/average_l0_252Load this SAE jumprelu blocks.3.attn.hook_z 131072 none layer_30/width_16k/average_l0_19Load this SAE jumprelu blocks.30.attn.hook_z 16384 none layer_30/width_16k/average_l0_36Load this SAE jumprelu blocks.30.attn.hook_z 16384 none layer_30/width_16k/average_l0_73Load this SAE jumprelu blocks.30.attn.hook_z 16384 none layer_30/width_16k/average_l0_157Load this SAE jumprelu blocks.30.attn.hook_z 16384 none layer_30/width_16k/average_l0_313Load this SAE jumprelu blocks.30.attn.hook_z 16384 none layer_30/width_16k/average_l0_558Load this SAE jumprelu blocks.30.attn.hook_z 16384 none layer_30/width_131k/average_l0_17Load this SAE jumprelu blocks.30.attn.hook_z 131072 none layer_30/width_131k/average_l0_29Load this SAE jumprelu blocks.30.attn.hook_z 131072 none layer_30/width_131k/average_l0_55Load this SAE jumprelu blocks.30.attn.hook_z 131072 none layer_30/width_131k/average_l0_109Load this SAE jumprelu blocks.30.attn.hook_z 131072 none layer_30/width_131k/average_l0_236Load this SAE jumprelu blocks.30.attn.hook_z 131072 none layer_30/width_131k/average_l0_491Load this SAE jumprelu blocks.30.attn.hook_z 131072 none layer_31/width_16k/average_l0_18Load this SAE jumprelu blocks.31.attn.hook_z 16384 none layer_31/width_16k/average_l0_36Load this SAE jumprelu blocks.31.attn.hook_z 16384 none layer_31/width_16k/average_l0_73Load this SAE jumprelu blocks.31.attn.hook_z 16384 none layer_31/width_16k/average_l0_168Load this SAE jumprelu blocks.31.attn.hook_z 16384 none layer_31/width_16k/average_l0_356Load this SAE jumprelu blocks.31.attn.hook_z 16384 none layer_31/width_16k/average_l0_630Load this SAE jumprelu blocks.31.attn.hook_z 16384 none layer_31/width_131k/average_l0_16Load this SAE jumprelu blocks.31.attn.hook_z 131072 none layer_31/width_131k/average_l0_29Load this SAE jumprelu blocks.31.attn.hook_z 131072 none layer_31/width_131k/average_l0_56Load this SAE jumprelu blocks.31.attn.hook_z 131072 none layer_31/width_131k/average_l0_117Load this SAE jumprelu blocks.31.attn.hook_z 131072 none layer_31/width_131k/average_l0_265Load this SAE jumprelu blocks.31.attn.hook_z 131072 none layer_31/width_131k/average_l0_543Load this SAE jumprelu blocks.31.attn.hook_z 131072 none layer_32/width_16k/average_l0_18Load this SAE jumprelu blocks.32.attn.hook_z 16384 none layer_32/width_16k/average_l0_35Load this SAE jumprelu blocks.32.attn.hook_z 16384 none layer_32/width_16k/average_l0_74Load this SAE jumprelu blocks.32.attn.hook_z 16384 none layer_32/width_16k/average_l0_158Load this SAE jumprelu blocks.32.attn.hook_z 16384 none layer_32/width_16k/average_l0_306Load this SAE jumprelu blocks.32.attn.hook_z 16384 none layer_32/width_16k/average_l0_534Load this SAE jumprelu blocks.32.attn.hook_z 16384 none layer_32/width_131k/average_l0_16Load this SAE jumprelu blocks.32.attn.hook_z 131072 none layer_32/width_131k/average_l0_31Load this SAE jumprelu blocks.32.attn.hook_z 131072 none layer_32/width_131k/average_l0_56Load this SAE jumprelu blocks.32.attn.hook_z 131072 none layer_32/width_131k/average_l0_117Load this SAE jumprelu blocks.32.attn.hook_z 131072 none layer_32/width_131k/average_l0_248Load this SAE jumprelu blocks.32.attn.hook_z 131072 none layer_32/width_131k/average_l0_464Load this SAE jumprelu blocks.32.attn.hook_z 131072 none layer_33/width_16k/average_l0_17Load this SAE jumprelu blocks.33.attn.hook_z 16384 none layer_33/width_16k/average_l0_37Load this SAE jumprelu blocks.33.attn.hook_z 16384 none layer_33/width_16k/average_l0_78Load this SAE jumprelu blocks.33.attn.hook_z 16384 none layer_33/width_16k/average_l0_158Load this SAE jumprelu blocks.33.attn.hook_z 16384 none layer_33/width_16k/average_l0_318Load this SAE jumprelu blocks.33.attn.hook_z 16384 none layer_33/width_16k/average_l0_531Load this SAE jumprelu blocks.33.attn.hook_z 16384 none layer_33/width_131k/average_l0_16Load this SAE jumprelu blocks.33.attn.hook_z 131072 none layer_33/width_131k/average_l0_28Load this SAE jumprelu blocks.33.attn.hook_z 131072 none layer_33/width_131k/average_l0_58Load this SAE jumprelu blocks.33.attn.hook_z 131072 none layer_33/width_131k/average_l0_128Load this SAE jumprelu blocks.33.attn.hook_z 131072 none layer_33/width_131k/average_l0_248Load this SAE jumprelu blocks.33.attn.hook_z 131072 none layer_33/width_131k/average_l0_471Load this SAE jumprelu blocks.33.attn.hook_z 131072 none layer_34/width_16k/average_l0_17Load this SAE jumprelu blocks.34.attn.hook_z 16384 none layer_34/width_16k/average_l0_38Load this SAE jumprelu blocks.34.attn.hook_z 16384 none layer_34/width_16k/average_l0_91Load this SAE jumprelu blocks.34.attn.hook_z 16384 none layer_34/width_16k/average_l0_192Load this SAE jumprelu blocks.34.attn.hook_z 16384 none layer_34/width_16k/average_l0_339Load this SAE jumprelu blocks.34.attn.hook_z 16384 none layer_34/width_16k/average_l0_527Load this SAE jumprelu blocks.34.attn.hook_z 16384 none layer_34/width_131k/average_l0_15Load this SAE jumprelu blocks.34.attn.hook_z 131072 none layer_34/width_131k/average_l0_29Load this SAE jumprelu blocks.34.attn.hook_z 131072 none layer_34/width_131k/average_l0_63Load this SAE jumprelu blocks.34.attn.hook_z 131072 none layer_34/width_131k/average_l0_199Load this SAE jumprelu blocks.34.attn.hook_z 131072 none layer_34/width_131k/average_l0_291Load this SAE jumprelu blocks.34.attn.hook_z 131072 none layer_34/width_131k/average_l0_483Load this SAE jumprelu blocks.34.attn.hook_z 131072 none layer_35/width_16k/average_l0_14Load this SAE jumprelu blocks.35.attn.hook_z 16384 none layer_35/width_16k/average_l0_33Load this SAE jumprelu blocks.35.attn.hook_z 16384 none layer_35/width_16k/average_l0_77Load this SAE jumprelu blocks.35.attn.hook_z 16384 none layer_35/width_16k/average_l0_168Load this SAE jumprelu blocks.35.attn.hook_z 16384 none layer_35/width_16k/average_l0_303Load this SAE jumprelu blocks.35.attn.hook_z 16384 none layer_35/width_16k/average_l0_488Load this SAE jumprelu blocks.35.attn.hook_z 16384 none layer_35/width_131k/average_l0_13Load this SAE jumprelu blocks.35.attn.hook_z 131072 none layer_35/width_131k/average_l0_26Load this SAE jumprelu blocks.35.attn.hook_z 131072 none layer_35/width_131k/average_l0_54Load this SAE jumprelu blocks.35.attn.hook_z 131072 none layer_35/width_131k/average_l0_124Load this SAE jumprelu blocks.35.attn.hook_z 131072 none layer_35/width_131k/average_l0_258Load this SAE jumprelu blocks.35.attn.hook_z 131072 none layer_35/width_131k/average_l0_427Load this SAE jumprelu blocks.35.attn.hook_z 131072 none layer_36/width_16k/average_l0_15Load this SAE jumprelu blocks.36.attn.hook_z 16384 none layer_36/width_16k/average_l0_32Load this SAE jumprelu blocks.36.attn.hook_z 16384 none layer_36/width_16k/average_l0_69Load this SAE jumprelu blocks.36.attn.hook_z 16384 none layer_36/width_16k/average_l0_144Load this SAE jumprelu blocks.36.attn.hook_z 16384 none layer_36/width_16k/average_l0_293Load this SAE jumprelu blocks.36.attn.hook_z 16384 none layer_36/width_16k/average_l0_544Load this SAE jumprelu blocks.36.attn.hook_z 16384 none layer_36/width_131k/average_l0_16Load this SAE jumprelu blocks.36.attn.hook_z 131072 none layer_36/width_131k/average_l0_26Load this SAE jumprelu blocks.36.attn.hook_z 131072 none layer_36/width_131k/average_l0_51Load this SAE jumprelu blocks.36.attn.hook_z 131072 none layer_36/width_131k/average_l0_105Load this SAE jumprelu blocks.36.attn.hook_z 131072 none layer_36/width_131k/average_l0_229Load this SAE jumprelu blocks.36.attn.hook_z 131072 none layer_36/width_131k/average_l0_455Load this SAE jumprelu blocks.36.attn.hook_z 131072 none layer_37/width_16k/average_l0_17Load this SAE jumprelu blocks.37.attn.hook_z 16384 none layer_37/width_16k/average_l0_34Load this SAE jumprelu blocks.37.attn.hook_z 16384 none layer_37/width_16k/average_l0_82Load this SAE jumprelu blocks.37.attn.hook_z 16384 none layer_37/width_16k/average_l0_172Load this SAE jumprelu blocks.37.attn.hook_z 16384 none layer_37/width_16k/average_l0_309Load this SAE jumprelu blocks.37.attn.hook_z 16384 none layer_37/width_16k/average_l0_519Load this SAE jumprelu blocks.37.attn.hook_z 16384 none layer_37/width_131k/average_l0_15Load this SAE jumprelu blocks.37.attn.hook_z 131072 none layer_37/width_131k/average_l0_28Load this SAE jumprelu blocks.37.attn.hook_z 131072 none layer_37/width_131k/average_l0_55Load this SAE jumprelu blocks.37.attn.hook_z 131072 none layer_37/width_131k/average_l0_124Load this SAE jumprelu blocks.37.attn.hook_z 131072 none layer_37/width_131k/average_l0_248Load this SAE jumprelu blocks.37.attn.hook_z 131072 none layer_37/width_131k/average_l0_450Load this SAE jumprelu blocks.37.attn.hook_z 131072 none layer_38/width_16k/average_l0_18Load this SAE jumprelu blocks.38.attn.hook_z 16384 none layer_38/width_16k/average_l0_36Load this SAE jumprelu blocks.38.attn.hook_z 16384 none layer_38/width_16k/average_l0_81Load this SAE jumprelu blocks.38.attn.hook_z 16384 none layer_38/width_16k/average_l0_175Load this SAE jumprelu blocks.38.attn.hook_z 16384 none layer_38/width_16k/average_l0_334Load this SAE jumprelu blocks.38.attn.hook_z 16384 none layer_38/width_16k/average_l0_547Load this SAE jumprelu blocks.38.attn.hook_z 16384 none layer_38/width_131k/average_l0_17Load this SAE jumprelu blocks.38.attn.hook_z 131072 none layer_38/width_131k/average_l0_30Load this SAE jumprelu blocks.38.attn.hook_z 131072 none layer_38/width_131k/average_l0_60Load this SAE jumprelu blocks.38.attn.hook_z 131072 none layer_38/width_131k/average_l0_135Load this SAE jumprelu blocks.38.attn.hook_z 131072 none layer_38/width_131k/average_l0_284Load this SAE jumprelu blocks.38.attn.hook_z 131072 none layer_38/width_131k/average_l0_489Load this SAE jumprelu blocks.38.attn.hook_z 131072 none layer_39/width_16k/average_l0_15Load this SAE jumprelu blocks.39.attn.hook_z 16384 none layer_39/width_16k/average_l0_33Load this SAE jumprelu blocks.39.attn.hook_z 16384 none layer_39/width_16k/average_l0_77Load this SAE jumprelu blocks.39.attn.hook_z 16384 none layer_39/width_16k/average_l0_176Load this SAE jumprelu blocks.39.attn.hook_z 16384 none layer_39/width_16k/average_l0_391Load this SAE jumprelu blocks.39.attn.hook_z 16384 none layer_39/width_16k/average_l0_694Load this SAE jumprelu blocks.39.attn.hook_z 16384 none layer_39/width_131k/average_l0_17Load this SAE jumprelu blocks.39.attn.hook_z 131072 none layer_39/width_131k/average_l0_27Load this SAE jumprelu blocks.39.attn.hook_z 131072 none layer_39/width_131k/average_l0_54Load this SAE jumprelu blocks.39.attn.hook_z 131072 none layer_39/width_131k/average_l0_120Load this SAE jumprelu blocks.39.attn.hook_z 131072 none layer_39/width_131k/average_l0_273Load this SAE jumprelu blocks.39.attn.hook_z 131072 none layer_39/width_131k/average_l0_604Load this SAE jumprelu blocks.39.attn.hook_z 131072 none layer_4/width_16k/average_l0_26Load this SAE jumprelu blocks.4.attn.hook_z 16384 none layer_4/width_16k/average_l0_54Load this SAE jumprelu blocks.4.attn.hook_z 16384 none layer_4/width_16k/average_l0_126Load this SAE jumprelu blocks.4.attn.hook_z 16384 none layer_4/width_16k/average_l0_274Load this SAE jumprelu blocks.4.attn.hook_z 16384 none layer_4/width_16k/average_l0_524Load this SAE jumprelu blocks.4.attn.hook_z 16384 none layer_4/width_131k/average_l0_12Load this SAE jumprelu blocks.4.attn.hook_z 131072 none layer_4/width_131k/average_l0_21Load this SAE jumprelu blocks.4.attn.hook_z 131072 none layer_4/width_131k/average_l0_38Load this SAE jumprelu blocks.4.attn.hook_z 131072 none layer_4/width_131k/average_l0_75Load this SAE jumprelu blocks.4.attn.hook_z 131072 none layer_4/width_131k/average_l0_163Load this SAE jumprelu blocks.4.attn.hook_z 131072 none layer_4/width_131k/average_l0_330Load this SAE jumprelu blocks.4.attn.hook_z 131072 none layer_40/width_16k/average_l0_18Load this SAE jumprelu blocks.40.attn.hook_z 16384 none layer_40/width_16k/average_l0_38Load this SAE jumprelu blocks.40.attn.hook_z 16384 none layer_40/width_16k/average_l0_91Load this SAE jumprelu blocks.40.attn.hook_z 16384 none layer_40/width_16k/average_l0_189Load this SAE jumprelu blocks.40.attn.hook_z 16384 none layer_40/width_16k/average_l0_341Load this SAE jumprelu blocks.40.attn.hook_z 16384 none layer_40/width_16k/average_l0_603Load this SAE jumprelu blocks.40.attn.hook_z 16384 none layer_40/width_131k/average_l0_16Load this SAE jumprelu blocks.40.attn.hook_z 131072 none layer_40/width_131k/average_l0_30Load this SAE jumprelu blocks.40.attn.hook_z 131072 none layer_40/width_131k/average_l0_64Load this SAE jumprelu blocks.40.attn.hook_z 131072 none layer_40/width_131k/average_l0_144Load this SAE jumprelu blocks.40.attn.hook_z 131072 none layer_40/width_131k/average_l0_269Load this SAE jumprelu blocks.40.attn.hook_z 131072 none layer_40/width_131k/average_l0_493Load this SAE jumprelu blocks.40.attn.hook_z 131072 none layer_41/width_16k/average_l0_13Load this SAE jumprelu blocks.41.attn.hook_z 16384 none layer_41/width_16k/average_l0_25Load this SAE jumprelu blocks.41.attn.hook_z 16384 none layer_41/width_16k/average_l0_56Load this SAE jumprelu blocks.41.attn.hook_z 16384 none layer_41/width_16k/average_l0_129Load this SAE jumprelu blocks.41.attn.hook_z 16384 none layer_41/width_16k/average_l0_254Load this SAE jumprelu blocks.41.attn.hook_z 16384 none layer_41/width_16k/average_l0_450Load this SAE jumprelu blocks.41.attn.hook_z 16384 none layer_41/width_131k/average_l0_13Load this SAE jumprelu blocks.41.attn.hook_z 131072 none layer_41/width_131k/average_l0_22Load this SAE jumprelu blocks.41.attn.hook_z 131072 none layer_41/width_131k/average_l0_43Load this SAE jumprelu blocks.41.attn.hook_z 131072 none layer_41/width_131k/average_l0_92Load this SAE jumprelu blocks.41.attn.hook_z 131072 none layer_41/width_131k/average_l0_202Load this SAE jumprelu blocks.41.attn.hook_z 131072 none layer_41/width_131k/average_l0_370Load this SAE jumprelu blocks.41.attn.hook_z 131072 none layer_5/width_16k/average_l0_25Load this SAE jumprelu blocks.5.attn.hook_z 16384 none layer_5/width_16k/average_l0_53Load this SAE jumprelu blocks.5.attn.hook_z 16384 none layer_5/width_16k/average_l0_125Load this SAE jumprelu blocks.5.attn.hook_z 16384 none layer_5/width_16k/average_l0_270Load this SAE jumprelu blocks.5.attn.hook_z 16384 none layer_5/width_16k/average_l0_529Load this SAE jumprelu blocks.5.attn.hook_z 16384 none layer_5/width_131k/average_l0_12Load this SAE jumprelu blocks.5.attn.hook_z 131072 none layer_5/width_131k/average_l0_21Load this SAE jumprelu blocks.5.attn.hook_z 131072 none layer_5/width_131k/average_l0_39Load this SAE jumprelu blocks.5.attn.hook_z 131072 none layer_5/width_131k/average_l0_78Load this SAE jumprelu blocks.5.attn.hook_z 131072 none layer_5/width_131k/average_l0_160Load this SAE jumprelu blocks.5.attn.hook_z 131072 none layer_5/width_131k/average_l0_351Load this SAE jumprelu blocks.5.attn.hook_z 131072 none layer_6/width_16k/average_l0_16Load this SAE jumprelu blocks.6.attn.hook_z 16384 none layer_6/width_16k/average_l0_28Load this SAE jumprelu blocks.6.attn.hook_z 16384 none layer_6/width_16k/average_l0_51Load this SAE jumprelu blocks.6.attn.hook_z 16384 none layer_6/width_16k/average_l0_108Load this SAE jumprelu blocks.6.attn.hook_z 16384 none layer_6/width_16k/average_l0_258Load this SAE jumprelu blocks.6.attn.hook_z 16384 none layer_6/width_131k/average_l0_14Load this SAE jumprelu blocks.6.attn.hook_z 131072 none layer_6/width_131k/average_l0_24Load this SAE jumprelu blocks.6.attn.hook_z 131072 none layer_6/width_131k/average_l0_41Load this SAE jumprelu blocks.6.attn.hook_z 131072 none layer_6/width_131k/average_l0_73Load this SAE jumprelu blocks.6.attn.hook_z 131072 none layer_6/width_131k/average_l0_148Load this SAE jumprelu blocks.6.attn.hook_z 131072 none layer_6/width_131k/average_l0_353Load this SAE jumprelu blocks.6.attn.hook_z 131072 none layer_7/width_16k/average_l0_18Load this SAE jumprelu blocks.7.attn.hook_z 16384 none layer_7/width_16k/average_l0_33Load this SAE jumprelu blocks.7.attn.hook_z 16384 none layer_7/width_16k/average_l0_70Load this SAE jumprelu blocks.7.attn.hook_z 16384 none layer_7/width_16k/average_l0_160Load this SAE jumprelu blocks.7.attn.hook_z 16384 none layer_7/width_16k/average_l0_335Load this SAE jumprelu blocks.7.attn.hook_z 16384 none layer_7/width_131k/average_l0_16Load this SAE jumprelu blocks.7.attn.hook_z 131072 none layer_7/width_131k/average_l0_28Load this SAE jumprelu blocks.7.attn.hook_z 131072 none layer_7/width_131k/average_l0_52Load this SAE jumprelu blocks.7.attn.hook_z 131072 none layer_7/width_131k/average_l0_106Load this SAE jumprelu blocks.7.attn.hook_z 131072 none layer_7/width_131k/average_l0_229Load this SAE jumprelu blocks.7.attn.hook_z 131072 none layer_7/width_131k/average_l0_473Load this SAE jumprelu blocks.7.attn.hook_z 131072 none layer_8/width_16k/average_l0_17Load this SAE jumprelu blocks.8.attn.hook_z 16384 none layer_8/width_16k/average_l0_32Load this SAE jumprelu blocks.8.attn.hook_z 16384 none layer_8/width_16k/average_l0_65Load this SAE jumprelu blocks.8.attn.hook_z 16384 none layer_8/width_16k/average_l0_150Load this SAE jumprelu blocks.8.attn.hook_z 16384 none layer_8/width_16k/average_l0_362Load this SAE jumprelu blocks.8.attn.hook_z 16384 none layer_8/width_131k/average_l0_16Load this SAE jumprelu blocks.8.attn.hook_z 131072 none layer_8/width_131k/average_l0_27Load this SAE jumprelu blocks.8.attn.hook_z 131072 none layer_8/width_131k/average_l0_51Load this SAE jumprelu blocks.8.attn.hook_z 131072 none layer_8/width_131k/average_l0_98Load this SAE jumprelu blocks.8.attn.hook_z 131072 none layer_8/width_131k/average_l0_222Load this SAE jumprelu blocks.8.attn.hook_z 131072 none layer_8/width_131k/average_l0_531Load this SAE jumprelu blocks.8.attn.hook_z 131072 none layer_9/width_16k/average_l0_18Load this SAE jumprelu blocks.9.attn.hook_z 16384 none layer_9/width_16k/average_l0_34Load this SAE jumprelu blocks.9.attn.hook_z 16384 none layer_9/width_16k/average_l0_71Load this SAE jumprelu blocks.9.attn.hook_z 16384 none layer_9/width_16k/average_l0_172Load this SAE jumprelu blocks.9.attn.hook_z 16384 none layer_9/width_16k/average_l0_349Load this SAE jumprelu blocks.9.attn.hook_z 16384 none layer_9/width_131k/average_l0_16Load this SAE jumprelu blocks.9.attn.hook_z 131072 none layer_9/width_131k/average_l0_30Load this SAE jumprelu blocks.9.attn.hook_z 131072 none layer_9/width_131k/average_l0_54Load this SAE jumprelu blocks.9.attn.hook_z 131072 none layer_9/width_131k/average_l0_111Load this SAE jumprelu blocks.9.attn.hook_z 131072 none layer_9/width_131k/average_l0_263Load this SAE jumprelu blocks.9.attn.hook_z 131072 none layer_9/width_131k/average_l0_511Load this SAE jumprelu blocks.9.attn.hook_z 131072 none"},{"location":"sae_table/#gemma-scope-9b-pt-att-canonical","title":"gemma-scope-9b-pt-att-canonical","text":"<ul> <li>Huggingface Repo: google/gemma-scope-9b-pt-att</li> <li>model: gemma-2-9b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/0-gemmascope-att-16k blocks.0.attn.hook_z 16384 none layer_1/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/1-gemmascope-att-16k blocks.1.attn.hook_z 16384 none layer_2/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/2-gemmascope-att-16k blocks.2.attn.hook_z 16384 none layer_3/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/3-gemmascope-att-16k blocks.3.attn.hook_z 16384 none layer_4/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/4-gemmascope-att-16k blocks.4.attn.hook_z 16384 none layer_5/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/5-gemmascope-att-16k blocks.5.attn.hook_z 16384 none layer_6/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/6-gemmascope-att-16k blocks.6.attn.hook_z 16384 none layer_7/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/7-gemmascope-att-16k blocks.7.attn.hook_z 16384 none layer_8/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/8-gemmascope-att-16k blocks.8.attn.hook_z 16384 none layer_9/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/9-gemmascope-att-16k blocks.9.attn.hook_z 16384 none layer_10/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/10-gemmascope-att-16k blocks.10.attn.hook_z 16384 none layer_11/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/11-gemmascope-att-16k blocks.11.attn.hook_z 16384 none layer_12/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/12-gemmascope-att-16k blocks.12.attn.hook_z 16384 none layer_13/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/13-gemmascope-att-16k blocks.13.attn.hook_z 16384 none layer_14/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/14-gemmascope-att-16k blocks.14.attn.hook_z 16384 none layer_15/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/15-gemmascope-att-16k blocks.15.attn.hook_z 16384 none layer_16/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/16-gemmascope-att-16k blocks.16.attn.hook_z 16384 none layer_17/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/17-gemmascope-att-16k blocks.17.attn.hook_z 16384 none layer_18/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/18-gemmascope-att-16k blocks.18.attn.hook_z 16384 none layer_19/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/19-gemmascope-att-16k blocks.19.attn.hook_z 16384 none layer_20/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/20-gemmascope-att-16k blocks.20.attn.hook_z 16384 none layer_21/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/21-gemmascope-att-16k blocks.21.attn.hook_z 16384 none layer_22/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/22-gemmascope-att-16k blocks.22.attn.hook_z 16384 none layer_23/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/23-gemmascope-att-16k blocks.23.attn.hook_z 16384 none layer_24/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/24-gemmascope-att-16k blocks.24.attn.hook_z 16384 none layer_25/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/25-gemmascope-att-16k blocks.25.attn.hook_z 16384 none layer_26/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/26-gemmascope-att-16k blocks.26.attn.hook_z 16384 none layer_27/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/27-gemmascope-att-16k blocks.27.attn.hook_z 16384 none layer_28/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/28-gemmascope-att-16k blocks.28.attn.hook_z 16384 none layer_29/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/29-gemmascope-att-16k blocks.29.attn.hook_z 16384 none layer_30/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/30-gemmascope-att-16k blocks.30.attn.hook_z 16384 none layer_31/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/31-gemmascope-att-16k blocks.31.attn.hook_z 16384 none layer_32/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/32-gemmascope-att-16k blocks.32.attn.hook_z 16384 none layer_33/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/33-gemmascope-att-16k blocks.33.attn.hook_z 16384 none layer_34/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/34-gemmascope-att-16k blocks.34.attn.hook_z 16384 none layer_35/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/35-gemmascope-att-16k blocks.35.attn.hook_z 16384 none layer_36/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/36-gemmascope-att-16k blocks.36.attn.hook_z 16384 none layer_37/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/37-gemmascope-att-16k blocks.37.attn.hook_z 16384 none layer_38/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/38-gemmascope-att-16k blocks.38.attn.hook_z 16384 none layer_39/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/39-gemmascope-att-16k blocks.39.attn.hook_z 16384 none layer_40/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/40-gemmascope-att-16k blocks.40.attn.hook_z 16384 none layer_41/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/41-gemmascope-att-16k blocks.41.attn.hook_z 16384 none layer_0/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/0-gemmascope-att-131k blocks.0.attn.hook_z 131072 none layer_1/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/1-gemmascope-att-131k blocks.1.attn.hook_z 131072 none layer_2/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/2-gemmascope-att-131k blocks.2.attn.hook_z 131072 none layer_3/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/3-gemmascope-att-131k blocks.3.attn.hook_z 131072 none layer_4/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/4-gemmascope-att-131k blocks.4.attn.hook_z 131072 none layer_5/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/5-gemmascope-att-131k blocks.5.attn.hook_z 131072 none layer_6/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/6-gemmascope-att-131k blocks.6.attn.hook_z 131072 none layer_7/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/7-gemmascope-att-131k blocks.7.attn.hook_z 131072 none layer_8/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/8-gemmascope-att-131k blocks.8.attn.hook_z 131072 none layer_9/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/9-gemmascope-att-131k blocks.9.attn.hook_z 131072 none layer_10/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/10-gemmascope-att-131k blocks.10.attn.hook_z 131072 none layer_11/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/11-gemmascope-att-131k blocks.11.attn.hook_z 131072 none layer_12/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/12-gemmascope-att-131k blocks.12.attn.hook_z 131072 none layer_13/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/13-gemmascope-att-131k blocks.13.attn.hook_z 131072 none layer_14/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/14-gemmascope-att-131k blocks.14.attn.hook_z 131072 none layer_15/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/15-gemmascope-att-131k blocks.15.attn.hook_z 131072 none layer_16/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/16-gemmascope-att-131k blocks.16.attn.hook_z 131072 none layer_17/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/17-gemmascope-att-131k blocks.17.attn.hook_z 131072 none layer_18/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/18-gemmascope-att-131k blocks.18.attn.hook_z 131072 none layer_19/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/19-gemmascope-att-131k blocks.19.attn.hook_z 131072 none layer_20/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/20-gemmascope-att-131k blocks.20.attn.hook_z 131072 none layer_21/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/21-gemmascope-att-131k blocks.21.attn.hook_z 131072 none layer_22/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/22-gemmascope-att-131k blocks.22.attn.hook_z 131072 none layer_23/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/23-gemmascope-att-131k blocks.23.attn.hook_z 131072 none layer_24/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/24-gemmascope-att-131k blocks.24.attn.hook_z 131072 none layer_25/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/25-gemmascope-att-131k blocks.25.attn.hook_z 131072 none layer_26/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/26-gemmascope-att-131k blocks.26.attn.hook_z 131072 none layer_27/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/27-gemmascope-att-131k blocks.27.attn.hook_z 131072 none layer_28/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/28-gemmascope-att-131k blocks.28.attn.hook_z 131072 none layer_29/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/29-gemmascope-att-131k blocks.29.attn.hook_z 131072 none layer_30/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/30-gemmascope-att-131k blocks.30.attn.hook_z 131072 none layer_31/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/31-gemmascope-att-131k blocks.31.attn.hook_z 131072 none layer_32/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/32-gemmascope-att-131k blocks.32.attn.hook_z 131072 none layer_33/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/33-gemmascope-att-131k blocks.33.attn.hook_z 131072 none layer_34/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/34-gemmascope-att-131k blocks.34.attn.hook_z 131072 none layer_35/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/35-gemmascope-att-131k blocks.35.attn.hook_z 131072 none layer_36/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/36-gemmascope-att-131k blocks.36.attn.hook_z 131072 none layer_37/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/37-gemmascope-att-131k blocks.37.attn.hook_z 131072 none layer_38/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/38-gemmascope-att-131k blocks.38.attn.hook_z 131072 none layer_39/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/39-gemmascope-att-131k blocks.39.attn.hook_z 131072 none layer_40/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/40-gemmascope-att-131k blocks.40.attn.hook_z 131072 none layer_41/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/41-gemmascope-att-131k blocks.41.attn.hook_z 131072 none"},{"location":"sae_table/#gemma-scope-9b-pt-mlp","title":"gemma-scope-9b-pt-mlp","text":"<ul> <li>Huggingface Repo: google/gemma-scope-9b-pt-mlp</li> <li>model: gemma-2-9b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0/width_16k/average_l0_6Load this SAE jumprelu blocks.0.hook_mlp_out 16384 none layer_0/width_16k/average_l0_10Load this SAE jumprelu blocks.0.hook_mlp_out 16384 none layer_0/width_16k/average_l0_16Load this SAE jumprelu blocks.0.hook_mlp_out 16384 none layer_0/width_16k/average_l0_28Load this SAE jumprelu blocks.0.hook_mlp_out 16384 none layer_0/width_16k/average_l0_50Load this SAE jumprelu gemma-2-9b/0-gemmascope-mlp-16k__l0-50 blocks.0.hook_mlp_out 16384 none layer_0/width_131k/average_l0_3Load this SAE jumprelu blocks.0.hook_mlp_out 131072 none layer_0/width_131k/average_l0_5Load this SAE jumprelu blocks.0.hook_mlp_out 131072 none layer_0/width_131k/average_l0_7Load this SAE jumprelu blocks.0.hook_mlp_out 131072 none layer_0/width_131k/average_l0_11Load this SAE jumprelu blocks.0.hook_mlp_out 131072 none layer_0/width_131k/average_l0_18Load this SAE jumprelu blocks.0.hook_mlp_out 131072 none layer_0/width_131k/average_l0_30Load this SAE jumprelu blocks.0.hook_mlp_out 131072 none layer_1/width_16k/average_l0_12Load this SAE jumprelu blocks.1.hook_mlp_out 16384 none layer_1/width_16k/average_l0_26Load this SAE jumprelu blocks.1.hook_mlp_out 16384 none layer_1/width_16k/average_l0_56Load this SAE jumprelu gemma-2-9b/1-gemmascope-mlp-16k__l0-56 blocks.1.hook_mlp_out 16384 none layer_1/width_16k/average_l0_128Load this SAE jumprelu blocks.1.hook_mlp_out 16384 none layer_1/width_16k/average_l0_278Load this SAE jumprelu blocks.1.hook_mlp_out 16384 none layer_1/width_131k/average_l0_6Load this SAE jumprelu blocks.1.hook_mlp_out 131072 none layer_1/width_131k/average_l0_10Load this SAE jumprelu blocks.1.hook_mlp_out 131072 none layer_1/width_131k/average_l0_18Load this SAE jumprelu blocks.1.hook_mlp_out 131072 none layer_1/width_131k/average_l0_32Load this SAE jumprelu blocks.1.hook_mlp_out 131072 none layer_1/width_131k/average_l0_59Load this SAE jumprelu blocks.1.hook_mlp_out 131072 none layer_1/width_131k/average_l0_106Load this SAE jumprelu blocks.1.hook_mlp_out 131072 none layer_10/width_16k/average_l0_13Load this SAE jumprelu blocks.10.hook_mlp_out 16384 none layer_10/width_16k/average_l0_24Load this SAE jumprelu blocks.10.hook_mlp_out 16384 none layer_10/width_16k/average_l0_49Load this SAE jumprelu gemma-2-9b/10-gemmascope-mlp-16k__l0-49 blocks.10.hook_mlp_out 16384 none layer_10/width_16k/average_l0_114Load this SAE jumprelu blocks.10.hook_mlp_out 16384 none layer_10/width_16k/average_l0_276Load this SAE jumprelu blocks.10.hook_mlp_out 16384 none layer_10/width_131k/average_l0_12Load this SAE jumprelu blocks.10.hook_mlp_out 131072 none layer_10/width_131k/average_l0_22Load this SAE jumprelu blocks.10.hook_mlp_out 131072 none layer_10/width_131k/average_l0_42Load this SAE jumprelu blocks.10.hook_mlp_out 131072 none layer_10/width_131k/average_l0_83Load this SAE jumprelu blocks.10.hook_mlp_out 131072 none layer_10/width_131k/average_l0_167Load this SAE jumprelu blocks.10.hook_mlp_out 131072 none layer_10/width_131k/average_l0_362Load this SAE jumprelu blocks.10.hook_mlp_out 131072 none layer_11/width_16k/average_l0_17Load this SAE jumprelu blocks.11.hook_mlp_out 16384 none layer_11/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/11-gemmascope-mlp-16k__l0-34 blocks.11.hook_mlp_out 16384 none layer_11/width_16k/average_l0_76Load this SAE jumprelu blocks.11.hook_mlp_out 16384 none layer_11/width_16k/average_l0_180Load this SAE jumprelu blocks.11.hook_mlp_out 16384 none layer_11/width_16k/average_l0_421Load this SAE jumprelu blocks.11.hook_mlp_out 16384 none layer_11/width_131k/average_l0_17Load this SAE jumprelu blocks.11.hook_mlp_out 131072 none layer_11/width_131k/average_l0_31Load this SAE jumprelu blocks.11.hook_mlp_out 131072 none layer_11/width_131k/average_l0_60Load this SAE jumprelu blocks.11.hook_mlp_out 131072 none layer_11/width_131k/average_l0_120Load this SAE jumprelu blocks.11.hook_mlp_out 131072 none layer_11/width_131k/average_l0_259Load this SAE jumprelu blocks.11.hook_mlp_out 131072 none layer_11/width_131k/average_l0_569Load this SAE jumprelu blocks.11.hook_mlp_out 131072 none layer_12/width_16k/average_l0_20Load this SAE jumprelu blocks.12.hook_mlp_out 16384 none layer_12/width_16k/average_l0_42Load this SAE jumprelu gemma-2-9b/12-gemmascope-mlp-16k__l0-42 blocks.12.hook_mlp_out 16384 none layer_12/width_16k/average_l0_96Load this SAE jumprelu blocks.12.hook_mlp_out 16384 none layer_12/width_16k/average_l0_237Load this SAE jumprelu blocks.12.hook_mlp_out 16384 none layer_12/width_16k/average_l0_543Load this SAE jumprelu blocks.12.hook_mlp_out 16384 none layer_12/width_16k/average_l0_1033Load this SAE jumprelu blocks.12.hook_mlp_out 16384 none layer_12/width_131k/average_l0_19Load this SAE jumprelu blocks.12.hook_mlp_out 131072 none layer_12/width_131k/average_l0_38Load this SAE jumprelu blocks.12.hook_mlp_out 131072 none layer_12/width_131k/average_l0_77Load this SAE jumprelu blocks.12.hook_mlp_out 131072 none layer_12/width_131k/average_l0_159Load this SAE jumprelu blocks.12.hook_mlp_out 131072 none layer_12/width_131k/average_l0_338Load this SAE jumprelu blocks.12.hook_mlp_out 131072 none layer_12/width_131k/average_l0_745Load this SAE jumprelu blocks.12.hook_mlp_out 131072 none layer_13/width_16k/average_l0_19Load this SAE jumprelu blocks.13.hook_mlp_out 16384 none layer_13/width_16k/average_l0_40Load this SAE jumprelu gemma-2-9b/13-gemmascope-mlp-16k__l0-40 blocks.13.hook_mlp_out 16384 none layer_13/width_16k/average_l0_94Load this SAE jumprelu blocks.13.hook_mlp_out 16384 none layer_13/width_16k/average_l0_225Load this SAE jumprelu blocks.13.hook_mlp_out 16384 none layer_13/width_16k/average_l0_512Load this SAE jumprelu blocks.13.hook_mlp_out 16384 none layer_13/width_16k/average_l0_992Load this SAE jumprelu blocks.13.hook_mlp_out 16384 none layer_13/width_131k/average_l0_20Load this SAE jumprelu blocks.13.hook_mlp_out 131072 none layer_13/width_131k/average_l0_38Load this SAE jumprelu blocks.13.hook_mlp_out 131072 none layer_13/width_131k/average_l0_75Load this SAE jumprelu blocks.13.hook_mlp_out 131072 none layer_13/width_131k/average_l0_160Load this SAE jumprelu blocks.13.hook_mlp_out 131072 none layer_13/width_131k/average_l0_351Load this SAE jumprelu blocks.13.hook_mlp_out 131072 none layer_13/width_131k/average_l0_703Load this SAE jumprelu blocks.13.hook_mlp_out 131072 none layer_14/width_16k/average_l0_19Load this SAE jumprelu blocks.14.hook_mlp_out 16384 none layer_14/width_16k/average_l0_41Load this SAE jumprelu gemma-2-9b/14-gemmascope-mlp-16k__l0-41 blocks.14.hook_mlp_out 16384 none layer_14/width_16k/average_l0_97Load this SAE jumprelu blocks.14.hook_mlp_out 16384 none layer_14/width_16k/average_l0_236Load this SAE jumprelu blocks.14.hook_mlp_out 16384 none layer_14/width_16k/average_l0_538Load this SAE jumprelu blocks.14.hook_mlp_out 16384 none layer_14/width_16k/average_l0_1023Load this SAE jumprelu blocks.14.hook_mlp_out 16384 none layer_14/width_131k/average_l0_20Load this SAE jumprelu blocks.14.hook_mlp_out 131072 none layer_14/width_131k/average_l0_40Load this SAE jumprelu blocks.14.hook_mlp_out 131072 none layer_14/width_131k/average_l0_80Load this SAE jumprelu blocks.14.hook_mlp_out 131072 none layer_14/width_131k/average_l0_174Load this SAE jumprelu blocks.14.hook_mlp_out 131072 none layer_14/width_131k/average_l0_374Load this SAE jumprelu blocks.14.hook_mlp_out 131072 none layer_14/width_131k/average_l0_743Load this SAE jumprelu blocks.14.hook_mlp_out 131072 none layer_15/width_16k/average_l0_20Load this SAE jumprelu blocks.15.hook_mlp_out 16384 none layer_15/width_16k/average_l0_45Load this SAE jumprelu gemma-2-9b/15-gemmascope-mlp-16k__l0-45 blocks.15.hook_mlp_out 16384 none layer_15/width_16k/average_l0_107Load this SAE jumprelu blocks.15.hook_mlp_out 16384 none layer_15/width_16k/average_l0_260Load this SAE jumprelu blocks.15.hook_mlp_out 16384 none layer_15/width_16k/average_l0_572Load this SAE jumprelu blocks.15.hook_mlp_out 16384 none layer_15/width_16k/average_l0_1053Load this SAE jumprelu blocks.15.hook_mlp_out 16384 none layer_15/width_131k/average_l0_21Load this SAE jumprelu blocks.15.hook_mlp_out 131072 none layer_15/width_131k/average_l0_43Load this SAE jumprelu blocks.15.hook_mlp_out 131072 none layer_15/width_131k/average_l0_89Load this SAE jumprelu blocks.15.hook_mlp_out 131072 none layer_15/width_131k/average_l0_194Load this SAE jumprelu blocks.15.hook_mlp_out 131072 none layer_15/width_131k/average_l0_421Load this SAE jumprelu blocks.15.hook_mlp_out 131072 none layer_15/width_131k/average_l0_828Load this SAE jumprelu blocks.15.hook_mlp_out 131072 none layer_16/width_16k/average_l0_16Load this SAE jumprelu blocks.16.hook_mlp_out 16384 none layer_16/width_16k/average_l0_37Load this SAE jumprelu gemma-2-9b/16-gemmascope-mlp-16k__l0-37 blocks.16.hook_mlp_out 16384 none layer_16/width_16k/average_l0_91Load this SAE jumprelu blocks.16.hook_mlp_out 16384 none layer_16/width_16k/average_l0_221Load this SAE jumprelu blocks.16.hook_mlp_out 16384 none layer_16/width_16k/average_l0_489Load this SAE jumprelu blocks.16.hook_mlp_out 16384 none layer_16/width_16k/average_l0_916Load this SAE jumprelu blocks.16.hook_mlp_out 16384 none layer_16/width_131k/average_l0_17Load this SAE jumprelu blocks.16.hook_mlp_out 131072 none layer_16/width_131k/average_l0_38Load this SAE jumprelu blocks.16.hook_mlp_out 131072 none layer_16/width_131k/average_l0_81Load this SAE jumprelu blocks.16.hook_mlp_out 131072 none layer_16/width_131k/average_l0_175Load this SAE jumprelu blocks.16.hook_mlp_out 131072 none layer_16/width_131k/average_l0_384Load this SAE jumprelu blocks.16.hook_mlp_out 131072 none layer_16/width_131k/average_l0_744Load this SAE jumprelu blocks.16.hook_mlp_out 131072 none layer_17/width_16k/average_l0_18Load this SAE jumprelu blocks.17.hook_mlp_out 16384 none layer_17/width_16k/average_l0_41Load this SAE jumprelu gemma-2-9b/17-gemmascope-mlp-16k__l0-41 blocks.17.hook_mlp_out 16384 none layer_17/width_16k/average_l0_104Load this SAE jumprelu blocks.17.hook_mlp_out 16384 none layer_17/width_16k/average_l0_274Load this SAE jumprelu blocks.17.hook_mlp_out 16384 none layer_17/width_16k/average_l0_624Load this SAE jumprelu blocks.17.hook_mlp_out 16384 none layer_17/width_16k/average_l0_1137Load this SAE jumprelu blocks.17.hook_mlp_out 16384 none layer_17/width_131k/average_l0_22Load this SAE jumprelu blocks.17.hook_mlp_out 131072 none layer_17/width_131k/average_l0_43Load this SAE jumprelu blocks.17.hook_mlp_out 131072 none layer_17/width_131k/average_l0_91Load this SAE jumprelu blocks.17.hook_mlp_out 131072 none layer_17/width_131k/average_l0_207Load this SAE jumprelu blocks.17.hook_mlp_out 131072 none layer_17/width_131k/average_l0_483Load this SAE jumprelu blocks.17.hook_mlp_out 131072 none layer_17/width_131k/average_l0_950Load this SAE jumprelu blocks.17.hook_mlp_out 131072 none layer_18/width_16k/average_l0_16Load this SAE jumprelu blocks.18.hook_mlp_out 16384 none layer_18/width_16k/average_l0_36Load this SAE jumprelu gemma-2-9b/18-gemmascope-mlp-16k__l0-36 blocks.18.hook_mlp_out 16384 none layer_18/width_16k/average_l0_89Load this SAE jumprelu blocks.18.hook_mlp_out 16384 none layer_18/width_16k/average_l0_235Load this SAE jumprelu blocks.18.hook_mlp_out 16384 none layer_18/width_16k/average_l0_564Load this SAE jumprelu blocks.18.hook_mlp_out 16384 none layer_18/width_16k/average_l0_1052Load this SAE jumprelu blocks.18.hook_mlp_out 16384 none layer_18/width_131k/average_l0_19Load this SAE jumprelu blocks.18.hook_mlp_out 131072 none layer_18/width_131k/average_l0_37Load this SAE jumprelu blocks.18.hook_mlp_out 131072 none layer_18/width_131k/average_l0_82Load this SAE jumprelu blocks.18.hook_mlp_out 131072 none layer_18/width_131k/average_l0_174Load this SAE jumprelu blocks.18.hook_mlp_out 131072 none layer_18/width_131k/average_l0_420Load this SAE jumprelu blocks.18.hook_mlp_out 131072 none layer_18/width_131k/average_l0_865Load this SAE jumprelu blocks.18.hook_mlp_out 131072 none layer_19/width_16k/average_l0_16Load this SAE jumprelu blocks.19.hook_mlp_out 16384 none layer_19/width_16k/average_l0_38Load this SAE jumprelu gemma-2-9b/19-gemmascope-mlp-16k__l0-38 blocks.19.hook_mlp_out 16384 none layer_19/width_16k/average_l0_98Load this SAE jumprelu blocks.19.hook_mlp_out 16384 none layer_19/width_16k/average_l0_255Load this SAE jumprelu blocks.19.hook_mlp_out 16384 none layer_19/width_16k/average_l0_599Load this SAE jumprelu blocks.19.hook_mlp_out 16384 none layer_19/width_16k/average_l0_1097Load this SAE jumprelu blocks.19.hook_mlp_out 16384 none layer_19/width_131k/average_l0_19Load this SAE jumprelu blocks.19.hook_mlp_out 131072 none layer_19/width_131k/average_l0_40Load this SAE jumprelu blocks.19.hook_mlp_out 131072 none layer_19/width_131k/average_l0_85Load this SAE jumprelu blocks.19.hook_mlp_out 131072 none layer_19/width_131k/average_l0_189Load this SAE jumprelu blocks.19.hook_mlp_out 131072 none layer_19/width_131k/average_l0_440Load this SAE jumprelu blocks.19.hook_mlp_out 131072 none layer_19/width_131k/average_l0_899Load this SAE jumprelu blocks.19.hook_mlp_out 131072 none layer_2/width_16k/average_l0_8Load this SAE jumprelu blocks.2.hook_mlp_out 16384 none layer_2/width_16k/average_l0_16Load this SAE jumprelu blocks.2.hook_mlp_out 16384 none layer_2/width_16k/average_l0_33Load this SAE jumprelu gemma-2-9b/2-gemmascope-mlp-16k__l0-33 blocks.2.hook_mlp_out 16384 none layer_2/width_16k/average_l0_81Load this SAE jumprelu blocks.2.hook_mlp_out 16384 none layer_2/width_16k/average_l0_163Load this SAE jumprelu blocks.2.hook_mlp_out 16384 none layer_2/width_131k/average_l0_4Load this SAE jumprelu blocks.2.hook_mlp_out 131072 none layer_2/width_131k/average_l0_7Load this SAE jumprelu blocks.2.hook_mlp_out 131072 none layer_2/width_131k/average_l0_12Load this SAE jumprelu blocks.2.hook_mlp_out 131072 none layer_2/width_131k/average_l0_20Load this SAE jumprelu blocks.2.hook_mlp_out 131072 none layer_2/width_131k/average_l0_35Load this SAE jumprelu blocks.2.hook_mlp_out 131072 none layer_2/width_131k/average_l0_61Load this SAE jumprelu blocks.2.hook_mlp_out 131072 none layer_20/width_16k/average_l0_17Load this SAE jumprelu blocks.20.hook_mlp_out 16384 none layer_20/width_16k/average_l0_41Load this SAE jumprelu gemma-2-9b/20-gemmascope-mlp-16k__l0-41 blocks.20.hook_mlp_out 16384 none layer_20/width_16k/average_l0_108Load this SAE jumprelu blocks.20.hook_mlp_out 16384 none layer_20/width_16k/average_l0_284Load this SAE jumprelu blocks.20.hook_mlp_out 16384 none layer_20/width_16k/average_l0_643Load this SAE jumprelu blocks.20.hook_mlp_out 16384 none layer_20/width_16k/average_l0_1127Load this SAE jumprelu blocks.20.hook_mlp_out 16384 none layer_20/width_131k/average_l0_20Load this SAE jumprelu blocks.20.hook_mlp_out 131072 none layer_20/width_131k/average_l0_41Load this SAE jumprelu blocks.20.hook_mlp_out 131072 none layer_20/width_131k/average_l0_93Load this SAE jumprelu blocks.20.hook_mlp_out 131072 none layer_20/width_131k/average_l0_212Load this SAE jumprelu blocks.20.hook_mlp_out 131072 none layer_20/width_131k/average_l0_477Load this SAE jumprelu blocks.20.hook_mlp_out 131072 none layer_20/width_131k/average_l0_992Load this SAE jumprelu blocks.20.hook_mlp_out 131072 none layer_21/width_16k/average_l0_15Load this SAE jumprelu blocks.21.hook_mlp_out 16384 none layer_21/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/21-gemmascope-mlp-16k__l0-34 blocks.21.hook_mlp_out 16384 none layer_21/width_16k/average_l0_88Load this SAE jumprelu blocks.21.hook_mlp_out 16384 none layer_21/width_16k/average_l0_241Load this SAE jumprelu blocks.21.hook_mlp_out 16384 none layer_21/width_16k/average_l0_571Load this SAE jumprelu blocks.21.hook_mlp_out 16384 none layer_21/width_16k/average_l0_1063Load this SAE jumprelu blocks.21.hook_mlp_out 16384 none layer_21/width_131k/average_l0_16Load this SAE jumprelu blocks.21.hook_mlp_out 131072 none layer_21/width_131k/average_l0_35Load this SAE jumprelu blocks.21.hook_mlp_out 131072 none layer_21/width_131k/average_l0_79Load this SAE jumprelu blocks.21.hook_mlp_out 131072 none layer_21/width_131k/average_l0_179Load this SAE jumprelu blocks.21.hook_mlp_out 131072 none layer_21/width_131k/average_l0_417Load this SAE jumprelu blocks.21.hook_mlp_out 131072 none layer_21/width_131k/average_l0_884Load this SAE jumprelu blocks.21.hook_mlp_out 131072 none layer_22/width_16k/average_l0_15Load this SAE jumprelu blocks.22.hook_mlp_out 16384 none layer_22/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/22-gemmascope-mlp-16k__l0-34 blocks.22.hook_mlp_out 16384 none layer_22/width_16k/average_l0_85Load this SAE jumprelu blocks.22.hook_mlp_out 16384 none layer_22/width_16k/average_l0_226Load this SAE jumprelu blocks.22.hook_mlp_out 16384 none layer_22/width_16k/average_l0_566Load this SAE jumprelu blocks.22.hook_mlp_out 16384 none layer_22/width_16k/average_l0_1065Load this SAE jumprelu blocks.22.hook_mlp_out 16384 none layer_22/width_131k/average_l0_17Load this SAE jumprelu blocks.22.hook_mlp_out 131072 none layer_22/width_131k/average_l0_35Load this SAE jumprelu blocks.22.hook_mlp_out 131072 none layer_22/width_131k/average_l0_77Load this SAE jumprelu blocks.22.hook_mlp_out 131072 none layer_22/width_131k/average_l0_172Load this SAE jumprelu blocks.22.hook_mlp_out 131072 none layer_22/width_131k/average_l0_404Load this SAE jumprelu blocks.22.hook_mlp_out 131072 none layer_22/width_131k/average_l0_861Load this SAE jumprelu blocks.22.hook_mlp_out 131072 none layer_23/width_16k/average_l0_15Load this SAE jumprelu blocks.23.hook_mlp_out 16384 none layer_23/width_16k/average_l0_31Load this SAE jumprelu blocks.23.hook_mlp_out 16384 none layer_23/width_16k/average_l0_73Load this SAE jumprelu gemma-2-9b/23-gemmascope-mlp-16k__l0-73 blocks.23.hook_mlp_out 16384 none layer_23/width_16k/average_l0_190Load this SAE jumprelu blocks.23.hook_mlp_out 16384 none layer_23/width_16k/average_l0_492Load this SAE jumprelu blocks.23.hook_mlp_out 16384 none layer_23/width_16k/average_l0_990Load this SAE jumprelu blocks.23.hook_mlp_out 16384 none layer_23/width_131k/average_l0_17Load this SAE jumprelu blocks.23.hook_mlp_out 131072 none layer_23/width_131k/average_l0_32Load this SAE jumprelu blocks.23.hook_mlp_out 131072 none layer_23/width_131k/average_l0_67Load this SAE jumprelu blocks.23.hook_mlp_out 131072 none layer_23/width_131k/average_l0_146Load this SAE jumprelu blocks.23.hook_mlp_out 131072 none layer_23/width_131k/average_l0_354Load this SAE jumprelu blocks.23.hook_mlp_out 131072 none layer_23/width_131k/average_l0_754Load this SAE jumprelu blocks.23.hook_mlp_out 131072 none layer_24/width_16k/average_l0_15Load this SAE jumprelu blocks.24.hook_mlp_out 16384 none layer_24/width_16k/average_l0_32Load this SAE jumprelu gemma-2-9b/24-gemmascope-mlp-16k__l0-32 blocks.24.hook_mlp_out 16384 none layer_24/width_16k/average_l0_73Load this SAE jumprelu blocks.24.hook_mlp_out 16384 none layer_24/width_16k/average_l0_192Load this SAE jumprelu blocks.24.hook_mlp_out 16384 none layer_24/width_16k/average_l0_494Load this SAE jumprelu blocks.24.hook_mlp_out 16384 none layer_24/width_16k/average_l0_999Load this SAE jumprelu blocks.24.hook_mlp_out 16384 none layer_24/width_131k/average_l0_17Load this SAE jumprelu blocks.24.hook_mlp_out 131072 none layer_24/width_131k/average_l0_33Load this SAE jumprelu blocks.24.hook_mlp_out 131072 none layer_24/width_131k/average_l0_67Load this SAE jumprelu blocks.24.hook_mlp_out 131072 none layer_24/width_131k/average_l0_147Load this SAE jumprelu blocks.24.hook_mlp_out 131072 none layer_24/width_131k/average_l0_351Load this SAE jumprelu blocks.24.hook_mlp_out 131072 none layer_24/width_131k/average_l0_709Load this SAE jumprelu blocks.24.hook_mlp_out 131072 none layer_25/width_16k/average_l0_15Load this SAE jumprelu blocks.25.hook_mlp_out 16384 none layer_25/width_16k/average_l0_31Load this SAE jumprelu blocks.25.hook_mlp_out 16384 none layer_25/width_16k/average_l0_72Load this SAE jumprelu gemma-2-9b/25-gemmascope-mlp-16k__l0-72 blocks.25.hook_mlp_out 16384 none layer_25/width_16k/average_l0_184Load this SAE jumprelu blocks.25.hook_mlp_out 16384 none layer_25/width_16k/average_l0_494Load this SAE jumprelu blocks.25.hook_mlp_out 16384 none layer_25/width_16k/average_l0_1013Load this SAE jumprelu blocks.25.hook_mlp_out 16384 none layer_25/width_131k/average_l0_16Load this SAE jumprelu blocks.25.hook_mlp_out 131072 none layer_25/width_131k/average_l0_32Load this SAE jumprelu blocks.25.hook_mlp_out 131072 none layer_25/width_131k/average_l0_65Load this SAE jumprelu blocks.25.hook_mlp_out 131072 none layer_25/width_131k/average_l0_139Load this SAE jumprelu blocks.25.hook_mlp_out 131072 none layer_25/width_131k/average_l0_326Load this SAE jumprelu blocks.25.hook_mlp_out 131072 none layer_25/width_131k/average_l0_691Load this SAE jumprelu blocks.25.hook_mlp_out 131072 none layer_26/width_16k/average_l0_14Load this SAE jumprelu blocks.26.hook_mlp_out 16384 none layer_26/width_16k/average_l0_26Load this SAE jumprelu blocks.26.hook_mlp_out 16384 none layer_26/width_16k/average_l0_57Load this SAE jumprelu gemma-2-9b/26-gemmascope-mlp-16k__l0-57 blocks.26.hook_mlp_out 16384 none layer_26/width_16k/average_l0_142Load this SAE jumprelu blocks.26.hook_mlp_out 16384 none layer_26/width_16k/average_l0_394Load this SAE jumprelu blocks.26.hook_mlp_out 16384 none layer_26/width_16k/average_l0_887Load this SAE jumprelu blocks.26.hook_mlp_out 16384 none layer_26/width_131k/average_l0_14Load this SAE jumprelu blocks.26.hook_mlp_out 131072 none layer_26/width_131k/average_l0_27Load this SAE jumprelu blocks.26.hook_mlp_out 131072 none layer_26/width_131k/average_l0_53Load this SAE jumprelu blocks.26.hook_mlp_out 131072 none layer_26/width_131k/average_l0_110Load this SAE jumprelu blocks.26.hook_mlp_out 131072 none layer_26/width_131k/average_l0_249Load this SAE jumprelu blocks.26.hook_mlp_out 131072 none layer_26/width_131k/average_l0_568Load this SAE jumprelu blocks.26.hook_mlp_out 131072 none layer_27/width_16k/average_l0_14Load this SAE jumprelu blocks.27.hook_mlp_out 16384 none layer_27/width_16k/average_l0_25Load this SAE jumprelu blocks.27.hook_mlp_out 16384 none layer_27/width_16k/average_l0_52Load this SAE jumprelu gemma-2-9b/27-gemmascope-mlp-16k__l0-52 blocks.27.hook_mlp_out 16384 none layer_27/width_16k/average_l0_126Load this SAE jumprelu blocks.27.hook_mlp_out 16384 none layer_27/width_16k/average_l0_352Load this SAE jumprelu blocks.27.hook_mlp_out 16384 none layer_27/width_16k/average_l0_813Load this SAE jumprelu blocks.27.hook_mlp_out 16384 none layer_27/width_131k/average_l0_14Load this SAE jumprelu blocks.27.hook_mlp_out 131072 none layer_27/width_131k/average_l0_26Load this SAE jumprelu blocks.27.hook_mlp_out 131072 none layer_27/width_131k/average_l0_49Load this SAE jumprelu blocks.27.hook_mlp_out 131072 none layer_27/width_131k/average_l0_99Load this SAE jumprelu blocks.27.hook_mlp_out 131072 none layer_27/width_131k/average_l0_211Load this SAE jumprelu blocks.27.hook_mlp_out 131072 none layer_27/width_131k/average_l0_487Load this SAE jumprelu blocks.27.hook_mlp_out 131072 none layer_28/width_16k/average_l0_14Load this SAE jumprelu blocks.28.hook_mlp_out 16384 none layer_28/width_16k/average_l0_26Load this SAE jumprelu blocks.28.hook_mlp_out 16384 none layer_28/width_16k/average_l0_50Load this SAE jumprelu gemma-2-9b/28-gemmascope-mlp-16k__l0-50 blocks.28.hook_mlp_out 16384 none layer_28/width_16k/average_l0_115Load this SAE jumprelu blocks.28.hook_mlp_out 16384 none layer_28/width_16k/average_l0_324Load this SAE jumprelu blocks.28.hook_mlp_out 16384 none layer_28/width_16k/average_l0_773Load this SAE jumprelu blocks.28.hook_mlp_out 16384 none layer_28/width_131k/average_l0_15Load this SAE jumprelu blocks.28.hook_mlp_out 131072 none layer_28/width_131k/average_l0_26Load this SAE jumprelu blocks.28.hook_mlp_out 131072 none layer_28/width_131k/average_l0_47Load this SAE jumprelu blocks.28.hook_mlp_out 131072 none layer_28/width_131k/average_l0_91Load this SAE jumprelu blocks.28.hook_mlp_out 131072 none layer_28/width_131k/average_l0_189Load this SAE jumprelu blocks.28.hook_mlp_out 131072 none layer_28/width_131k/average_l0_425Load this SAE jumprelu blocks.28.hook_mlp_out 131072 none layer_29/width_16k/average_l0_15Load this SAE jumprelu blocks.29.hook_mlp_out 16384 none layer_29/width_16k/average_l0_26Load this SAE jumprelu blocks.29.hook_mlp_out 16384 none layer_29/width_16k/average_l0_49Load this SAE jumprelu gemma-2-9b/29-gemmascope-mlp-16k__l0-49 blocks.29.hook_mlp_out 16384 none layer_29/width_16k/average_l0_111Load this SAE jumprelu blocks.29.hook_mlp_out 16384 none layer_29/width_16k/average_l0_311Load this SAE jumprelu blocks.29.hook_mlp_out 16384 none layer_29/width_16k/average_l0_725Load this SAE jumprelu blocks.29.hook_mlp_out 16384 none layer_29/width_131k/average_l0_15Load this SAE jumprelu blocks.29.hook_mlp_out 131072 none layer_29/width_131k/average_l0_26Load this SAE jumprelu blocks.29.hook_mlp_out 131072 none layer_29/width_131k/average_l0_47Load this SAE jumprelu blocks.29.hook_mlp_out 131072 none layer_29/width_131k/average_l0_87Load this SAE jumprelu blocks.29.hook_mlp_out 131072 none layer_29/width_131k/average_l0_174Load this SAE jumprelu blocks.29.hook_mlp_out 131072 none layer_29/width_131k/average_l0_386Load this SAE jumprelu blocks.29.hook_mlp_out 131072 none layer_3/width_16k/average_l0_13Load this SAE jumprelu blocks.3.hook_mlp_out 16384 none layer_3/width_16k/average_l0_25Load this SAE jumprelu blocks.3.hook_mlp_out 16384 none layer_3/width_16k/average_l0_55Load this SAE jumprelu gemma-2-9b/3-gemmascope-mlp-16k__l0-55 blocks.3.hook_mlp_out 16384 none layer_3/width_16k/average_l0_126Load this SAE jumprelu blocks.3.hook_mlp_out 16384 none layer_3/width_16k/average_l0_234Load this SAE jumprelu blocks.3.hook_mlp_out 16384 none layer_3/width_131k/average_l0_6Load this SAE jumprelu blocks.3.hook_mlp_out 131072 none layer_3/width_131k/average_l0_11Load this SAE jumprelu blocks.3.hook_mlp_out 131072 none layer_3/width_131k/average_l0_19Load this SAE jumprelu blocks.3.hook_mlp_out 131072 none layer_3/width_131k/average_l0_33Load this SAE jumprelu blocks.3.hook_mlp_out 131072 none layer_3/width_131k/average_l0_58Load this SAE jumprelu blocks.3.hook_mlp_out 131072 none layer_3/width_131k/average_l0_109Load this SAE jumprelu blocks.3.hook_mlp_out 131072 none layer_30/width_16k/average_l0_14Load this SAE jumprelu blocks.30.hook_mlp_out 16384 none layer_30/width_16k/average_l0_26Load this SAE jumprelu blocks.30.hook_mlp_out 16384 none layer_30/width_16k/average_l0_51Load this SAE jumprelu gemma-2-9b/30-gemmascope-mlp-16k__l0-51 blocks.30.hook_mlp_out 16384 none layer_30/width_16k/average_l0_116Load this SAE jumprelu blocks.30.hook_mlp_out 16384 none layer_30/width_16k/average_l0_333Load this SAE jumprelu blocks.30.hook_mlp_out 16384 none layer_30/width_16k/average_l0_806Load this SAE jumprelu blocks.30.hook_mlp_out 16384 none layer_30/width_131k/average_l0_14Load this SAE jumprelu blocks.30.hook_mlp_out 131072 none layer_30/width_131k/average_l0_26Load this SAE jumprelu blocks.30.hook_mlp_out 131072 none layer_30/width_131k/average_l0_47Load this SAE jumprelu blocks.30.hook_mlp_out 131072 none layer_30/width_131k/average_l0_89Load this SAE jumprelu blocks.30.hook_mlp_out 131072 none layer_30/width_131k/average_l0_179Load this SAE jumprelu blocks.30.hook_mlp_out 131072 none layer_30/width_131k/average_l0_391Load this SAE jumprelu blocks.30.hook_mlp_out 131072 none layer_31/width_16k/average_l0_12Load this SAE jumprelu blocks.31.hook_mlp_out 16384 none layer_31/width_16k/average_l0_22Load this SAE jumprelu blocks.31.hook_mlp_out 16384 none layer_31/width_16k/average_l0_43Load this SAE jumprelu gemma-2-9b/31-gemmascope-mlp-16k__l0-43 blocks.31.hook_mlp_out 16384 none layer_31/width_16k/average_l0_94Load this SAE jumprelu blocks.31.hook_mlp_out 16384 none layer_31/width_16k/average_l0_263Load this SAE jumprelu blocks.31.hook_mlp_out 16384 none layer_31/width_16k/average_l0_671Load this SAE jumprelu blocks.31.hook_mlp_out 16384 none layer_31/width_131k/average_l0_12Load this SAE jumprelu blocks.31.hook_mlp_out 131072 none layer_31/width_131k/average_l0_22Load this SAE jumprelu blocks.31.hook_mlp_out 131072 none layer_31/width_131k/average_l0_40Load this SAE jumprelu blocks.31.hook_mlp_out 131072 none layer_31/width_131k/average_l0_77Load this SAE jumprelu blocks.31.hook_mlp_out 131072 none layer_31/width_131k/average_l0_153Load this SAE jumprelu blocks.31.hook_mlp_out 131072 none layer_31/width_131k/average_l0_326Load this SAE jumprelu blocks.31.hook_mlp_out 131072 none layer_32/width_16k/average_l0_11Load this SAE jumprelu blocks.32.hook_mlp_out 16384 none layer_32/width_16k/average_l0_21Load this SAE jumprelu blocks.32.hook_mlp_out 16384 none layer_32/width_16k/average_l0_44Load this SAE jumprelu gemma-2-9b/32-gemmascope-mlp-16k__l0-44 blocks.32.hook_mlp_out 16384 none layer_32/width_16k/average_l0_98Load this SAE jumprelu blocks.32.hook_mlp_out 16384 none layer_32/width_16k/average_l0_267Load this SAE jumprelu blocks.32.hook_mlp_out 16384 none layer_32/width_16k/average_l0_623Load this SAE jumprelu blocks.32.hook_mlp_out 16384 none layer_32/width_131k/average_l0_12Load this SAE jumprelu blocks.32.hook_mlp_out 131072 none layer_32/width_131k/average_l0_21Load this SAE jumprelu blocks.32.hook_mlp_out 131072 none layer_32/width_131k/average_l0_40Load this SAE jumprelu blocks.32.hook_mlp_out 131072 none layer_32/width_131k/average_l0_76Load this SAE jumprelu blocks.32.hook_mlp_out 131072 none layer_32/width_131k/average_l0_155Load this SAE jumprelu blocks.32.hook_mlp_out 131072 none layer_32/width_131k/average_l0_336Load this SAE jumprelu blocks.32.hook_mlp_out 131072 none layer_33/width_16k/average_l0_12Load this SAE jumprelu blocks.33.hook_mlp_out 16384 none layer_33/width_16k/average_l0_23Load this SAE jumprelu blocks.33.hook_mlp_out 16384 none layer_33/width_16k/average_l0_48Load this SAE jumprelu gemma-2-9b/33-gemmascope-mlp-16k__l0-48 blocks.33.hook_mlp_out 16384 none layer_33/width_16k/average_l0_107Load this SAE jumprelu blocks.33.hook_mlp_out 16384 none layer_33/width_16k/average_l0_282Load this SAE jumprelu blocks.33.hook_mlp_out 16384 none layer_33/width_16k/average_l0_628Load this SAE jumprelu blocks.33.hook_mlp_out 16384 none layer_33/width_131k/average_l0_12Load this SAE jumprelu blocks.33.hook_mlp_out 131072 none layer_33/width_131k/average_l0_22Load this SAE jumprelu blocks.33.hook_mlp_out 131072 none layer_33/width_131k/average_l0_42Load this SAE jumprelu blocks.33.hook_mlp_out 131072 none layer_33/width_131k/average_l0_83Load this SAE jumprelu blocks.33.hook_mlp_out 131072 none layer_33/width_131k/average_l0_168Load this SAE jumprelu blocks.33.hook_mlp_out 131072 none layer_33/width_131k/average_l0_394Load this SAE jumprelu blocks.33.hook_mlp_out 131072 none layer_34/width_16k/average_l0_11Load this SAE jumprelu blocks.34.hook_mlp_out 16384 none layer_34/width_16k/average_l0_22Load this SAE jumprelu blocks.34.hook_mlp_out 16384 none layer_34/width_16k/average_l0_47Load this SAE jumprelu gemma-2-9b/34-gemmascope-mlp-16k__l0-47 blocks.34.hook_mlp_out 16384 none layer_34/width_16k/average_l0_107Load this SAE jumprelu blocks.34.hook_mlp_out 16384 none layer_34/width_16k/average_l0_268Load this SAE jumprelu blocks.34.hook_mlp_out 16384 none layer_34/width_16k/average_l0_559Load this SAE jumprelu blocks.34.hook_mlp_out 16384 none layer_34/width_131k/average_l0_10Load this SAE jumprelu blocks.34.hook_mlp_out 131072 none layer_34/width_131k/average_l0_20Load this SAE jumprelu blocks.34.hook_mlp_out 131072 none layer_34/width_131k/average_l0_40Load this SAE jumprelu blocks.34.hook_mlp_out 131072 none layer_34/width_131k/average_l0_82Load this SAE jumprelu blocks.34.hook_mlp_out 131072 none layer_34/width_131k/average_l0_167Load this SAE jumprelu blocks.34.hook_mlp_out 131072 none layer_34/width_131k/average_l0_390Load this SAE jumprelu blocks.34.hook_mlp_out 131072 none layer_35/width_16k/average_l0_10Load this SAE jumprelu blocks.35.hook_mlp_out 16384 none layer_35/width_16k/average_l0_21Load this SAE jumprelu blocks.35.hook_mlp_out 16384 none layer_35/width_16k/average_l0_46Load this SAE jumprelu gemma-2-9b/35-gemmascope-mlp-16k__l0-46 blocks.35.hook_mlp_out 16384 none layer_35/width_16k/average_l0_108Load this SAE jumprelu blocks.35.hook_mlp_out 16384 none layer_35/width_16k/average_l0_254Load this SAE jumprelu blocks.35.hook_mlp_out 16384 none layer_35/width_16k/average_l0_538Load this SAE jumprelu blocks.35.hook_mlp_out 16384 none layer_35/width_131k/average_l0_10Load this SAE jumprelu blocks.35.hook_mlp_out 131072 none layer_35/width_131k/average_l0_19Load this SAE jumprelu blocks.35.hook_mlp_out 131072 none layer_35/width_131k/average_l0_39Load this SAE jumprelu blocks.35.hook_mlp_out 131072 none layer_35/width_131k/average_l0_80Load this SAE jumprelu blocks.35.hook_mlp_out 131072 none layer_35/width_131k/average_l0_176Load this SAE jumprelu blocks.35.hook_mlp_out 131072 none layer_35/width_131k/average_l0_389Load this SAE jumprelu blocks.35.hook_mlp_out 131072 none layer_36/width_16k/average_l0_11Load this SAE jumprelu blocks.36.hook_mlp_out 16384 none layer_36/width_16k/average_l0_22Load this SAE jumprelu blocks.36.hook_mlp_out 16384 none layer_36/width_16k/average_l0_47Load this SAE jumprelu gemma-2-9b/36-gemmascope-mlp-16k__l0-47 blocks.36.hook_mlp_out 16384 none layer_36/width_16k/average_l0_109Load this SAE jumprelu blocks.36.hook_mlp_out 16384 none layer_36/width_16k/average_l0_256Load this SAE jumprelu blocks.36.hook_mlp_out 16384 none layer_36/width_16k/average_l0_523Load this SAE jumprelu blocks.36.hook_mlp_out 16384 none layer_36/width_131k/average_l0_11Load this SAE jumprelu blocks.36.hook_mlp_out 131072 none layer_36/width_131k/average_l0_20Load this SAE jumprelu blocks.36.hook_mlp_out 131072 none layer_36/width_131k/average_l0_40Load this SAE jumprelu blocks.36.hook_mlp_out 131072 none layer_36/width_131k/average_l0_81Load this SAE jumprelu blocks.36.hook_mlp_out 131072 none layer_36/width_131k/average_l0_174Load this SAE jumprelu blocks.36.hook_mlp_out 131072 none layer_36/width_131k/average_l0_389Load this SAE jumprelu blocks.36.hook_mlp_out 131072 none layer_37/width_16k/average_l0_12Load this SAE jumprelu blocks.37.hook_mlp_out 16384 none layer_37/width_16k/average_l0_24Load this SAE jumprelu blocks.37.hook_mlp_out 16384 none layer_37/width_16k/average_l0_53Load this SAE jumprelu gemma-2-9b/37-gemmascope-mlp-16k__l0-53 blocks.37.hook_mlp_out 16384 none layer_37/width_16k/average_l0_119Load this SAE jumprelu blocks.37.hook_mlp_out 16384 none layer_37/width_16k/average_l0_267Load this SAE jumprelu blocks.37.hook_mlp_out 16384 none layer_37/width_16k/average_l0_549Load this SAE jumprelu blocks.37.hook_mlp_out 16384 none layer_37/width_131k/average_l0_12Load this SAE jumprelu blocks.37.hook_mlp_out 131072 none layer_37/width_131k/average_l0_23Load this SAE jumprelu blocks.37.hook_mlp_out 131072 none layer_37/width_131k/average_l0_44Load this SAE jumprelu blocks.37.hook_mlp_out 131072 none layer_37/width_131k/average_l0_90Load this SAE jumprelu blocks.37.hook_mlp_out 131072 none layer_37/width_131k/average_l0_203Load this SAE jumprelu blocks.37.hook_mlp_out 131072 none layer_37/width_131k/average_l0_403Load this SAE jumprelu blocks.37.hook_mlp_out 131072 none layer_38/width_16k/average_l0_11Load this SAE jumprelu blocks.38.hook_mlp_out 16384 none layer_38/width_16k/average_l0_22Load this SAE jumprelu blocks.38.hook_mlp_out 16384 none layer_38/width_16k/average_l0_45Load this SAE jumprelu gemma-2-9b/38-gemmascope-mlp-16k__l0-45 blocks.38.hook_mlp_out 16384 none layer_38/width_16k/average_l0_98Load this SAE jumprelu blocks.38.hook_mlp_out 16384 none layer_38/width_16k/average_l0_225Load this SAE jumprelu blocks.38.hook_mlp_out 16384 none layer_38/width_16k/average_l0_491Load this SAE jumprelu blocks.38.hook_mlp_out 16384 none layer_38/width_131k/average_l0_11Load this SAE jumprelu blocks.38.hook_mlp_out 131072 none layer_38/width_131k/average_l0_21Load this SAE jumprelu blocks.38.hook_mlp_out 131072 none layer_38/width_131k/average_l0_40Load this SAE jumprelu blocks.38.hook_mlp_out 131072 none layer_38/width_131k/average_l0_79Load this SAE jumprelu blocks.38.hook_mlp_out 131072 none layer_38/width_131k/average_l0_161Load this SAE jumprelu blocks.38.hook_mlp_out 131072 none layer_38/width_131k/average_l0_347Load this SAE jumprelu blocks.38.hook_mlp_out 131072 none layer_39/width_16k/average_l0_12Load this SAE jumprelu blocks.39.hook_mlp_out 16384 none layer_39/width_16k/average_l0_22Load this SAE jumprelu blocks.39.hook_mlp_out 16384 none layer_39/width_16k/average_l0_43Load this SAE jumprelu gemma-2-9b/39-gemmascope-mlp-16k__l0-43 blocks.39.hook_mlp_out 16384 none layer_39/width_16k/average_l0_90Load this SAE jumprelu blocks.39.hook_mlp_out 16384 none layer_39/width_16k/average_l0_207Load this SAE jumprelu blocks.39.hook_mlp_out 16384 none layer_39/width_16k/average_l0_458Load this SAE jumprelu blocks.39.hook_mlp_out 16384 none layer_39/width_131k/average_l0_11Load this SAE jumprelu blocks.39.hook_mlp_out 131072 none layer_39/width_131k/average_l0_21Load this SAE jumprelu blocks.39.hook_mlp_out 131072 none layer_39/width_131k/average_l0_38Load this SAE jumprelu blocks.39.hook_mlp_out 131072 none layer_39/width_131k/average_l0_75Load this SAE jumprelu blocks.39.hook_mlp_out 131072 none layer_39/width_131k/average_l0_150Load this SAE jumprelu blocks.39.hook_mlp_out 131072 none layer_39/width_131k/average_l0_319Load this SAE jumprelu blocks.39.hook_mlp_out 131072 none layer_4/width_16k/average_l0_15Load this SAE jumprelu blocks.4.hook_mlp_out 16384 none layer_4/width_16k/average_l0_30Load this SAE jumprelu blocks.4.hook_mlp_out 16384 none layer_4/width_16k/average_l0_66Load this SAE jumprelu gemma-2-9b/4-gemmascope-mlp-16k__l0-66 blocks.4.hook_mlp_out 16384 none layer_4/width_16k/average_l0_151Load this SAE jumprelu blocks.4.hook_mlp_out 16384 none layer_4/width_16k/average_l0_343Load this SAE jumprelu blocks.4.hook_mlp_out 16384 none layer_4/width_131k/average_l0_8Load this SAE jumprelu blocks.4.hook_mlp_out 131072 none layer_4/width_131k/average_l0_14Load this SAE jumprelu blocks.4.hook_mlp_out 131072 none layer_4/width_131k/average_l0_24Load this SAE jumprelu blocks.4.hook_mlp_out 131072 none layer_4/width_131k/average_l0_45Load this SAE jumprelu blocks.4.hook_mlp_out 131072 none layer_4/width_131k/average_l0_84Load this SAE jumprelu blocks.4.hook_mlp_out 131072 none layer_4/width_131k/average_l0_157Load this SAE jumprelu blocks.4.hook_mlp_out 131072 none layer_40/width_16k/average_l0_11Load this SAE jumprelu blocks.40.hook_mlp_out 16384 none layer_40/width_16k/average_l0_19Load this SAE jumprelu blocks.40.hook_mlp_out 16384 none layer_40/width_16k/average_l0_37Load this SAE jumprelu gemma-2-9b/40-gemmascope-mlp-16k__l0-37 blocks.40.hook_mlp_out 16384 none layer_40/width_16k/average_l0_74Load this SAE jumprelu blocks.40.hook_mlp_out 16384 none layer_40/width_16k/average_l0_162Load this SAE jumprelu blocks.40.hook_mlp_out 16384 none layer_40/width_16k/average_l0_371Load this SAE jumprelu blocks.40.hook_mlp_out 16384 none layer_40/width_131k/average_l0_11Load this SAE jumprelu blocks.40.hook_mlp_out 131072 none layer_40/width_131k/average_l0_18Load this SAE jumprelu blocks.40.hook_mlp_out 131072 none layer_40/width_131k/average_l0_33Load this SAE jumprelu blocks.40.hook_mlp_out 131072 none layer_40/width_131k/average_l0_63Load this SAE jumprelu blocks.40.hook_mlp_out 131072 none layer_40/width_131k/average_l0_125Load this SAE jumprelu blocks.40.hook_mlp_out 131072 none layer_40/width_131k/average_l0_267Load this SAE jumprelu blocks.40.hook_mlp_out 131072 none layer_41/width_16k/average_l0_8Load this SAE jumprelu blocks.41.hook_mlp_out 16384 none layer_41/width_16k/average_l0_15Load this SAE jumprelu blocks.41.hook_mlp_out 16384 none layer_41/width_16k/average_l0_28Load this SAE jumprelu blocks.41.hook_mlp_out 16384 none layer_41/width_16k/average_l0_58Load this SAE jumprelu gemma-2-9b/41-gemmascope-mlp-16k__l0-58 blocks.41.hook_mlp_out 16384 none layer_41/width_16k/average_l0_126Load this SAE jumprelu blocks.41.hook_mlp_out 16384 none layer_41/width_16k/average_l0_288Load this SAE jumprelu blocks.41.hook_mlp_out 16384 none layer_41/width_131k/average_l0_8Load this SAE jumprelu blocks.41.hook_mlp_out 131072 none layer_41/width_131k/average_l0_14Load this SAE jumprelu blocks.41.hook_mlp_out 131072 none layer_41/width_131k/average_l0_25Load this SAE jumprelu blocks.41.hook_mlp_out 131072 none layer_41/width_131k/average_l0_49Load this SAE jumprelu blocks.41.hook_mlp_out 131072 none layer_41/width_131k/average_l0_99Load this SAE jumprelu blocks.41.hook_mlp_out 131072 none layer_41/width_131k/average_l0_208Load this SAE jumprelu blocks.41.hook_mlp_out 131072 none layer_5/width_16k/average_l0_14Load this SAE jumprelu blocks.5.hook_mlp_out 16384 none layer_5/width_16k/average_l0_24Load this SAE jumprelu blocks.5.hook_mlp_out 16384 none layer_5/width_16k/average_l0_46Load this SAE jumprelu gemma-2-9b/5-gemmascope-mlp-16k__l0-46 blocks.5.hook_mlp_out 16384 none layer_5/width_16k/average_l0_93Load this SAE jumprelu blocks.5.hook_mlp_out 16384 none layer_5/width_16k/average_l0_194Load this SAE jumprelu blocks.5.hook_mlp_out 16384 none layer_5/width_131k/average_l0_7Load this SAE jumprelu blocks.5.hook_mlp_out 131072 none layer_5/width_131k/average_l0_12Load this SAE jumprelu blocks.5.hook_mlp_out 131072 none layer_5/width_131k/average_l0_21Load this SAE jumprelu blocks.5.hook_mlp_out 131072 none layer_5/width_131k/average_l0_37Load this SAE jumprelu blocks.5.hook_mlp_out 131072 none layer_5/width_131k/average_l0_68Load this SAE jumprelu blocks.5.hook_mlp_out 131072 none layer_5/width_131k/average_l0_131Load this SAE jumprelu blocks.5.hook_mlp_out 131072 none layer_6/width_16k/average_l0_12Load this SAE jumprelu blocks.6.hook_mlp_out 16384 none layer_6/width_16k/average_l0_23Load this SAE jumprelu blocks.6.hook_mlp_out 16384 none layer_6/width_16k/average_l0_46Load this SAE jumprelu gemma-2-9b/6-gemmascope-mlp-16k__l0-46 blocks.6.hook_mlp_out 16384 none layer_6/width_16k/average_l0_96Load this SAE jumprelu blocks.6.hook_mlp_out 16384 none layer_6/width_16k/average_l0_206Load this SAE jumprelu blocks.6.hook_mlp_out 16384 none layer_6/width_131k/average_l0_12Load this SAE jumprelu blocks.6.hook_mlp_out 131072 none layer_6/width_131k/average_l0_22Load this SAE jumprelu blocks.6.hook_mlp_out 131072 none layer_6/width_131k/average_l0_40Load this SAE jumprelu blocks.6.hook_mlp_out 131072 none layer_6/width_131k/average_l0_72Load this SAE jumprelu blocks.6.hook_mlp_out 131072 none layer_6/width_131k/average_l0_135Load this SAE jumprelu blocks.6.hook_mlp_out 131072 none layer_6/width_131k/average_l0_271Load this SAE jumprelu blocks.6.hook_mlp_out 131072 none layer_7/width_16k/average_l0_14Load this SAE jumprelu blocks.7.hook_mlp_out 16384 none layer_7/width_16k/average_l0_25Load this SAE jumprelu blocks.7.hook_mlp_out 16384 none layer_7/width_16k/average_l0_47Load this SAE jumprelu gemma-2-9b/7-gemmascope-mlp-16k__l0-47 blocks.7.hook_mlp_out 16384 none layer_7/width_16k/average_l0_101Load this SAE jumprelu blocks.7.hook_mlp_out 16384 none layer_7/width_16k/average_l0_231Load this SAE jumprelu blocks.7.hook_mlp_out 16384 none layer_7/width_131k/average_l0_13Load this SAE jumprelu blocks.7.hook_mlp_out 131072 none layer_7/width_131k/average_l0_23Load this SAE jumprelu blocks.7.hook_mlp_out 131072 none layer_7/width_131k/average_l0_41Load this SAE jumprelu blocks.7.hook_mlp_out 131072 none layer_7/width_131k/average_l0_75Load this SAE jumprelu blocks.7.hook_mlp_out 131072 none layer_7/width_131k/average_l0_143Load this SAE jumprelu blocks.7.hook_mlp_out 131072 none layer_7/width_131k/average_l0_309Load this SAE jumprelu blocks.7.hook_mlp_out 131072 none layer_8/width_16k/average_l0_15Load this SAE jumprelu blocks.8.hook_mlp_out 16384 none layer_8/width_16k/average_l0_27Load this SAE jumprelu blocks.8.hook_mlp_out 16384 none layer_8/width_16k/average_l0_55Load this SAE jumprelu gemma-2-9b/8-gemmascope-mlp-16k__l0-55 blocks.8.hook_mlp_out 16384 none layer_8/width_16k/average_l0_124Load this SAE jumprelu blocks.8.hook_mlp_out 16384 none layer_8/width_16k/average_l0_309Load this SAE jumprelu blocks.8.hook_mlp_out 16384 none layer_8/width_131k/average_l0_15Load this SAE jumprelu blocks.8.hook_mlp_out 131072 none layer_8/width_131k/average_l0_26Load this SAE jumprelu blocks.8.hook_mlp_out 131072 none layer_8/width_131k/average_l0_48Load this SAE jumprelu blocks.8.hook_mlp_out 131072 none layer_8/width_131k/average_l0_89Load this SAE jumprelu blocks.8.hook_mlp_out 131072 none layer_8/width_131k/average_l0_184Load this SAE jumprelu blocks.8.hook_mlp_out 131072 none layer_8/width_131k/average_l0_423Load this SAE jumprelu blocks.8.hook_mlp_out 131072 none layer_9/width_16k/average_l0_12Load this SAE jumprelu blocks.9.hook_mlp_out 16384 none layer_9/width_16k/average_l0_21Load this SAE jumprelu blocks.9.hook_mlp_out 16384 none layer_9/width_16k/average_l0_40Load this SAE jumprelu blocks.9.hook_mlp_out 16384 none layer_9/width_16k/average_l0_83Load this SAE jumprelu gemma-2-9b/9-gemmascope-mlp-16k__l0-83 blocks.9.hook_mlp_out 16384 none layer_9/width_16k/average_l0_200Load this SAE jumprelu blocks.9.hook_mlp_out 16384 none layer_9/width_131k/average_l0_12Load this SAE jumprelu blocks.9.hook_mlp_out 131072 none layer_9/width_131k/average_l0_21Load this SAE jumprelu blocks.9.hook_mlp_out 131072 none layer_9/width_131k/average_l0_38Load this SAE jumprelu blocks.9.hook_mlp_out 131072 none layer_9/width_131k/average_l0_67Load this SAE jumprelu blocks.9.hook_mlp_out 131072 none layer_9/width_131k/average_l0_129Load this SAE jumprelu blocks.9.hook_mlp_out 131072 none layer_9/width_131k/average_l0_269Load this SAE jumprelu blocks.9.hook_mlp_out 131072 none"},{"location":"sae_table/#gemma-scope-9b-pt-mlp-canonical","title":"gemma-scope-9b-pt-mlp-canonical","text":"<ul> <li>Huggingface Repo: google/gemma-scope-9b-pt-mlp</li> <li>model: gemma-2-9b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/0-gemmascope-mlp-16k blocks.0.hook_mlp_out 16384 none layer_1/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/1-gemmascope-mlp-16k blocks.1.hook_mlp_out 16384 none layer_2/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/2-gemmascope-mlp-16k blocks.2.hook_mlp_out 16384 none layer_3/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/3-gemmascope-mlp-16k blocks.3.hook_mlp_out 16384 none layer_4/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/4-gemmascope-mlp-16k blocks.4.hook_mlp_out 16384 none layer_5/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/5-gemmascope-mlp-16k blocks.5.hook_mlp_out 16384 none layer_6/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/6-gemmascope-mlp-16k blocks.6.hook_mlp_out 16384 none layer_7/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/7-gemmascope-mlp-16k blocks.7.hook_mlp_out 16384 none layer_8/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/8-gemmascope-mlp-16k blocks.8.hook_mlp_out 16384 none layer_9/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/9-gemmascope-mlp-16k blocks.9.hook_mlp_out 16384 none layer_10/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/10-gemmascope-mlp-16k blocks.10.hook_mlp_out 16384 none layer_11/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/11-gemmascope-mlp-16k blocks.11.hook_mlp_out 16384 none layer_12/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/12-gemmascope-mlp-16k blocks.12.hook_mlp_out 16384 none layer_13/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/13-gemmascope-mlp-16k blocks.13.hook_mlp_out 16384 none layer_14/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/14-gemmascope-mlp-16k blocks.14.hook_mlp_out 16384 none layer_15/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/15-gemmascope-mlp-16k blocks.15.hook_mlp_out 16384 none layer_16/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/16-gemmascope-mlp-16k blocks.16.hook_mlp_out 16384 none layer_17/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/17-gemmascope-mlp-16k blocks.17.hook_mlp_out 16384 none layer_18/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/18-gemmascope-mlp-16k blocks.18.hook_mlp_out 16384 none layer_19/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/19-gemmascope-mlp-16k blocks.19.hook_mlp_out 16384 none layer_20/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/20-gemmascope-mlp-16k blocks.20.hook_mlp_out 16384 none layer_21/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/21-gemmascope-mlp-16k blocks.21.hook_mlp_out 16384 none layer_22/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/22-gemmascope-mlp-16k blocks.22.hook_mlp_out 16384 none layer_23/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/23-gemmascope-mlp-16k blocks.23.hook_mlp_out 16384 none layer_24/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/24-gemmascope-mlp-16k blocks.24.hook_mlp_out 16384 none layer_25/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/25-gemmascope-mlp-16k blocks.25.hook_mlp_out 16384 none layer_26/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/26-gemmascope-mlp-16k blocks.26.hook_mlp_out 16384 none layer_27/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/27-gemmascope-mlp-16k blocks.27.hook_mlp_out 16384 none layer_28/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/28-gemmascope-mlp-16k blocks.28.hook_mlp_out 16384 none layer_29/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/29-gemmascope-mlp-16k blocks.29.hook_mlp_out 16384 none layer_30/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/30-gemmascope-mlp-16k blocks.30.hook_mlp_out 16384 none layer_31/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/31-gemmascope-mlp-16k blocks.31.hook_mlp_out 16384 none layer_32/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/32-gemmascope-mlp-16k blocks.32.hook_mlp_out 16384 none layer_33/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/33-gemmascope-mlp-16k blocks.33.hook_mlp_out 16384 none layer_34/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/34-gemmascope-mlp-16k blocks.34.hook_mlp_out 16384 none layer_35/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/35-gemmascope-mlp-16k blocks.35.hook_mlp_out 16384 none layer_36/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/36-gemmascope-mlp-16k blocks.36.hook_mlp_out 16384 none layer_37/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/37-gemmascope-mlp-16k blocks.37.hook_mlp_out 16384 none layer_38/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/38-gemmascope-mlp-16k blocks.38.hook_mlp_out 16384 none layer_39/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/39-gemmascope-mlp-16k blocks.39.hook_mlp_out 16384 none layer_40/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/40-gemmascope-mlp-16k blocks.40.hook_mlp_out 16384 none layer_41/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/41-gemmascope-mlp-16k blocks.41.hook_mlp_out 16384 none layer_0/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/0-gemmascope-mlp-131k blocks.0.hook_mlp_out 131072 none layer_1/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/1-gemmascope-mlp-131k blocks.1.hook_mlp_out 131072 none layer_2/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/2-gemmascope-mlp-131k blocks.2.hook_mlp_out 131072 none layer_3/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/3-gemmascope-mlp-131k blocks.3.hook_mlp_out 131072 none layer_4/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/4-gemmascope-mlp-131k blocks.4.hook_mlp_out 131072 none layer_5/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/5-gemmascope-mlp-131k blocks.5.hook_mlp_out 131072 none layer_6/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/6-gemmascope-mlp-131k blocks.6.hook_mlp_out 131072 none layer_7/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/7-gemmascope-mlp-131k blocks.7.hook_mlp_out 131072 none layer_8/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/8-gemmascope-mlp-131k blocks.8.hook_mlp_out 131072 none layer_9/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/9-gemmascope-mlp-131k blocks.9.hook_mlp_out 131072 none layer_10/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/10-gemmascope-mlp-131k blocks.10.hook_mlp_out 131072 none layer_11/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/11-gemmascope-mlp-131k blocks.11.hook_mlp_out 131072 none layer_12/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/12-gemmascope-mlp-131k blocks.12.hook_mlp_out 131072 none layer_13/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/13-gemmascope-mlp-131k blocks.13.hook_mlp_out 131072 none layer_14/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/14-gemmascope-mlp-131k blocks.14.hook_mlp_out 131072 none layer_15/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/15-gemmascope-mlp-131k blocks.15.hook_mlp_out 131072 none layer_16/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/16-gemmascope-mlp-131k blocks.16.hook_mlp_out 131072 none layer_17/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/17-gemmascope-mlp-131k blocks.17.hook_mlp_out 131072 none layer_18/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/18-gemmascope-mlp-131k blocks.18.hook_mlp_out 131072 none layer_19/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/19-gemmascope-mlp-131k blocks.19.hook_mlp_out 131072 none layer_20/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/20-gemmascope-mlp-131k blocks.20.hook_mlp_out 131072 none layer_21/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/21-gemmascope-mlp-131k blocks.21.hook_mlp_out 131072 none layer_22/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/22-gemmascope-mlp-131k blocks.22.hook_mlp_out 131072 none layer_23/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/23-gemmascope-mlp-131k blocks.23.hook_mlp_out 131072 none layer_24/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/24-gemmascope-mlp-131k blocks.24.hook_mlp_out 131072 none layer_25/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/25-gemmascope-mlp-131k blocks.25.hook_mlp_out 131072 none layer_26/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/26-gemmascope-mlp-131k blocks.26.hook_mlp_out 131072 none layer_27/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/27-gemmascope-mlp-131k blocks.27.hook_mlp_out 131072 none layer_28/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/28-gemmascope-mlp-131k blocks.28.hook_mlp_out 131072 none layer_29/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/29-gemmascope-mlp-131k blocks.29.hook_mlp_out 131072 none layer_30/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/30-gemmascope-mlp-131k blocks.30.hook_mlp_out 131072 none layer_31/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/31-gemmascope-mlp-131k blocks.31.hook_mlp_out 131072 none layer_32/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/32-gemmascope-mlp-131k blocks.32.hook_mlp_out 131072 none layer_33/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/33-gemmascope-mlp-131k blocks.33.hook_mlp_out 131072 none layer_34/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/34-gemmascope-mlp-131k blocks.34.hook_mlp_out 131072 none layer_35/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/35-gemmascope-mlp-131k blocks.35.hook_mlp_out 131072 none layer_36/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/36-gemmascope-mlp-131k blocks.36.hook_mlp_out 131072 none layer_37/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/37-gemmascope-mlp-131k blocks.37.hook_mlp_out 131072 none layer_38/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/38-gemmascope-mlp-131k blocks.38.hook_mlp_out 131072 none layer_39/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/39-gemmascope-mlp-131k blocks.39.hook_mlp_out 131072 none layer_40/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/40-gemmascope-mlp-131k blocks.40.hook_mlp_out 131072 none layer_41/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/41-gemmascope-mlp-131k blocks.41.hook_mlp_out 131072 none"},{"location":"sae_table/#gemma-scope-9b-pt-res","title":"gemma-scope-9b-pt-res","text":"<ul> <li>Huggingface Repo: google/gemma-scope-9b-pt-res</li> <li>model: gemma-2-9b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations embedding/width_4k/average_l0_14Load this SAE jumprelu hook_embed 4096 none embedding/width_4k/average_l0_22Load this SAE jumprelu hook_embed 4096 none embedding/width_4k/average_l0_7Load this SAE jumprelu hook_embed 4096 none embedding/width_4k/average_l0_80Load this SAE jumprelu hook_embed 4096 none layer_0/width_131k/average_l0_11Load this SAE jumprelu blocks.0.hook_resid_post 131072 none layer_0/width_131k/average_l0_15Load this SAE jumprelu blocks.0.hook_resid_post 131072 none layer_0/width_131k/average_l0_21Load this SAE jumprelu blocks.0.hook_resid_post 131072 none layer_0/width_131k/average_l0_30Load this SAE jumprelu gemma-2-9b/0-gemmascope-res-131k__l0-30 blocks.0.hook_resid_post 131072 none layer_0/width_131k/average_l0_41Load this SAE jumprelu blocks.0.hook_resid_post 131072 none layer_0/width_131k/average_l0_8Load this SAE jumprelu blocks.0.hook_resid_post 131072 none layer_0/width_16k/average_l0_11Load this SAE jumprelu blocks.0.hook_resid_post 16384 none layer_0/width_16k/average_l0_129Load this SAE jumprelu blocks.0.hook_resid_post 16384 none layer_0/width_16k/average_l0_17Load this SAE jumprelu blocks.0.hook_resid_post 16384 none layer_0/width_16k/average_l0_35Load this SAE jumprelu gemma-2-9b/0-gemmascope-res-16k__l0-35 blocks.0.hook_resid_post 16384 none layer_0/width_16k/average_l0_68Load this SAE jumprelu blocks.0.hook_resid_post 16384 none layer_1/width_131k/average_l0_13Load this SAE jumprelu blocks.1.hook_resid_post 131072 none layer_1/width_131k/average_l0_20Load this SAE jumprelu blocks.1.hook_resid_post 131072 none layer_1/width_131k/average_l0_33Load this SAE jumprelu gemma-2-9b/1-gemmascope-res-131k__l0-33 blocks.1.hook_resid_post 131072 none layer_1/width_131k/average_l0_56Load this SAE jumprelu blocks.1.hook_resid_post 131072 none layer_1/width_131k/average_l0_6Load this SAE jumprelu blocks.1.hook_resid_post 131072 none layer_1/width_131k/average_l0_9Load this SAE jumprelu blocks.1.hook_resid_post 131072 none layer_1/width_16k/average_l0_15Load this SAE jumprelu blocks.1.hook_resid_post 16384 none layer_1/width_16k/average_l0_175Load this SAE jumprelu blocks.1.hook_resid_post 16384 none layer_1/width_16k/average_l0_31Load this SAE jumprelu blocks.1.hook_resid_post 16384 none layer_1/width_16k/average_l0_69Load this SAE jumprelu gemma-2-9b/1-gemmascope-res-16k__l0-69 blocks.1.hook_resid_post 16384 none layer_1/width_16k/average_l0_9Load this SAE jumprelu blocks.1.hook_resid_post 16384 none layer_10/width_131k/average_l0_15Load this SAE jumprelu blocks.10.hook_resid_post 131072 none layer_10/width_131k/average_l0_151Load this SAE jumprelu blocks.10.hook_resid_post 131072 none layer_10/width_131k/average_l0_27Load this SAE jumprelu blocks.10.hook_resid_post 131072 none layer_10/width_131k/average_l0_47Load this SAE jumprelu gemma-2-9b/10-gemmascope-res-131k__l0-47 blocks.10.hook_resid_post 131072 none layer_10/width_131k/average_l0_84Load this SAE jumprelu blocks.10.hook_resid_post 131072 none layer_10/width_131k/average_l0_9Load this SAE jumprelu blocks.10.hook_resid_post 131072 none layer_10/width_16k/average_l0_10Load this SAE jumprelu blocks.10.hook_resid_post 16384 none layer_10/width_16k/average_l0_113Load this SAE jumprelu blocks.10.hook_resid_post 16384 none layer_10/width_16k/average_l0_17Load this SAE jumprelu blocks.10.hook_resid_post 16384 none layer_10/width_16k/average_l0_243Load this SAE jumprelu blocks.10.hook_resid_post 16384 none layer_10/width_16k/average_l0_31Load this SAE jumprelu blocks.10.hook_resid_post 16384 none layer_10/width_16k/average_l0_57Load this SAE jumprelu gemma-2-9b/10-gemmascope-res-16k__l0-57 blocks.10.hook_resid_post 16384 none layer_11/width_131k/average_l0_16Load this SAE jumprelu blocks.11.hook_resid_post 131072 none layer_11/width_131k/average_l0_162Load this SAE jumprelu blocks.11.hook_resid_post 131072 none layer_11/width_131k/average_l0_27Load this SAE jumprelu blocks.11.hook_resid_post 131072 none layer_11/width_131k/average_l0_49Load this SAE jumprelu gemma-2-9b/11-gemmascope-res-131k__l0-49 blocks.11.hook_resid_post 131072 none layer_11/width_131k/average_l0_88Load this SAE jumprelu blocks.11.hook_resid_post 131072 none layer_11/width_131k/average_l0_9Load this SAE jumprelu blocks.11.hook_resid_post 131072 none layer_11/width_16k/average_l0_10Load this SAE jumprelu blocks.11.hook_resid_post 16384 none layer_11/width_16k/average_l0_118Load this SAE jumprelu blocks.11.hook_resid_post 16384 none layer_11/width_16k/average_l0_18Load this SAE jumprelu blocks.11.hook_resid_post 16384 none layer_11/width_16k/average_l0_255Load this SAE jumprelu blocks.11.hook_resid_post 16384 none layer_11/width_16k/average_l0_32Load this SAE jumprelu gemma-2-9b/11-gemmascope-res-16k__l0-32 blocks.11.hook_resid_post 16384 none layer_11/width_16k/average_l0_60Load this SAE jumprelu blocks.11.hook_resid_post 16384 none layer_12/width_131k/average_l0_10Load this SAE jumprelu blocks.12.hook_resid_post 131072 none layer_12/width_131k/average_l0_17Load this SAE jumprelu blocks.12.hook_resid_post 131072 none layer_12/width_131k/average_l0_183Load this SAE jumprelu blocks.12.hook_resid_post 131072 none layer_12/width_131k/average_l0_29Load this SAE jumprelu blocks.12.hook_resid_post 131072 none layer_12/width_131k/average_l0_52Load this SAE jumprelu gemma-2-9b/12-gemmascope-res-131k__l0-52 blocks.12.hook_resid_post 131072 none layer_12/width_131k/average_l0_96Load this SAE jumprelu blocks.12.hook_resid_post 131072 none layer_12/width_16k/average_l0_10Load this SAE jumprelu blocks.12.hook_resid_post 16384 none layer_12/width_16k/average_l0_130Load this SAE jumprelu blocks.12.hook_resid_post 16384 none layer_12/width_16k/average_l0_19Load this SAE jumprelu blocks.12.hook_resid_post 16384 none layer_12/width_16k/average_l0_287Load this SAE jumprelu blocks.12.hook_resid_post 16384 none layer_12/width_16k/average_l0_33Load this SAE jumprelu gemma-2-9b/12-gemmascope-res-16k__l0-33 blocks.12.hook_resid_post 16384 none layer_12/width_16k/average_l0_64Load this SAE jumprelu blocks.12.hook_resid_post 16384 none layer_13/width_131k/average_l0_10Load this SAE jumprelu blocks.13.hook_resid_post 131072 none layer_13/width_131k/average_l0_17Load this SAE jumprelu blocks.13.hook_resid_post 131072 none layer_13/width_131k/average_l0_189Load this SAE jumprelu blocks.13.hook_resid_post 131072 none layer_13/width_131k/average_l0_30Load this SAE jumprelu gemma-2-9b/13-gemmascope-res-131k__l0-30 blocks.13.hook_resid_post 131072 none layer_13/width_131k/average_l0_54Load this SAE jumprelu blocks.13.hook_resid_post 131072 none layer_13/width_131k/average_l0_99Load this SAE jumprelu blocks.13.hook_resid_post 131072 none layer_13/width_16k/average_l0_11Load this SAE jumprelu blocks.13.hook_resid_post 16384 none layer_13/width_16k/average_l0_132Load this SAE jumprelu blocks.13.hook_resid_post 16384 none layer_13/width_16k/average_l0_19Load this SAE jumprelu blocks.13.hook_resid_post 16384 none layer_13/width_16k/average_l0_285Load this SAE jumprelu blocks.13.hook_resid_post 16384 none layer_13/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/13-gemmascope-res-16k__l0-34 blocks.13.hook_resid_post 16384 none layer_13/width_16k/average_l0_65Load this SAE jumprelu blocks.13.hook_resid_post 16384 none layer_14/width_131k/average_l0_10Load this SAE jumprelu blocks.14.hook_resid_post 131072 none layer_14/width_131k/average_l0_105Load this SAE jumprelu blocks.14.hook_resid_post 131072 none layer_14/width_131k/average_l0_18Load this SAE jumprelu blocks.14.hook_resid_post 131072 none layer_14/width_131k/average_l0_197Load this SAE jumprelu blocks.14.hook_resid_post 131072 none layer_14/width_131k/average_l0_31Load this SAE jumprelu blocks.14.hook_resid_post 131072 none layer_14/width_131k/average_l0_56Load this SAE jumprelu gemma-2-9b/14-gemmascope-res-131k__l0-56 blocks.14.hook_resid_post 131072 none layer_14/width_16k/average_l0_11Load this SAE jumprelu blocks.14.hook_resid_post 16384 none layer_14/width_16k/average_l0_137Load this SAE jumprelu blocks.14.hook_resid_post 16384 none layer_14/width_16k/average_l0_19Load this SAE jumprelu blocks.14.hook_resid_post 16384 none layer_14/width_16k/average_l0_294Load this SAE jumprelu blocks.14.hook_resid_post 16384 none layer_14/width_16k/average_l0_35Load this SAE jumprelu gemma-2-9b/14-gemmascope-res-16k__l0-35 blocks.14.hook_resid_post 16384 none layer_14/width_16k/average_l0_67Load this SAE jumprelu blocks.14.hook_resid_post 16384 none layer_15/width_131k/average_l0_10Load this SAE jumprelu blocks.15.hook_resid_post 131072 none layer_15/width_131k/average_l0_103Load this SAE jumprelu blocks.15.hook_resid_post 131072 none layer_15/width_131k/average_l0_17Load this SAE jumprelu blocks.15.hook_resid_post 131072 none layer_15/width_131k/average_l0_198Load this SAE jumprelu blocks.15.hook_resid_post 131072 none layer_15/width_131k/average_l0_30Load this SAE jumprelu blocks.15.hook_resid_post 131072 none layer_15/width_131k/average_l0_55Load this SAE jumprelu gemma-2-9b/15-gemmascope-res-131k__l0-55 blocks.15.hook_resid_post 131072 none layer_15/width_16k/average_l0_10Load this SAE jumprelu blocks.15.hook_resid_post 16384 none layer_15/width_16k/average_l0_131Load this SAE jumprelu blocks.15.hook_resid_post 16384 none layer_15/width_16k/average_l0_18Load this SAE jumprelu blocks.15.hook_resid_post 16384 none layer_15/width_16k/average_l0_290Load this SAE jumprelu blocks.15.hook_resid_post 16384 none layer_15/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/15-gemmascope-res-16k__l0-34 blocks.15.hook_resid_post 16384 none layer_15/width_16k/average_l0_65Load this SAE jumprelu blocks.15.hook_resid_post 16384 none layer_16/width_131k/average_l0_11Load this SAE jumprelu blocks.16.hook_resid_post 131072 none layer_16/width_131k/average_l0_121Load this SAE jumprelu blocks.16.hook_resid_post 131072 none layer_16/width_131k/average_l0_20Load this SAE jumprelu blocks.16.hook_resid_post 131072 none layer_16/width_131k/average_l0_232Load this SAE jumprelu blocks.16.hook_resid_post 131072 none layer_16/width_131k/average_l0_35Load this SAE jumprelu gemma-2-9b/16-gemmascope-res-131k__l0-35 blocks.16.hook_resid_post 131072 none layer_16/width_131k/average_l0_65Load this SAE jumprelu blocks.16.hook_resid_post 131072 none layer_16/width_16k/average_l0_11Load this SAE jumprelu blocks.16.hook_resid_post 16384 none layer_16/width_16k/average_l0_152Load this SAE jumprelu blocks.16.hook_resid_post 16384 none layer_16/width_16k/average_l0_21Load this SAE jumprelu blocks.16.hook_resid_post 16384 none layer_16/width_16k/average_l0_346Load this SAE jumprelu blocks.16.hook_resid_post 16384 none layer_16/width_16k/average_l0_39Load this SAE jumprelu gemma-2-9b/16-gemmascope-res-16k__l0-39 blocks.16.hook_resid_post 16384 none layer_16/width_16k/average_l0_75Load this SAE jumprelu blocks.16.hook_resid_post 16384 none layer_17/width_131k/average_l0_11Load this SAE jumprelu blocks.17.hook_resid_post 131072 none layer_17/width_131k/average_l0_117Load this SAE jumprelu blocks.17.hook_resid_post 131072 none layer_17/width_131k/average_l0_19Load this SAE jumprelu blocks.17.hook_resid_post 131072 none layer_17/width_131k/average_l0_221Load this SAE jumprelu blocks.17.hook_resid_post 131072 none layer_17/width_131k/average_l0_35Load this SAE jumprelu gemma-2-9b/17-gemmascope-res-131k__l0-35 blocks.17.hook_resid_post 131072 none layer_17/width_131k/average_l0_64Load this SAE jumprelu blocks.17.hook_resid_post 131072 none layer_17/width_16k/average_l0_12Load this SAE jumprelu blocks.17.hook_resid_post 16384 none layer_17/width_16k/average_l0_144Load this SAE jumprelu blocks.17.hook_resid_post 16384 none layer_17/width_16k/average_l0_21Load this SAE jumprelu blocks.17.hook_resid_post 16384 none layer_17/width_16k/average_l0_317Load this SAE jumprelu blocks.17.hook_resid_post 16384 none layer_17/width_16k/average_l0_38Load this SAE jumprelu gemma-2-9b/17-gemmascope-res-16k__l0-38 blocks.17.hook_resid_post 16384 none layer_17/width_16k/average_l0_73Load this SAE jumprelu blocks.17.hook_resid_post 16384 none layer_18/width_131k/average_l0_11Load this SAE jumprelu blocks.18.hook_resid_post 131072 none layer_18/width_131k/average_l0_113Load this SAE jumprelu blocks.18.hook_resid_post 131072 none layer_18/width_131k/average_l0_19Load this SAE jumprelu blocks.18.hook_resid_post 131072 none layer_18/width_131k/average_l0_221Load this SAE jumprelu blocks.18.hook_resid_post 131072 none layer_18/width_131k/average_l0_34Load this SAE jumprelu gemma-2-9b/18-gemmascope-res-131k__l0-34 blocks.18.hook_resid_post 131072 none layer_18/width_131k/average_l0_62Load this SAE jumprelu blocks.18.hook_resid_post 131072 none layer_18/width_16k/average_l0_11Load this SAE jumprelu blocks.18.hook_resid_post 16384 none layer_18/width_16k/average_l0_140Load this SAE jumprelu blocks.18.hook_resid_post 16384 none layer_18/width_16k/average_l0_20Load this SAE jumprelu blocks.18.hook_resid_post 16384 none layer_18/width_16k/average_l0_309Load this SAE jumprelu blocks.18.hook_resid_post 16384 none layer_18/width_16k/average_l0_37Load this SAE jumprelu gemma-2-9b/18-gemmascope-res-16k__l0-37 blocks.18.hook_resid_post 16384 none layer_18/width_16k/average_l0_71Load this SAE jumprelu blocks.18.hook_resid_post 16384 none layer_19/width_131k/average_l0_10Load this SAE jumprelu blocks.19.hook_resid_post 131072 none layer_19/width_131k/average_l0_110Load this SAE jumprelu blocks.19.hook_resid_post 131072 none layer_19/width_131k/average_l0_18Load this SAE jumprelu blocks.19.hook_resid_post 131072 none layer_19/width_131k/average_l0_210Load this SAE jumprelu blocks.19.hook_resid_post 131072 none layer_19/width_131k/average_l0_32Load this SAE jumprelu gemma-2-9b/19-gemmascope-res-131k__l0-32 blocks.19.hook_resid_post 131072 none layer_19/width_131k/average_l0_60Load this SAE jumprelu blocks.19.hook_resid_post 131072 none layer_19/width_16k/average_l0_11Load this SAE jumprelu blocks.19.hook_resid_post 16384 none layer_19/width_16k/average_l0_132Load this SAE jumprelu blocks.19.hook_resid_post 16384 none layer_19/width_16k/average_l0_19Load this SAE jumprelu blocks.19.hook_resid_post 16384 none layer_19/width_16k/average_l0_293Load this SAE jumprelu blocks.19.hook_resid_post 16384 none layer_19/width_16k/average_l0_35Load this SAE jumprelu gemma-2-9b/19-gemmascope-res-16k__l0-35 blocks.19.hook_resid_post 16384 none layer_19/width_16k/average_l0_67Load this SAE jumprelu blocks.19.hook_resid_post 16384 none layer_2/width_131k/average_l0_12Load this SAE jumprelu blocks.2.hook_resid_post 131072 none layer_2/width_131k/average_l0_19Load this SAE jumprelu blocks.2.hook_resid_post 131072 none layer_2/width_131k/average_l0_36Load this SAE jumprelu gemma-2-9b/2-gemmascope-res-131k__l0-36 blocks.2.hook_resid_post 131072 none layer_2/width_131k/average_l0_5Load this SAE jumprelu blocks.2.hook_resid_post 131072 none layer_2/width_131k/average_l0_70Load this SAE jumprelu blocks.2.hook_resid_post 131072 none layer_2/width_131k/average_l0_8Load this SAE jumprelu blocks.2.hook_resid_post 131072 none layer_2/width_16k/average_l0_14Load this SAE jumprelu blocks.2.hook_resid_post 16384 none layer_2/width_16k/average_l0_189Load this SAE jumprelu blocks.2.hook_resid_post 16384 none layer_2/width_16k/average_l0_29Load this SAE jumprelu blocks.2.hook_resid_post 16384 none layer_2/width_16k/average_l0_67Load this SAE jumprelu gemma-2-9b/2-gemmascope-res-16k__l0-67 blocks.2.hook_resid_post 16384 none layer_2/width_16k/average_l0_8Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-131k__l0-10 blocks.2.hook_resid_post 16384 none layer_20/width_131k/average_l0_11Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-131k__l0-11 blocks.20.hook_resid_post 131072 none layer_20/width_131k/average_l0_114Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-131k__l0-12 blocks.20.hook_resid_post 131072 none layer_20/width_131k/average_l0_19Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-131k__l0-19 blocks.20.hook_resid_post 131072 none layer_20/width_131k/average_l0_221Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-131k__l0-221 blocks.20.hook_resid_post 131072 none layer_20/width_131k/average_l0_276Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-131k__l0-276 blocks.20.hook_resid_post 131072 none layer_20/width_131k/average_l0_34Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-131k__l0-34 blocks.20.hook_resid_post 131072 none layer_20/width_131k/average_l0_53Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-131k__l0-53 blocks.20.hook_resid_post 131072 none layer_20/width_131k/average_l0_62Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-131k__l0-62 blocks.20.hook_resid_post 131072 none layer_20/width_16k/average_l0_11Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-16k__l0-11 blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_138Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-16k__l0-138 blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_20Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-16k__l0-20 blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_310Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-16k__l0-310 blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_36Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-16k__l0-36 blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_408Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-16k__l0-408 blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_58Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-16k__l0-58 blocks.20.hook_resid_post 16384 none layer_20/width_16k/average_l0_68Load this SAE jumprelu blocks.20.hook_resid_post 16384 none layer_20/width_1m/average_l0_101Load this SAE jumprelu blocks.20.hook_resid_post 1048576 none layer_20/width_1m/average_l0_11Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-1m__l0-11 blocks.20.hook_resid_post 1048576 none layer_20/width_1m/average_l0_19Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-1m__l0-19 blocks.20.hook_resid_post 1048576 none layer_20/width_1m/average_l0_193Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-1m__l0-193 blocks.20.hook_resid_post 1048576 none layer_20/width_1m/average_l0_34Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-1m__l0-34 blocks.20.hook_resid_post 1048576 none layer_20/width_1m/average_l0_57Load this SAE jumprelu gemma-2-9b/20-gemmascope-res-1m__l0-57 blocks.20.hook_resid_post 1048576 none layer_20/width_262k/average_l0_11Load this SAE jumprelu blocks.20.hook_resid_post 262144 none layer_20/width_262k/average_l0_259Load this SAE jumprelu blocks.20.hook_resid_post 262144 none layer_20/width_262k/average_l0_50Load this SAE jumprelu blocks.20.hook_resid_post 262144 none layer_20/width_32k/average_l0_11Load this SAE jumprelu blocks.20.hook_resid_post 32768 none layer_20/width_32k/average_l0_344Load this SAE jumprelu blocks.20.hook_resid_post 32768 none layer_20/width_32k/average_l0_57Load this SAE jumprelu blocks.20.hook_resid_post 32768 none layer_20/width_524k/average_l0_10Load this SAE jumprelu blocks.20.hook_resid_post 524288 none layer_20/width_524k/average_l0_241Load this SAE jumprelu blocks.20.hook_resid_post 524288 none layer_20/width_524k/average_l0_48Load this SAE jumprelu blocks.20.hook_resid_post 524288 none layer_20/width_65k/average_l0_11Load this SAE jumprelu blocks.20.hook_resid_post 65536 none layer_20/width_65k/average_l0_55Load this SAE jumprelu blocks.20.hook_resid_post 65536 none layer_20/width_65k/average_l0_298Load this SAE jumprelu blocks.20.hook_resid_post 65536 none layer_21/width_131k/average_l0_109Load this SAE jumprelu blocks.21.hook_resid_post 131072 none layer_21/width_131k/average_l0_11Load this SAE jumprelu blocks.21.hook_resid_post 131072 none layer_21/width_131k/average_l0_19Load this SAE jumprelu blocks.21.hook_resid_post 131072 none layer_21/width_131k/average_l0_204Load this SAE jumprelu blocks.21.hook_resid_post 131072 none layer_21/width_131k/average_l0_33Load this SAE jumprelu gemma-2-9b/21-gemmascope-res-131k__l0-33 blocks.21.hook_resid_post 131072 none layer_21/width_131k/average_l0_58Load this SAE jumprelu blocks.21.hook_resid_post 131072 none layer_21/width_16k/average_l0_11Load this SAE jumprelu blocks.21.hook_resid_post 16384 none layer_21/width_16k/average_l0_129Load this SAE jumprelu blocks.21.hook_resid_post 16384 none layer_21/width_16k/average_l0_19Load this SAE jumprelu blocks.21.hook_resid_post 16384 none layer_21/width_16k/average_l0_278Load this SAE jumprelu blocks.21.hook_resid_post 16384 none layer_21/width_16k/average_l0_36Load this SAE jumprelu gemma-2-9b/21-gemmascope-res-16k__l0-36 blocks.21.hook_resid_post 16384 none layer_21/width_16k/average_l0_66Load this SAE jumprelu blocks.21.hook_resid_post 16384 none layer_22/width_131k/average_l0_10Load this SAE jumprelu blocks.22.hook_resid_post 131072 none layer_22/width_131k/average_l0_105Load this SAE jumprelu blocks.22.hook_resid_post 131072 none layer_22/width_131k/average_l0_18Load this SAE jumprelu blocks.22.hook_resid_post 131072 none layer_22/width_131k/average_l0_197Load this SAE jumprelu blocks.22.hook_resid_post 131072 none layer_22/width_131k/average_l0_32Load this SAE jumprelu gemma-2-9b/22-gemmascope-res-131k__l0-32 blocks.22.hook_resid_post 131072 none layer_22/width_131k/average_l0_58Load this SAE jumprelu blocks.22.hook_resid_post 131072 none layer_22/width_16k/average_l0_11Load this SAE jumprelu blocks.22.hook_resid_post 16384 none layer_22/width_16k/average_l0_123Load this SAE jumprelu blocks.22.hook_resid_post 16384 none layer_22/width_16k/average_l0_19Load this SAE jumprelu blocks.22.hook_resid_post 16384 none layer_22/width_16k/average_l0_268Load this SAE jumprelu blocks.22.hook_resid_post 16384 none layer_22/width_16k/average_l0_35Load this SAE jumprelu gemma-2-9b/22-gemmascope-res-16k__l0-35 blocks.22.hook_resid_post 16384 none layer_22/width_16k/average_l0_65Load this SAE jumprelu blocks.22.hook_resid_post 16384 none layer_23/width_131k/average_l0_10Load this SAE jumprelu blocks.23.hook_resid_post 131072 none layer_23/width_131k/average_l0_101Load this SAE jumprelu blocks.23.hook_resid_post 131072 none layer_23/width_131k/average_l0_18Load this SAE jumprelu blocks.23.hook_resid_post 131072 none layer_23/width_131k/average_l0_187Load this SAE jumprelu blocks.23.hook_resid_post 131072 none layer_23/width_131k/average_l0_32Load this SAE jumprelu gemma-2-9b/23-gemmascope-res-131k__l0-32 blocks.23.hook_resid_post 131072 none layer_23/width_131k/average_l0_56Load this SAE jumprelu blocks.23.hook_resid_post 131072 none layer_23/width_16k/average_l0_11Load this SAE jumprelu blocks.23.hook_resid_post 16384 none layer_23/width_16k/average_l0_120Load this SAE jumprelu blocks.23.hook_resid_post 16384 none layer_23/width_16k/average_l0_19Load this SAE jumprelu blocks.23.hook_resid_post 16384 none layer_23/width_16k/average_l0_248Load this SAE jumprelu blocks.23.hook_resid_post 16384 none layer_23/width_16k/average_l0_35Load this SAE jumprelu gemma-2-9b/23-gemmascope-res-16k__l0-35 blocks.23.hook_resid_post 16384 none layer_23/width_16k/average_l0_63Load this SAE jumprelu blocks.23.hook_resid_post 16384 none layer_24/width_131k/average_l0_10Load this SAE jumprelu blocks.24.hook_resid_post 131072 none layer_24/width_131k/average_l0_17Load this SAE jumprelu blocks.24.hook_resid_post 131072 none layer_24/width_131k/average_l0_180Load this SAE jumprelu blocks.24.hook_resid_post 131072 none layer_24/width_131k/average_l0_30Load this SAE jumprelu blocks.24.hook_resid_post 131072 none layer_24/width_131k/average_l0_55Load this SAE jumprelu gemma-2-9b/24-gemmascope-res-131k__l0-55 blocks.24.hook_resid_post 131072 none layer_24/width_131k/average_l0_97Load this SAE jumprelu blocks.24.hook_resid_post 131072 none layer_24/width_16k/average_l0_10Load this SAE jumprelu blocks.24.hook_resid_post 16384 none layer_24/width_16k/average_l0_114Load this SAE jumprelu blocks.24.hook_resid_post 16384 none layer_24/width_16k/average_l0_19Load this SAE jumprelu blocks.24.hook_resid_post 16384 none layer_24/width_16k/average_l0_234Load this SAE jumprelu blocks.24.hook_resid_post 16384 none layer_24/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/24-gemmascope-res-16k__l0-34 blocks.24.hook_resid_post 16384 none layer_24/width_16k/average_l0_61Load this SAE jumprelu blocks.24.hook_resid_post 16384 none layer_25/width_131k/average_l0_10Load this SAE jumprelu blocks.25.hook_resid_post 131072 none layer_25/width_131k/average_l0_177Load this SAE jumprelu blocks.25.hook_resid_post 131072 none layer_25/width_131k/average_l0_18Load this SAE jumprelu blocks.25.hook_resid_post 131072 none layer_25/width_131k/average_l0_31Load this SAE jumprelu blocks.25.hook_resid_post 131072 none layer_25/width_131k/average_l0_54Load this SAE jumprelu gemma-2-9b/25-gemmascope-res-131k__l0-54 blocks.25.hook_resid_post 131072 none layer_25/width_131k/average_l0_96Load this SAE jumprelu blocks.25.hook_resid_post 131072 none layer_25/width_16k/average_l0_11Load this SAE jumprelu blocks.25.hook_resid_post 16384 none layer_25/width_16k/average_l0_114Load this SAE jumprelu blocks.25.hook_resid_post 16384 none layer_25/width_16k/average_l0_19Load this SAE jumprelu blocks.25.hook_resid_post 16384 none layer_25/width_16k/average_l0_231Load this SAE jumprelu blocks.25.hook_resid_post 16384 none layer_25/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/25-gemmascope-res-16k__l0-34 blocks.25.hook_resid_post 16384 none layer_25/width_16k/average_l0_61Load this SAE jumprelu blocks.25.hook_resid_post 16384 none layer_26/width_131k/average_l0_10Load this SAE jumprelu blocks.26.hook_resid_post 131072 none layer_26/width_131k/average_l0_176Load this SAE jumprelu blocks.26.hook_resid_post 131072 none layer_26/width_131k/average_l0_18Load this SAE jumprelu blocks.26.hook_resid_post 131072 none layer_26/width_131k/average_l0_32Load this SAE jumprelu gemma-2-9b/26-gemmascope-res-131k__l0-32 blocks.26.hook_resid_post 131072 none layer_26/width_131k/average_l0_55Load this SAE jumprelu blocks.26.hook_resid_post 131072 none layer_26/width_131k/average_l0_97Load this SAE jumprelu blocks.26.hook_resid_post 131072 none layer_26/width_16k/average_l0_11Load this SAE jumprelu blocks.26.hook_resid_post 16384 none layer_26/width_16k/average_l0_116Load this SAE jumprelu blocks.26.hook_resid_post 16384 none layer_26/width_16k/average_l0_20Load this SAE jumprelu blocks.26.hook_resid_post 16384 none layer_26/width_16k/average_l0_233Load this SAE jumprelu blocks.26.hook_resid_post 16384 none layer_26/width_16k/average_l0_35Load this SAE jumprelu gemma-2-9b/26-gemmascope-res-16k__l0-35 blocks.26.hook_resid_post 16384 none layer_26/width_16k/average_l0_63Load this SAE jumprelu blocks.26.hook_resid_post 16384 none layer_27/width_131k/average_l0_11Load this SAE jumprelu blocks.27.hook_resid_post 131072 none layer_27/width_131k/average_l0_171Load this SAE jumprelu blocks.27.hook_resid_post 131072 none layer_27/width_131k/average_l0_19Load this SAE jumprelu blocks.27.hook_resid_post 131072 none layer_27/width_131k/average_l0_33Load this SAE jumprelu gemma-2-9b/27-gemmascope-res-131k__l0-33 blocks.27.hook_resid_post 131072 none layer_27/width_131k/average_l0_56Load this SAE jumprelu blocks.27.hook_resid_post 131072 none layer_27/width_131k/average_l0_96Load this SAE jumprelu blocks.27.hook_resid_post 131072 none layer_27/width_16k/average_l0_118Load this SAE jumprelu blocks.27.hook_resid_post 16384 none layer_27/width_16k/average_l0_12Load this SAE jumprelu blocks.27.hook_resid_post 16384 none layer_27/width_16k/average_l0_21Load this SAE jumprelu blocks.27.hook_resid_post 16384 none layer_27/width_16k/average_l0_230Load this SAE jumprelu blocks.27.hook_resid_post 16384 none layer_27/width_16k/average_l0_36Load this SAE jumprelu gemma-2-9b/27-gemmascope-res-16k__l0-36 blocks.27.hook_resid_post 16384 none layer_27/width_16k/average_l0_65Load this SAE jumprelu blocks.27.hook_resid_post 16384 none layer_28/width_131k/average_l0_11Load this SAE jumprelu blocks.28.hook_resid_post 131072 none layer_28/width_131k/average_l0_171Load this SAE jumprelu blocks.28.hook_resid_post 131072 none layer_28/width_131k/average_l0_19Load this SAE jumprelu blocks.28.hook_resid_post 131072 none layer_28/width_131k/average_l0_32Load this SAE jumprelu gemma-2-9b/28-gemmascope-res-131k__l0-32 blocks.28.hook_resid_post 131072 none layer_28/width_131k/average_l0_57Load this SAE jumprelu blocks.28.hook_resid_post 131072 none layer_28/width_131k/average_l0_98Load this SAE jumprelu blocks.28.hook_resid_post 131072 none layer_28/width_16k/average_l0_119Load this SAE jumprelu blocks.28.hook_resid_post 16384 none layer_28/width_16k/average_l0_12Load this SAE jumprelu blocks.28.hook_resid_post 16384 none layer_28/width_16k/average_l0_21Load this SAE jumprelu blocks.28.hook_resid_post 16384 none layer_28/width_16k/average_l0_229Load this SAE jumprelu blocks.28.hook_resid_post 16384 none layer_28/width_16k/average_l0_37Load this SAE jumprelu gemma-2-9b/28-gemmascope-res-16k__l0-37 blocks.28.hook_resid_post 16384 none layer_28/width_16k/average_l0_65Load this SAE jumprelu blocks.28.hook_resid_post 16384 none layer_29/width_131k/average_l0_11Load this SAE jumprelu blocks.29.hook_resid_post 131072 none layer_29/width_131k/average_l0_171Load this SAE jumprelu blocks.29.hook_resid_post 131072 none layer_29/width_131k/average_l0_19Load this SAE jumprelu blocks.29.hook_resid_post 131072 none layer_29/width_131k/average_l0_33Load this SAE jumprelu gemma-2-9b/29-gemmascope-res-131k__l0-33 blocks.29.hook_resid_post 131072 none layer_29/width_131k/average_l0_56Load this SAE jumprelu blocks.29.hook_resid_post 131072 none layer_29/width_131k/average_l0_97Load this SAE jumprelu blocks.29.hook_resid_post 131072 none layer_29/width_16k/average_l0_119Load this SAE jumprelu blocks.29.hook_resid_post 16384 none layer_29/width_16k/average_l0_12Load this SAE jumprelu blocks.29.hook_resid_post 16384 none layer_29/width_16k/average_l0_21Load this SAE jumprelu blocks.29.hook_resid_post 16384 none layer_29/width_16k/average_l0_224Load this SAE jumprelu blocks.29.hook_resid_post 16384 none layer_29/width_16k/average_l0_38Load this SAE jumprelu gemma-2-9b/29-gemmascope-res-16k__l0-38 blocks.29.hook_resid_post 16384 none layer_29/width_16k/average_l0_66Load this SAE jumprelu blocks.29.hook_resid_post 16384 none layer_3/width_131k/average_l0_103Load this SAE jumprelu blocks.3.hook_resid_post 131072 none layer_3/width_131k/average_l0_14Load this SAE jumprelu blocks.3.hook_resid_post 131072 none layer_3/width_131k/average_l0_25Load this SAE jumprelu blocks.3.hook_resid_post 131072 none layer_3/width_131k/average_l0_46Load this SAE jumprelu gemma-2-9b/3-gemmascope-res-131k__l0-46 blocks.3.hook_resid_post 131072 none layer_3/width_131k/average_l0_6Load this SAE jumprelu blocks.3.hook_resid_post 131072 none layer_3/width_131k/average_l0_9Load this SAE jumprelu blocks.3.hook_resid_post 131072 none layer_3/width_16k/average_l0_10Load this SAE jumprelu blocks.3.hook_resid_post 16384 none layer_3/width_16k/average_l0_17Load this SAE jumprelu blocks.3.hook_resid_post 16384 none layer_3/width_16k/average_l0_229Load this SAE jumprelu blocks.3.hook_resid_post 16384 none layer_3/width_16k/average_l0_37Load this SAE jumprelu gemma-2-9b/3-gemmascope-res-16k__l0-37 blocks.3.hook_resid_post 16384 none layer_3/width_16k/average_l0_90Load this SAE jumprelu blocks.3.hook_resid_post 16384 none layer_30/width_131k/average_l0_11Load this SAE jumprelu blocks.30.hook_resid_post 131072 none layer_30/width_131k/average_l0_170Load this SAE jumprelu blocks.30.hook_resid_post 131072 none layer_30/width_131k/average_l0_18Load this SAE jumprelu blocks.30.hook_resid_post 131072 none layer_30/width_131k/average_l0_32Load this SAE jumprelu gemma-2-9b/30-gemmascope-res-131k__l0-32 blocks.30.hook_resid_post 131072 none layer_30/width_131k/average_l0_56Load this SAE jumprelu blocks.30.hook_resid_post 131072 none layer_30/width_131k/average_l0_95Load this SAE jumprelu blocks.30.hook_resid_post 131072 none layer_30/width_16k/average_l0_12Load this SAE jumprelu blocks.30.hook_resid_post 16384 none layer_30/width_16k/average_l0_120Load this SAE jumprelu blocks.30.hook_resid_post 16384 none layer_30/width_16k/average_l0_21Load this SAE jumprelu blocks.30.hook_resid_post 16384 none layer_30/width_16k/average_l0_226Load this SAE jumprelu blocks.30.hook_resid_post 16384 none layer_30/width_16k/average_l0_37Load this SAE jumprelu gemma-2-9b/30-gemmascope-res-16k__l0-37 blocks.30.hook_resid_post 16384 none layer_30/width_16k/average_l0_66Load this SAE jumprelu blocks.30.hook_resid_post 16384 none layer_31/width_131k/average_l0_10Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-131k__l0-10 blocks.31.hook_resid_post 131072 none layer_31/width_131k/average_l0_160Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-131k__l0-160 blocks.31.hook_resid_post 131072 none layer_31/width_131k/average_l0_18Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-131k__l0-18 blocks.31.hook_resid_post 131072 none layer_31/width_131k/average_l0_31Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-131k__l0-31 blocks.31.hook_resid_post 131072 none layer_31/width_131k/average_l0_52Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-131k__l0-52 blocks.31.hook_resid_post 131072 none layer_31/width_131k/average_l0_92Load this SAE jumprelu blocks.31.hook_resid_post 131072 none layer_31/width_16k/average_l0_11Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-16k__l0-11 blocks.31.hook_resid_post 16384 none layer_31/width_16k/average_l0_114Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-16k__l0-114 blocks.31.hook_resid_post 16384 none layer_31/width_16k/average_l0_20Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-16k__l0-20 blocks.31.hook_resid_post 16384 none layer_31/width_16k/average_l0_218Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-16k__l0-218 blocks.31.hook_resid_post 16384 none layer_31/width_16k/average_l0_35Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-16k__l0-35 blocks.31.hook_resid_post 16384 none layer_31/width_16k/average_l0_63Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-16k__l0-63 blocks.31.hook_resid_post 16384 none layer_31/width_1m/average_l0_11Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-1m__l0-11 blocks.31.hook_resid_post 1048576 none layer_31/width_1m/average_l0_132Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-1m__l0-132 blocks.31.hook_resid_post 1048576 none layer_31/width_1m/average_l0_25Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-1m__l0-25 blocks.31.hook_resid_post 1048576 none layer_31/width_1m/average_l0_27Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-1m__l0-27 blocks.31.hook_resid_post 1048576 none layer_31/width_1m/average_l0_45Load this SAE jumprelu gemma-2-9b/31-gemmascope-res-1m__l0-45 blocks.31.hook_resid_post 1048576 none layer_31/width_1m/average_l0_77Load this SAE jumprelu blocks.31.hook_resid_post 1048576 none layer_32/width_131k/average_l0_10Load this SAE jumprelu blocks.32.hook_resid_post 131072 none layer_32/width_131k/average_l0_158Load this SAE jumprelu blocks.32.hook_resid_post 131072 none layer_32/width_131k/average_l0_18Load this SAE jumprelu blocks.32.hook_resid_post 131072 none layer_32/width_131k/average_l0_30Load this SAE jumprelu blocks.32.hook_resid_post 131072 none layer_32/width_131k/average_l0_51Load this SAE jumprelu gemma-2-9b/32-gemmascope-res-131k__l0-51 blocks.32.hook_resid_post 131072 none layer_32/width_131k/average_l0_88Load this SAE jumprelu blocks.32.hook_resid_post 131072 none layer_32/width_16k/average_l0_11Load this SAE jumprelu blocks.32.hook_resid_post 16384 none layer_32/width_16k/average_l0_111Load this SAE jumprelu blocks.32.hook_resid_post 16384 none layer_32/width_16k/average_l0_20Load this SAE jumprelu blocks.32.hook_resid_post 16384 none layer_32/width_16k/average_l0_219Load this SAE jumprelu blocks.32.hook_resid_post 16384 none layer_32/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/32-gemmascope-res-16k__l0-34 blocks.32.hook_resid_post 16384 none layer_32/width_16k/average_l0_61Load this SAE jumprelu blocks.32.hook_resid_post 16384 none layer_33/width_131k/average_l0_10Load this SAE jumprelu blocks.33.hook_resid_post 131072 none layer_33/width_131k/average_l0_165Load this SAE jumprelu blocks.33.hook_resid_post 131072 none layer_33/width_131k/average_l0_18Load this SAE jumprelu blocks.33.hook_resid_post 131072 none layer_33/width_131k/average_l0_30Load this SAE jumprelu blocks.33.hook_resid_post 131072 none layer_33/width_131k/average_l0_51Load this SAE jumprelu gemma-2-9b/33-gemmascope-res-131k__l0-51 blocks.33.hook_resid_post 131072 none layer_33/width_131k/average_l0_91Load this SAE jumprelu blocks.33.hook_resid_post 131072 none layer_33/width_16k/average_l0_11Load this SAE jumprelu blocks.33.hook_resid_post 16384 none layer_33/width_16k/average_l0_114Load this SAE jumprelu blocks.33.hook_resid_post 16384 none layer_33/width_16k/average_l0_20Load this SAE jumprelu blocks.33.hook_resid_post 16384 none layer_33/width_16k/average_l0_228Load this SAE jumprelu blocks.33.hook_resid_post 16384 none layer_33/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/33-gemmascope-res-16k__l0-34 blocks.33.hook_resid_post 16384 none layer_33/width_16k/average_l0_63Load this SAE jumprelu blocks.33.hook_resid_post 16384 none layer_34/width_131k/average_l0_10Load this SAE jumprelu blocks.34.hook_resid_post 131072 none layer_34/width_131k/average_l0_163Load this SAE jumprelu blocks.34.hook_resid_post 131072 none layer_34/width_131k/average_l0_17Load this SAE jumprelu blocks.34.hook_resid_post 131072 none layer_34/width_131k/average_l0_30Load this SAE jumprelu blocks.34.hook_resid_post 131072 none layer_34/width_131k/average_l0_51Load this SAE jumprelu gemma-2-9b/34-gemmascope-res-131k__l0-51 blocks.34.hook_resid_post 131072 none layer_34/width_131k/average_l0_89Load this SAE jumprelu blocks.34.hook_resid_post 131072 none layer_34/width_16k/average_l0_11Load this SAE jumprelu blocks.34.hook_resid_post 16384 none layer_34/width_16k/average_l0_114Load this SAE jumprelu blocks.34.hook_resid_post 16384 none layer_34/width_16k/average_l0_19Load this SAE jumprelu blocks.34.hook_resid_post 16384 none layer_34/width_16k/average_l0_229Load this SAE jumprelu blocks.34.hook_resid_post 16384 none layer_34/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/34-gemmascope-res-16k__l0-34 blocks.34.hook_resid_post 16384 none layer_34/width_16k/average_l0_60Load this SAE jumprelu blocks.34.hook_resid_post 16384 none layer_35/width_131k/average_l0_10Load this SAE jumprelu blocks.35.hook_resid_post 131072 none layer_35/width_131k/average_l0_17Load this SAE jumprelu blocks.35.hook_resid_post 131072 none layer_35/width_131k/average_l0_171Load this SAE jumprelu blocks.35.hook_resid_post 131072 none layer_35/width_131k/average_l0_30Load this SAE jumprelu blocks.35.hook_resid_post 131072 none layer_35/width_131k/average_l0_51Load this SAE jumprelu gemma-2-9b/35-gemmascope-res-131k__l0-51 blocks.35.hook_resid_post 131072 none layer_35/width_131k/average_l0_94Load this SAE jumprelu blocks.35.hook_resid_post 131072 none layer_35/width_16k/average_l0_11Load this SAE jumprelu blocks.35.hook_resid_post 16384 none layer_35/width_16k/average_l0_120Load this SAE jumprelu blocks.35.hook_resid_post 16384 none layer_35/width_16k/average_l0_19Load this SAE jumprelu blocks.35.hook_resid_post 16384 none layer_35/width_16k/average_l0_246Load this SAE jumprelu blocks.35.hook_resid_post 16384 none layer_35/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/35-gemmascope-res-16k__l0-34 blocks.35.hook_resid_post 16384 none layer_35/width_16k/average_l0_61Load this SAE jumprelu blocks.35.hook_resid_post 16384 none layer_36/width_131k/average_l0_10Load this SAE jumprelu blocks.36.hook_resid_post 131072 none layer_36/width_131k/average_l0_17Load this SAE jumprelu blocks.36.hook_resid_post 131072 none layer_36/width_131k/average_l0_180Load this SAE jumprelu blocks.36.hook_resid_post 131072 none layer_36/width_131k/average_l0_30Load this SAE jumprelu blocks.36.hook_resid_post 131072 none layer_36/width_131k/average_l0_51Load this SAE jumprelu gemma-2-9b/36-gemmascope-res-131k__l0-51 blocks.36.hook_resid_post 131072 none layer_36/width_131k/average_l0_93Load this SAE jumprelu blocks.36.hook_resid_post 131072 none layer_36/width_16k/average_l0_11Load this SAE jumprelu blocks.36.hook_resid_post 16384 none layer_36/width_16k/average_l0_120Load this SAE jumprelu blocks.36.hook_resid_post 16384 none layer_36/width_16k/average_l0_19Load this SAE jumprelu blocks.36.hook_resid_post 16384 none layer_36/width_16k/average_l0_252Load this SAE jumprelu blocks.36.hook_resid_post 16384 none layer_36/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/36-gemmascope-res-16k__l0-34 blocks.36.hook_resid_post 16384 none layer_36/width_16k/average_l0_61Load this SAE jumprelu blocks.36.hook_resid_post 16384 none layer_37/width_131k/average_l0_10Load this SAE jumprelu blocks.37.hook_resid_post 131072 none layer_37/width_131k/average_l0_18Load this SAE jumprelu blocks.37.hook_resid_post 131072 none layer_37/width_131k/average_l0_184Load this SAE jumprelu blocks.37.hook_resid_post 131072 none layer_37/width_131k/average_l0_30Load this SAE jumprelu blocks.37.hook_resid_post 131072 none layer_37/width_131k/average_l0_53Load this SAE jumprelu gemma-2-9b/37-gemmascope-res-131k__l0-53 blocks.37.hook_resid_post 131072 none layer_37/width_131k/average_l0_96Load this SAE jumprelu blocks.37.hook_resid_post 131072 none layer_37/width_16k/average_l0_11Load this SAE jumprelu blocks.37.hook_resid_post 16384 none layer_37/width_16k/average_l0_124Load this SAE jumprelu blocks.37.hook_resid_post 16384 none layer_37/width_16k/average_l0_20Load this SAE jumprelu blocks.37.hook_resid_post 16384 none layer_37/width_16k/average_l0_266Load this SAE jumprelu blocks.37.hook_resid_post 16384 none layer_37/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/37-gemmascope-res-16k__l0-34 blocks.37.hook_resid_post 16384 none layer_37/width_16k/average_l0_63Load this SAE jumprelu blocks.37.hook_resid_post 16384 none layer_38/width_131k/average_l0_10Load this SAE jumprelu blocks.38.hook_resid_post 131072 none layer_38/width_131k/average_l0_101Load this SAE jumprelu blocks.38.hook_resid_post 131072 none layer_38/width_131k/average_l0_18Load this SAE jumprelu blocks.38.hook_resid_post 131072 none layer_38/width_131k/average_l0_194Load this SAE jumprelu blocks.38.hook_resid_post 131072 none layer_38/width_131k/average_l0_30Load this SAE jumprelu blocks.38.hook_resid_post 131072 none layer_38/width_131k/average_l0_53Load this SAE jumprelu gemma-2-9b/38-gemmascope-res-131k__l0-53 blocks.38.hook_resid_post 131072 none layer_38/width_16k/average_l0_11Load this SAE jumprelu blocks.38.hook_resid_post 16384 none layer_38/width_16k/average_l0_128Load this SAE jumprelu blocks.38.hook_resid_post 16384 none layer_38/width_16k/average_l0_20Load this SAE jumprelu blocks.38.hook_resid_post 16384 none layer_38/width_16k/average_l0_286Load this SAE jumprelu blocks.38.hook_resid_post 16384 none layer_38/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/38-gemmascope-res-16k__l0-34 blocks.38.hook_resid_post 16384 none layer_38/width_16k/average_l0_64Load this SAE jumprelu blocks.38.hook_resid_post 16384 none layer_39/width_131k/average_l0_10Load this SAE jumprelu blocks.39.hook_resid_post 131072 none layer_39/width_131k/average_l0_18Load this SAE jumprelu blocks.39.hook_resid_post 131072 none layer_39/width_131k/average_l0_199Load this SAE jumprelu blocks.39.hook_resid_post 131072 none layer_39/width_131k/average_l0_30Load this SAE jumprelu blocks.39.hook_resid_post 131072 none layer_39/width_131k/average_l0_54Load this SAE jumprelu gemma-2-9b/39-gemmascope-res-131k__l0-54 blocks.39.hook_resid_post 131072 none layer_39/width_131k/average_l0_99Load this SAE jumprelu blocks.39.hook_resid_post 131072 none layer_39/width_16k/average_l0_11Load this SAE jumprelu blocks.39.hook_resid_post 16384 none layer_39/width_16k/average_l0_131Load this SAE jumprelu blocks.39.hook_resid_post 16384 none layer_39/width_16k/average_l0_19Load this SAE jumprelu blocks.39.hook_resid_post 16384 none layer_39/width_16k/average_l0_298Load this SAE jumprelu blocks.39.hook_resid_post 16384 none layer_39/width_16k/average_l0_34Load this SAE jumprelu gemma-2-9b/39-gemmascope-res-16k__l0-34 blocks.39.hook_resid_post 16384 none layer_39/width_16k/average_l0_64Load this SAE jumprelu blocks.39.hook_resid_post 16384 none layer_4/width_131k/average_l0_101Load this SAE jumprelu blocks.4.hook_resid_post 131072 none layer_4/width_131k/average_l0_16Load this SAE jumprelu blocks.4.hook_resid_post 131072 none layer_4/width_131k/average_l0_28Load this SAE jumprelu blocks.4.hook_resid_post 131072 none layer_4/width_131k/average_l0_51Load this SAE jumprelu gemma-2-9b/4-gemmascope-res-131k__l0-51 blocks.4.hook_resid_post 131072 none layer_4/width_131k/average_l0_6Load this SAE jumprelu blocks.4.hook_resid_post 131072 none layer_4/width_131k/average_l0_9Load this SAE jumprelu blocks.4.hook_resid_post 131072 none layer_4/width_16k/average_l0_10Load this SAE jumprelu blocks.4.hook_resid_post 16384 none layer_4/width_16k/average_l0_18Load this SAE jumprelu blocks.4.hook_resid_post 16384 none layer_4/width_16k/average_l0_234Load this SAE jumprelu blocks.4.hook_resid_post 16384 none layer_4/width_16k/average_l0_37Load this SAE jumprelu gemma-2-9b/4-gemmascope-res-16k__l0-37 blocks.4.hook_resid_post 16384 none layer_4/width_16k/average_l0_91Load this SAE jumprelu blocks.4.hook_resid_post 16384 none layer_40/width_131k/average_l0_10Load this SAE jumprelu blocks.40.hook_resid_post 131072 none layer_40/width_131k/average_l0_17Load this SAE jumprelu blocks.40.hook_resid_post 131072 none layer_40/width_131k/average_l0_193Load this SAE jumprelu blocks.40.hook_resid_post 131072 none layer_40/width_131k/average_l0_29Load this SAE jumprelu blocks.40.hook_resid_post 131072 none layer_40/width_131k/average_l0_49Load this SAE jumprelu gemma-2-9b/40-gemmascope-res-131k__l0-49 blocks.40.hook_resid_post 131072 none layer_40/width_131k/average_l0_94Load this SAE jumprelu blocks.40.hook_resid_post 131072 none layer_40/width_16k/average_l0_10Load this SAE jumprelu blocks.40.hook_resid_post 16384 none layer_40/width_16k/average_l0_125Load this SAE jumprelu blocks.40.hook_resid_post 16384 none layer_40/width_16k/average_l0_18Load this SAE jumprelu blocks.40.hook_resid_post 16384 none layer_40/width_16k/average_l0_292Load this SAE jumprelu blocks.40.hook_resid_post 16384 none layer_40/width_16k/average_l0_32Load this SAE jumprelu gemma-2-9b/40-gemmascope-res-16k__l0-32 blocks.40.hook_resid_post 16384 none layer_40/width_16k/average_l0_61Load this SAE jumprelu blocks.40.hook_resid_post 16384 none layer_41/width_131k/average_l0_10Load this SAE jumprelu blocks.41.hook_resid_post 131072 none layer_41/width_131k/average_l0_15Load this SAE jumprelu blocks.41.hook_resid_post 131072 none layer_41/width_131k/average_l0_175Load this SAE jumprelu blocks.41.hook_resid_post 131072 none layer_41/width_131k/average_l0_26Load this SAE jumprelu blocks.41.hook_resid_post 131072 none layer_41/width_131k/average_l0_45Load this SAE jumprelu gemma-2-9b/41-gemmascope-res-131k__l0-45 blocks.41.hook_resid_post 131072 none layer_41/width_131k/average_l0_84Load this SAE jumprelu blocks.41.hook_resid_post 131072 none layer_41/width_16k/average_l0_10Load this SAE jumprelu blocks.41.hook_resid_post 16384 none layer_41/width_16k/average_l0_113Load this SAE jumprelu blocks.41.hook_resid_post 16384 none layer_41/width_16k/average_l0_16Load this SAE jumprelu blocks.41.hook_resid_post 16384 none layer_41/width_16k/average_l0_270Load this SAE jumprelu blocks.41.hook_resid_post 16384 none layer_41/width_16k/average_l0_28Load this SAE jumprelu blocks.41.hook_resid_post 16384 none layer_41/width_16k/average_l0_52Load this SAE jumprelu gemma-2-9b/41-gemmascope-res-16k__l0-52 blocks.41.hook_resid_post 16384 none layer_5/width_131k/average_l0_10Load this SAE jumprelu blocks.5.hook_resid_post 131072 none layer_5/width_131k/average_l0_16Load this SAE jumprelu blocks.5.hook_resid_post 131072 none layer_5/width_131k/average_l0_29Load this SAE jumprelu blocks.5.hook_resid_post 131072 none layer_5/width_131k/average_l0_51Load this SAE jumprelu gemma-2-9b/5-gemmascope-res-131k__l0-51 blocks.5.hook_resid_post 131072 none layer_5/width_131k/average_l0_6Load this SAE jumprelu blocks.5.hook_resid_post 131072 none layer_5/width_131k/average_l0_94Load this SAE jumprelu blocks.5.hook_resid_post 131072 none layer_5/width_16k/average_l0_11Load this SAE jumprelu blocks.5.hook_resid_post 16384 none layer_5/width_16k/average_l0_193Load this SAE jumprelu blocks.5.hook_resid_post 16384 none layer_5/width_16k/average_l0_20Load this SAE jumprelu blocks.5.hook_resid_post 16384 none layer_5/width_16k/average_l0_37Load this SAE jumprelu gemma-2-9b/5-gemmascope-res-16k__l0-37 blocks.5.hook_resid_post 16384 none layer_5/width_16k/average_l0_77Load this SAE jumprelu blocks.5.hook_resid_post 16384 none layer_6/width_131k/average_l0_12Load this SAE jumprelu blocks.6.hook_resid_post 131072 none layer_6/width_131k/average_l0_120Load this SAE jumprelu blocks.6.hook_resid_post 131072 none layer_6/width_131k/average_l0_21Load this SAE jumprelu blocks.6.hook_resid_post 131072 none layer_6/width_131k/average_l0_36Load this SAE jumprelu blocks.6.hook_resid_post 131072 none layer_6/width_131k/average_l0_66Load this SAE jumprelu gemma-2-9b/6-gemmascope-res-131k__l0-66 blocks.6.hook_resid_post 131072 none layer_6/width_131k/average_l0_7Load this SAE jumprelu blocks.6.hook_resid_post 131072 none layer_6/width_16k/average_l0_14Load this SAE jumprelu blocks.6.hook_resid_post 16384 none layer_6/width_16k/average_l0_224Load this SAE jumprelu blocks.6.hook_resid_post 16384 none layer_6/width_16k/average_l0_25Load this SAE jumprelu blocks.6.hook_resid_post 16384 none layer_6/width_16k/average_l0_47Load this SAE jumprelu gemma-2-9b/6-gemmascope-res-16k__l0-47 blocks.6.hook_resid_post 16384 none layer_6/width_16k/average_l0_8Load this SAE jumprelu blocks.6.hook_resid_post 16384 none layer_6/width_16k/average_l0_93Load this SAE jumprelu blocks.6.hook_resid_post 16384 none layer_7/width_131k/average_l0_119Load this SAE jumprelu blocks.7.hook_resid_post 131072 none layer_7/width_131k/average_l0_13Load this SAE jumprelu blocks.7.hook_resid_post 131072 none layer_7/width_131k/average_l0_21Load this SAE jumprelu blocks.7.hook_resid_post 131072 none layer_7/width_131k/average_l0_38Load this SAE jumprelu gemma-2-9b/7-gemmascope-res-131k__l0-38 blocks.7.hook_resid_post 131072 none layer_7/width_131k/average_l0_65Load this SAE jumprelu blocks.7.hook_resid_post 131072 none layer_7/width_131k/average_l0_8Load this SAE jumprelu blocks.7.hook_resid_post 131072 none layer_7/width_16k/average_l0_14Load this SAE jumprelu blocks.7.hook_resid_post 16384 none layer_7/width_16k/average_l0_198Load this SAE jumprelu blocks.7.hook_resid_post 16384 none layer_7/width_16k/average_l0_25Load this SAE jumprelu blocks.7.hook_resid_post 16384 none layer_7/width_16k/average_l0_46Load this SAE jumprelu gemma-2-9b/7-gemmascope-res-16k__l0-46 blocks.7.hook_resid_post 16384 none layer_7/width_16k/average_l0_8Load this SAE jumprelu blocks.7.hook_resid_post 16384 none layer_7/width_16k/average_l0_92Load this SAE jumprelu blocks.7.hook_resid_post 16384 none layer_8/width_131k/average_l0_129Load this SAE jumprelu blocks.8.hook_resid_post 131072 none layer_8/width_131k/average_l0_14Load this SAE jumprelu blocks.8.hook_resid_post 131072 none layer_8/width_131k/average_l0_24Load this SAE jumprelu blocks.8.hook_resid_post 131072 none layer_8/width_131k/average_l0_41Load this SAE jumprelu gemma-2-9b/8-gemmascope-res-131k__l0-41 blocks.8.hook_resid_post 131072 none layer_8/width_131k/average_l0_72Load this SAE jumprelu blocks.8.hook_resid_post 131072 none layer_8/width_131k/average_l0_8Load this SAE jumprelu blocks.8.hook_resid_post 131072 none layer_8/width_16k/average_l0_16Load this SAE jumprelu blocks.8.hook_resid_post 16384 none layer_8/width_16k/average_l0_207Load this SAE jumprelu blocks.8.hook_resid_post 16384 none layer_8/width_16k/average_l0_28Load this SAE jumprelu blocks.8.hook_resid_post 16384 none layer_8/width_16k/average_l0_51Load this SAE jumprelu gemma-2-9b/8-gemmascope-res-16k__l0-51 blocks.8.hook_resid_post 16384 none layer_8/width_16k/average_l0_9Load this SAE jumprelu blocks.8.hook_resid_post 16384 none layer_8/width_16k/average_l0_99Load this SAE jumprelu blocks.8.hook_resid_post 16384 none layer_9/width_131k/average_l0_134Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-131k__l0-134 blocks.9.hook_resid_post 131072 none layer_9/width_131k/average_l0_14Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-131k__l0-14 blocks.9.hook_resid_post 131072 none layer_9/width_131k/average_l0_25Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-131k__l0-25 blocks.9.hook_resid_post 131072 none layer_9/width_131k/average_l0_42Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-131k__l0-42 blocks.9.hook_resid_post 131072 none layer_9/width_131k/average_l0_75Load this SAE jumprelu blocks.9.hook_resid_post 131072 none layer_9/width_131k/average_l0_8Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-131k__l0-8 blocks.9.hook_resid_post 131072 none layer_9/width_16k/average_l0_100Load this SAE jumprelu blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_16Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-16k__l0-16 blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_209Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-16k__l0-209 blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_28Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-16k__l0-28 blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_51Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-16k__l0-51 blocks.9.hook_resid_post 16384 none layer_9/width_16k/average_l0_9Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-16k__l0-9 blocks.9.hook_resid_post 16384 none layer_9/width_1m/average_l0_122Load this SAE jumprelu blocks.9.hook_resid_post 1048576 none layer_9/width_1m/average_l0_14Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-1m__l0-14 blocks.9.hook_resid_post 1048576 none layer_9/width_1m/average_l0_24Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-1m__l0-24 blocks.9.hook_resid_post 1048576 none layer_9/width_1m/average_l0_41Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-1m__l0-41 blocks.9.hook_resid_post 1048576 none layer_9/width_1m/average_l0_70Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-1m__l0-70 blocks.9.hook_resid_post 1048576 none layer_9/width_1m/average_l0_9Load this SAE jumprelu gemma-2-9b/9-gemmascope-res-1m__l0-9 blocks.9.hook_resid_post 1048576 none"},{"location":"sae_table/#gemma-scope-9b-pt-res-canonical","title":"gemma-scope-9b-pt-res-canonical","text":"<ul> <li>Huggingface Repo: google/gemma-scope-9b-pt-res</li> <li>model: gemma-2-9b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/0-gemmascope-res-16k blocks.0.hook_resid_post 16384 none layer_1/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/1-gemmascope-res-16k blocks.1.hook_resid_post 16384 none layer_2/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/2-gemmascope-res-16k blocks.2.hook_resid_post 16384 none layer_3/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/3-gemmascope-res-16k blocks.3.hook_resid_post 16384 none layer_4/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/4-gemmascope-res-16k blocks.4.hook_resid_post 16384 none layer_5/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/5-gemmascope-res-16k blocks.5.hook_resid_post 16384 none layer_6/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/6-gemmascope-res-16k blocks.6.hook_resid_post 16384 none layer_7/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/7-gemmascope-res-16k blocks.7.hook_resid_post 16384 none layer_8/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/8-gemmascope-res-16k blocks.8.hook_resid_post 16384 none layer_9/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/9-gemmascope-res-16k blocks.9.hook_resid_post 16384 none layer_10/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/10-gemmascope-res-16k blocks.10.hook_resid_post 16384 none layer_11/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/11-gemmascope-res-16k blocks.11.hook_resid_post 16384 none layer_12/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/12-gemmascope-res-16k blocks.12.hook_resid_post 16384 none layer_13/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/13-gemmascope-res-16k blocks.13.hook_resid_post 16384 none layer_14/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/14-gemmascope-res-16k blocks.14.hook_resid_post 16384 none layer_15/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/15-gemmascope-res-16k blocks.15.hook_resid_post 16384 none layer_16/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/16-gemmascope-res-16k blocks.16.hook_resid_post 16384 none layer_17/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/17-gemmascope-res-16k blocks.17.hook_resid_post 16384 none layer_18/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/18-gemmascope-res-16k blocks.18.hook_resid_post 16384 none layer_19/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/19-gemmascope-res-16k blocks.19.hook_resid_post 16384 none layer_20/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/20-gemmascope-res-16k blocks.20.hook_resid_post 16384 none layer_21/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/21-gemmascope-res-16k blocks.21.hook_resid_post 16384 none layer_22/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/22-gemmascope-res-16k blocks.22.hook_resid_post 16384 none layer_23/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/23-gemmascope-res-16k blocks.23.hook_resid_post 16384 none layer_24/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/24-gemmascope-res-16k blocks.24.hook_resid_post 16384 none layer_25/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/25-gemmascope-res-16k blocks.25.hook_resid_post 16384 none layer_26/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/26-gemmascope-res-16k blocks.26.hook_resid_post 16384 none layer_27/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/27-gemmascope-res-16k blocks.27.hook_resid_post 16384 none layer_28/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/28-gemmascope-res-16k blocks.28.hook_resid_post 16384 none layer_29/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/29-gemmascope-res-16k blocks.29.hook_resid_post 16384 none layer_30/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/30-gemmascope-res-16k blocks.30.hook_resid_post 16384 none layer_31/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/31-gemmascope-res-16k blocks.31.hook_resid_post 16384 none layer_32/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/32-gemmascope-res-16k blocks.32.hook_resid_post 16384 none layer_33/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/33-gemmascope-res-16k blocks.33.hook_resid_post 16384 none layer_34/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/34-gemmascope-res-16k blocks.34.hook_resid_post 16384 none layer_35/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/35-gemmascope-res-16k blocks.35.hook_resid_post 16384 none layer_36/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/36-gemmascope-res-16k blocks.36.hook_resid_post 16384 none layer_37/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/37-gemmascope-res-16k blocks.37.hook_resid_post 16384 none layer_38/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/38-gemmascope-res-16k blocks.38.hook_resid_post 16384 none layer_39/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/39-gemmascope-res-16k blocks.39.hook_resid_post 16384 none layer_40/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/40-gemmascope-res-16k blocks.40.hook_resid_post 16384 none layer_41/width_16k/canonicalLoad this SAE jumprelu gemma-2-9b/41-gemmascope-res-16k blocks.41.hook_resid_post 16384 none layer_0/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/0-gemmascope-res-131k blocks.0.hook_resid_post 131072 none layer_1/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/1-gemmascope-res-131k blocks.1.hook_resid_post 131072 none layer_2/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/2-gemmascope-res-131k blocks.2.hook_resid_post 131072 none layer_3/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/3-gemmascope-res-131k blocks.3.hook_resid_post 131072 none layer_4/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/4-gemmascope-res-131k blocks.4.hook_resid_post 131072 none layer_5/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/5-gemmascope-res-131k blocks.5.hook_resid_post 131072 none layer_6/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/6-gemmascope-res-131k blocks.6.hook_resid_post 131072 none layer_7/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/7-gemmascope-res-131k blocks.7.hook_resid_post 131072 none layer_8/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/8-gemmascope-res-131k blocks.8.hook_resid_post 131072 none layer_9/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/9-gemmascope-res-131k blocks.9.hook_resid_post 131072 none layer_10/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/10-gemmascope-res-131k blocks.10.hook_resid_post 131072 none layer_11/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/11-gemmascope-res-131k blocks.11.hook_resid_post 131072 none layer_12/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/12-gemmascope-res-131k blocks.12.hook_resid_post 131072 none layer_13/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/13-gemmascope-res-131k blocks.13.hook_resid_post 131072 none layer_14/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/14-gemmascope-res-131k blocks.14.hook_resid_post 131072 none layer_15/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/15-gemmascope-res-131k blocks.15.hook_resid_post 131072 none layer_16/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/16-gemmascope-res-131k blocks.16.hook_resid_post 131072 none layer_17/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/17-gemmascope-res-131k blocks.17.hook_resid_post 131072 none layer_18/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/18-gemmascope-res-131k blocks.18.hook_resid_post 131072 none layer_19/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/19-gemmascope-res-131k blocks.19.hook_resid_post 131072 none layer_20/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/20-gemmascope-res-131k blocks.20.hook_resid_post 131072 none layer_21/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/21-gemmascope-res-131k blocks.21.hook_resid_post 131072 none layer_22/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/22-gemmascope-res-131k blocks.22.hook_resid_post 131072 none layer_23/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/23-gemmascope-res-131k blocks.23.hook_resid_post 131072 none layer_24/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/24-gemmascope-res-131k blocks.24.hook_resid_post 131072 none layer_25/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/25-gemmascope-res-131k blocks.25.hook_resid_post 131072 none layer_26/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/26-gemmascope-res-131k blocks.26.hook_resid_post 131072 none layer_27/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/27-gemmascope-res-131k blocks.27.hook_resid_post 131072 none layer_28/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/28-gemmascope-res-131k blocks.28.hook_resid_post 131072 none layer_29/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/29-gemmascope-res-131k blocks.29.hook_resid_post 131072 none layer_30/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/30-gemmascope-res-131k blocks.30.hook_resid_post 131072 none layer_31/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/31-gemmascope-res-131k blocks.31.hook_resid_post 131072 none layer_32/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/32-gemmascope-res-131k blocks.32.hook_resid_post 131072 none layer_33/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/33-gemmascope-res-131k blocks.33.hook_resid_post 131072 none layer_34/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/34-gemmascope-res-131k blocks.34.hook_resid_post 131072 none layer_35/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/35-gemmascope-res-131k blocks.35.hook_resid_post 131072 none layer_36/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/36-gemmascope-res-131k blocks.36.hook_resid_post 131072 none layer_37/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/37-gemmascope-res-131k blocks.37.hook_resid_post 131072 none layer_38/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/38-gemmascope-res-131k blocks.38.hook_resid_post 131072 none layer_39/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/39-gemmascope-res-131k blocks.39.hook_resid_post 131072 none layer_40/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/40-gemmascope-res-131k blocks.40.hook_resid_post 131072 none layer_41/width_131k/canonicalLoad this SAE jumprelu gemma-2-9b/41-gemmascope-res-131k blocks.41.hook_resid_post 131072 none layer_9/width_1m/canonicalLoad this SAE jumprelu gemma-2-9b/9-gemmascope-res-1m blocks.9.hook_resid_post 1048576 none layer_20/width_1m/canonicalLoad this SAE jumprelu gemma-2-9b/20-gemmascope-res-1m blocks.20.hook_resid_post 1048576 none layer_31/width_1m/canonicalLoad this SAE jumprelu gemma-2-9b/31-gemmascope-res-1m blocks.31.hook_resid_post 1048576 none layer_20/width_32k/canonicalLoad this SAE jumprelu gemma-2-9b/20-gemmascope-res-32k blocks.20.hook_resid_post 32768 none layer_20/width_524k/canonicalLoad this SAE jumprelu gemma-2-9b/20-gemmascope-res-524k blocks.20.hook_resid_post 524288 none layer_20/width_65k/canonicalLoad this SAE jumprelu gemma-2-9b/20-gemmascope-res-65k blocks.20.hook_resid_post 65536 none"},{"location":"sae_table/#gpt2-small-attn-out-v5-128k","title":"gpt2-small-attn-out-v5-128k","text":"<ul> <li>Huggingface Repo: jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs</li> <li>model: gpt2-small</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_attn_outLoad this SAE standard gpt2-small/0-att_128k-oai blocks.0.hook_attn_out 131072 layer_norm blocks.1.hook_attn_outLoad this SAE standard gpt2-small/1-att_128k-oai blocks.1.hook_attn_out 131072 layer_norm blocks.2.hook_attn_outLoad this SAE standard gpt2-small/2-att_128k-oai blocks.2.hook_attn_out 131072 layer_norm blocks.3.hook_attn_outLoad this SAE standard gpt2-small/3-att_128k-oai blocks.3.hook_attn_out 131072 layer_norm blocks.4.hook_attn_outLoad this SAE standard gpt2-small/4-att_128k-oai blocks.4.hook_attn_out 131072 layer_norm blocks.5.hook_attn_outLoad this SAE standard gpt2-small/5-att_128k-oai blocks.5.hook_attn_out 131072 layer_norm blocks.6.hook_attn_outLoad this SAE standard gpt2-small/6-att_128k-oai blocks.6.hook_attn_out 131072 layer_norm blocks.7.hook_attn_outLoad this SAE standard gpt2-small/7-att_128k-oai blocks.7.hook_attn_out 131072 layer_norm blocks.8.hook_attn_outLoad this SAE standard gpt2-small/8-att_128k-oai blocks.8.hook_attn_out 131072 layer_norm blocks.9.hook_attn_outLoad this SAE standard gpt2-small/9-att_128k-oai blocks.9.hook_attn_out 131072 layer_norm blocks.10.hook_attn_outLoad this SAE standard gpt2-small/10-att_128k-oai blocks.10.hook_attn_out 131072 layer_norm blocks.11.hook_attn_outLoad this SAE standard gpt2-small/11-att_128k-oai blocks.11.hook_attn_out 131072 layer_norm"},{"location":"sae_table/#gpt2-small-attn-out-v5-32k","title":"gpt2-small-attn-out-v5-32k","text":"<ul> <li>Huggingface Repo: jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs</li> <li>model: gpt2-small</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_attn_outLoad this SAE standard gpt2-small/0-att_32k-oai blocks.0.hook_attn_out 32768 layer_norm blocks.1.hook_attn_outLoad this SAE standard gpt2-small/1-att_32k-oai blocks.1.hook_attn_out 32768 layer_norm blocks.2.hook_attn_outLoad this SAE standard gpt2-small/2-att_32k-oai blocks.2.hook_attn_out 32768 layer_norm blocks.3.hook_attn_outLoad this SAE standard gpt2-small/3-att_32k-oai blocks.3.hook_attn_out 32768 layer_norm blocks.4.hook_attn_outLoad this SAE standard gpt2-small/4-att_32k-oai blocks.4.hook_attn_out 32768 layer_norm blocks.5.hook_attn_outLoad this SAE standard gpt2-small/5-att_32k-oai blocks.5.hook_attn_out 32768 layer_norm blocks.6.hook_attn_outLoad this SAE standard gpt2-small/6-att_32k-oai blocks.6.hook_attn_out 32768 layer_norm blocks.7.hook_attn_outLoad this SAE standard gpt2-small/7-att_32k-oai blocks.7.hook_attn_out 32768 layer_norm blocks.8.hook_attn_outLoad this SAE standard gpt2-small/8-att_32k-oai blocks.8.hook_attn_out 32768 layer_norm blocks.9.hook_attn_outLoad this SAE standard gpt2-small/9-att_32k-oai blocks.9.hook_attn_out 32768 layer_norm blocks.10.hook_attn_outLoad this SAE standard gpt2-small/10-att_32k-oai blocks.10.hook_attn_out 32768 layer_norm blocks.11.hook_attn_outLoad this SAE standard gpt2-small/11-att_32k-oai blocks.11.hook_attn_out 32768 layer_norm"},{"location":"sae_table/#gpt2-small-hook-z-kk","title":"gpt2-small-hook-z-kk","text":"<ul> <li>Huggingface Repo: ckkissane/attn-saes-gpt2-small-all-layers</li> <li>model: gpt2-small</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> <li>Publication</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_zLoad this SAE standard gpt2-small/0-att-kk blocks.0.attn.hook_z 24576 none blocks.1.hook_zLoad this SAE standard gpt2-small/1-att-kk blocks.1.attn.hook_z 24576 none blocks.2.hook_zLoad this SAE standard gpt2-small/2-att-kk blocks.2.attn.hook_z 24576 none blocks.3.hook_zLoad this SAE standard gpt2-small/3-att-kk blocks.3.attn.hook_z 24576 none blocks.4.hook_zLoad this SAE standard gpt2-small/4-att-kk blocks.4.attn.hook_z 24576 none blocks.5.hook_zLoad this SAE standard gpt2-small/5-att-kk blocks.5.attn.hook_z 49152 none blocks.6.hook_zLoad this SAE standard gpt2-small/6-att-kk blocks.6.attn.hook_z 24576 none blocks.7.hook_zLoad this SAE standard gpt2-small/7-att-kk blocks.7.attn.hook_z 49152 none blocks.8.hook_zLoad this SAE standard gpt2-small/8-att-kk blocks.8.attn.hook_z 24576 none blocks.9.hook_zLoad this SAE standard gpt2-small/9-att-kk blocks.9.attn.hook_z 24576 none blocks.10.hook_zLoad this SAE standard gpt2-small/10-att-kk blocks.10.attn.hook_z 24576 none blocks.11.hook_zLoad this SAE standard gpt2-small/11-att-kk blocks.11.attn.hook_z 24576 none"},{"location":"sae_table/#gpt2-small-mlp-out-v5-128k","title":"gpt2-small-mlp-out-v5-128k","text":"<ul> <li>Huggingface Repo: jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs</li> <li>model: gpt2-small</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_mlp_outLoad this SAE standard gpt2-small/0-mlp_128k-oai blocks.0.hook_mlp_out 131072 layer_norm blocks.1.hook_mlp_outLoad this SAE standard gpt2-small/1-mlp_128k-oai blocks.1.hook_mlp_out 131072 layer_norm blocks.2.hook_mlp_outLoad this SAE standard gpt2-small/2-mlp_128k-oai blocks.2.hook_mlp_out 131072 layer_norm blocks.3.hook_mlp_outLoad this SAE standard gpt2-small/3-mlp_128k-oai blocks.3.hook_mlp_out 131072 layer_norm blocks.4.hook_mlp_outLoad this SAE standard gpt2-small/4-mlp_128k-oai blocks.4.hook_mlp_out 131072 layer_norm blocks.5.hook_mlp_outLoad this SAE standard gpt2-small/5-mlp_128k-oai blocks.5.hook_mlp_out 131072 layer_norm blocks.6.hook_mlp_outLoad this SAE standard gpt2-small/6-mlp_128k-oai blocks.6.hook_mlp_out 131072 layer_norm blocks.7.hook_mlp_outLoad this SAE standard gpt2-small/7-mlp_128k-oai blocks.7.hook_mlp_out 131072 layer_norm blocks.8.hook_mlp_outLoad this SAE standard gpt2-small/8-mlp_128k-oai blocks.8.hook_mlp_out 131072 layer_norm blocks.9.hook_mlp_outLoad this SAE standard gpt2-small/9-mlp_128k-oai blocks.9.hook_mlp_out 131072 layer_norm blocks.10.hook_mlp_outLoad this SAE standard gpt2-small/10-mlp_128k-oai blocks.10.hook_mlp_out 131072 layer_norm blocks.11.hook_mlp_outLoad this SAE standard gpt2-small/11-mlp_128k-oai blocks.11.hook_mlp_out 131072 layer_norm"},{"location":"sae_table/#gpt2-small-mlp-out-v5-32k","title":"gpt2-small-mlp-out-v5-32k","text":"<ul> <li>Huggingface Repo: jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs</li> <li>model: gpt2-small</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_mlp_outLoad this SAE standard gpt2-small/0-mlp_32k-oai blocks.0.hook_mlp_out 32768 layer_norm blocks.1.hook_mlp_outLoad this SAE standard gpt2-small/1-mlp_32k-oai blocks.1.hook_mlp_out 32768 layer_norm blocks.2.hook_mlp_outLoad this SAE standard gpt2-small/2-mlp_32k-oai blocks.2.hook_mlp_out 32768 layer_norm blocks.3.hook_mlp_outLoad this SAE standard gpt2-small/3-mlp_32k-oai blocks.3.hook_mlp_out 32768 layer_norm blocks.4.hook_mlp_outLoad this SAE standard gpt2-small/4-mlp_32k-oai blocks.4.hook_mlp_out 32768 layer_norm blocks.5.hook_mlp_outLoad this SAE standard gpt2-small/5-mlp_32k-oai blocks.5.hook_mlp_out 32768 layer_norm blocks.6.hook_mlp_outLoad this SAE standard gpt2-small/6-mlp_32k-oai blocks.6.hook_mlp_out 32768 layer_norm blocks.7.hook_mlp_outLoad this SAE standard gpt2-small/7-mlp_32k-oai blocks.7.hook_mlp_out 32768 layer_norm blocks.8.hook_mlp_outLoad this SAE standard gpt2-small/8-mlp_32k-oai blocks.8.hook_mlp_out 32768 layer_norm blocks.9.hook_mlp_outLoad this SAE standard gpt2-small/9-mlp_32k-oai blocks.9.hook_mlp_out 32768 layer_norm blocks.10.hook_mlp_outLoad this SAE standard gpt2-small/10-mlp_32k-oai blocks.10.hook_mlp_out 32768 layer_norm blocks.11.hook_mlp_outLoad this SAE standard gpt2-small/11-mlp_32k-oai blocks.11.hook_mlp_out 32768 layer_norm"},{"location":"sae_table/#gpt2-small-mlp-tm","title":"gpt2-small-mlp-tm","text":"<ul> <li>Huggingface Repo: tommmcgrath/gpt2-small-mlp-out-saes</li> <li>model: gpt2-small</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_mlp_outLoad this SAE standard blocks.0.hook_mlp_out 24576 expected_average_only_in blocks.1.hook_mlp_outLoad this SAE standard blocks.1.hook_mlp_out 24576 expected_average_only_in blocks.2.hook_mlp_outLoad this SAE standard blocks.2.hook_mlp_out 24576 expected_average_only_in blocks.3.hook_mlp_outLoad this SAE standard blocks.3.hook_mlp_out 24576 expected_average_only_in blocks.4.hook_mlp_outLoad this SAE standard blocks.4.hook_mlp_out 24576 expected_average_only_in blocks.5.hook_mlp_outLoad this SAE standard blocks.5.hook_mlp_out 24576 expected_average_only_in blocks.6.hook_mlp_outLoad this SAE standard blocks.6.hook_mlp_out 24576 expected_average_only_in blocks.7.hook_mlp_outLoad this SAE standard blocks.7.hook_mlp_out 24576 expected_average_only_in blocks.8.hook_mlp_outLoad this SAE standard blocks.8.hook_mlp_out 24576 expected_average_only_in blocks.9.hook_mlp_outLoad this SAE standard blocks.9.hook_mlp_out 24576 expected_average_only_in blocks.10.hook_mlp_outLoad this SAE standard blocks.10.hook_mlp_out 24576 expected_average_only_in blocks.11.hook_mlp_outLoad this SAE standard blocks.11.hook_mlp_out 24576 expected_average_only_in"},{"location":"sae_table/#gpt2-small-res-jb","title":"gpt2-small-res-jb","text":"<ul> <li>Huggingface Repo: jbloom/GPT2-Small-SAEs-Reformatted</li> <li>model: gpt2-small</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> <li>Publication</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_resid_preLoad this SAE standard gpt2-small/0-res-jb blocks.0.hook_resid_pre 24576 none blocks.1.hook_resid_preLoad this SAE standard gpt2-small/1-res-jb blocks.1.hook_resid_pre 24576 none blocks.2.hook_resid_preLoad this SAE standard gpt2-small/2-res-jb blocks.2.hook_resid_pre 24576 none blocks.3.hook_resid_preLoad this SAE standard gpt2-small/3-res-jb blocks.3.hook_resid_pre 24576 none blocks.4.hook_resid_preLoad this SAE standard gpt2-small/4-res-jb blocks.4.hook_resid_pre 24576 none blocks.5.hook_resid_preLoad this SAE standard gpt2-small/5-res-jb blocks.5.hook_resid_pre 24576 none blocks.6.hook_resid_preLoad this SAE standard gpt2-small/6-res-jb blocks.6.hook_resid_pre 24576 none blocks.7.hook_resid_preLoad this SAE standard gpt2-small/7-res-jb blocks.7.hook_resid_pre 24576 none blocks.8.hook_resid_preLoad this SAE standard gpt2-small/8-res-jb blocks.8.hook_resid_pre 24576 none blocks.9.hook_resid_preLoad this SAE standard gpt2-small/9-res-jb blocks.9.hook_resid_pre 24576 none blocks.10.hook_resid_preLoad this SAE standard gpt2-small/10-res-jb blocks.10.hook_resid_pre 24576 none blocks.11.hook_resid_preLoad this SAE standard gpt2-small/11-res-jb blocks.11.hook_resid_pre 24576 none blocks.11.hook_resid_postLoad this SAE standard gpt2-small/12-res-jb blocks.11.hook_resid_post 24576 none"},{"location":"sae_table/#gpt2-small-res-jb-feature-splitting","title":"gpt2-small-res-jb-feature-splitting","text":"<ul> <li>Huggingface Repo: jbloom/GPT2-Small-Feature-Splitting-Experiment-Layer-8</li> <li>model: gpt2-small</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.8.hook_resid_pre_768Load this SAE standard gpt2-small/8-res_fs768-jb blocks.8.hook_resid_pre 768 none blocks.8.hook_resid_pre_1536Load this SAE standard gpt2-small/8-res_fs1536-jb blocks.8.hook_resid_pre 1536 none blocks.8.hook_resid_pre_3072Load this SAE standard gpt2-small/8-res_fs3072-jb blocks.8.hook_resid_pre 3072 none blocks.8.hook_resid_pre_6144Load this SAE standard gpt2-small/8-res_fs6144-jb blocks.8.hook_resid_pre 6144 none blocks.8.hook_resid_pre_12288Load this SAE standard gpt2-small/8-res_fs12288-jb blocks.8.hook_resid_pre 12288 none blocks.8.hook_resid_pre_24576Load this SAE standard gpt2-small/8-res_fs24576-jb blocks.8.hook_resid_pre 24576 none blocks.8.hook_resid_pre_49152Load this SAE standard gpt2-small/8-res_fs49152-jb blocks.8.hook_resid_pre 49152 none blocks.8.hook_resid_pre_98304Load this SAE standard gpt2-small/8-res_fs98304-jb blocks.8.hook_resid_pre 98304 none"},{"location":"sae_table/#gpt2-small-res_sce-ajt","title":"gpt2-small-res_sce-ajt","text":"<ul> <li>Huggingface Repo: neuronpedia/gpt2-small__res_sce-ajt</li> <li>model: gpt2-small</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.2.hook_resid_preLoad this SAE standard gpt2-small/2-res_sce-ajt blocks.2.hook_resid_pre 46080 none blocks.6.hook_resid_preLoad this SAE standard gpt2-small/6-res_sce-ajt blocks.6.hook_resid_pre 46080 none blocks.10.hook_resid_preLoad this SAE standard gpt2-small/10-res_sce-ajt blocks.10.hook_resid_pre 46080 none"},{"location":"sae_table/#gpt2-small-res_scefr-ajt","title":"gpt2-small-res_scefr-ajt","text":"<ul> <li>Huggingface Repo: neuronpedia/gpt2-small__res_scefr-ajt</li> <li>model: gpt2-small</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.2.hook_resid_preLoad this SAE standard gpt2-small/2-res_scefr-ajt blocks.2.hook_resid_pre 46080 none blocks.6.hook_resid_preLoad this SAE standard gpt2-small/6-res_scefr-ajt blocks.6.hook_resid_pre 46080 none blocks.10.hook_resid_preLoad this SAE standard gpt2-small/10-res_scefr-ajt blocks.10.hook_resid_pre 46080 none"},{"location":"sae_table/#gpt2-small-res_scl-ajt","title":"gpt2-small-res_scl-ajt","text":"<ul> <li>Huggingface Repo: neuronpedia/gpt2-small__res_scl-ajt</li> <li>model: gpt2-small</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.2.hook_resid_preLoad this SAE standard gpt2-small/2-res_scl-ajt blocks.2.hook_resid_pre 46080 none blocks.6.hook_resid_preLoad this SAE standard gpt2-small/6-res_scl-ajt blocks.6.hook_resid_pre 46080 none blocks.10.hook_resid_preLoad this SAE standard gpt2-small/10-res_scl-ajt blocks.10.hook_resid_pre 46080 none"},{"location":"sae_table/#gpt2-small-res_sle-ajt","title":"gpt2-small-res_sle-ajt","text":"<ul> <li>Huggingface Repo: neuronpedia/gpt2-small__res_sle-ajt</li> <li>model: gpt2-small</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.2.hook_resid_preLoad this SAE standard gpt2-small/2-res_sle-ajt blocks.2.hook_resid_pre 46080 none blocks.6.hook_resid_preLoad this SAE standard gpt2-small/6-res_sle-ajt blocks.6.hook_resid_pre 46080 none blocks.10.hook_resid_preLoad this SAE standard gpt2-small/10-res_sle-ajt blocks.10.hook_resid_pre 46080 none"},{"location":"sae_table/#gpt2-small-res_slefr-ajt","title":"gpt2-small-res_slefr-ajt","text":"<ul> <li>Huggingface Repo: neuronpedia/gpt2-small__res_slefr-ajt</li> <li>model: gpt2-small</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.2.hook_resid_preLoad this SAE standard gpt2-small/2-res_slefr-ajt blocks.2.hook_resid_pre 46080 none blocks.6.hook_resid_preLoad this SAE standard gpt2-small/6-res_slefr-ajt blocks.6.hook_resid_pre 46080 none blocks.10.hook_resid_preLoad this SAE standard gpt2-small/10-res_slefr-ajt blocks.10.hook_resid_pre 46080 none"},{"location":"sae_table/#gpt2-small-res_sll-ajt","title":"gpt2-small-res_sll-ajt","text":"<ul> <li>Huggingface Repo: neuronpedia/gpt2-small__res_sll-ajt</li> <li>model: gpt2-small</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.2.hook_resid_preLoad this SAE standard gpt2-small/2-res_sll-ajt blocks.2.hook_resid_pre 46080 none blocks.6.hook_resid_preLoad this SAE standard gpt2-small/6-res_sll-ajt blocks.6.hook_resid_pre 46080 none blocks.10.hook_resid_preLoad this SAE standard gpt2-small/10-res_sll-ajt blocks.10.hook_resid_pre 46080 none"},{"location":"sae_table/#gpt2-small-resid-mid-v5-128k","title":"gpt2-small-resid-mid-v5-128k","text":"<ul> <li>Huggingface Repo: jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs</li> <li>model: gpt2-small</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_resid_midLoad this SAE standard gpt2-small/0-res_mid_128k-oai blocks.0.hook_resid_mid 131072 layer_norm blocks.1.hook_resid_midLoad this SAE standard gpt2-small/1-res_mid_128k-oai blocks.1.hook_resid_mid 131072 layer_norm blocks.2.hook_resid_midLoad this SAE standard gpt2-small/2-res_mid_128k-oai blocks.2.hook_resid_mid 131072 layer_norm blocks.3.hook_resid_midLoad this SAE standard gpt2-small/3-res_mid_128k-oai blocks.3.hook_resid_mid 131072 layer_norm blocks.4.hook_resid_midLoad this SAE standard gpt2-small/4-res_mid_128k-oai blocks.4.hook_resid_mid 131072 layer_norm blocks.5.hook_resid_midLoad this SAE standard gpt2-small/5-res_mid_128k-oai blocks.5.hook_resid_mid 131072 layer_norm blocks.6.hook_resid_midLoad this SAE standard gpt2-small/6-res_mid_128k-oai blocks.6.hook_resid_mid 131072 layer_norm blocks.7.hook_resid_midLoad this SAE standard gpt2-small/7-res_mid_128k-oai blocks.7.hook_resid_mid 131072 layer_norm blocks.8.hook_resid_midLoad this SAE standard gpt2-small/8-res_mid_128k-oai blocks.8.hook_resid_mid 131072 layer_norm blocks.9.hook_resid_midLoad this SAE standard gpt2-small/9-res_mid_128k-oai blocks.9.hook_resid_mid 131072 layer_norm blocks.10.hook_resid_midLoad this SAE standard gpt2-small/10-res_mid_128k-oai blocks.10.hook_resid_mid 131072 layer_norm blocks.11.hook_resid_midLoad this SAE standard gpt2-small/11-res_mid_128k-oai blocks.11.hook_resid_mid 131072 layer_norm"},{"location":"sae_table/#gpt2-small-resid-mid-v5-32k","title":"gpt2-small-resid-mid-v5-32k","text":"<ul> <li>Huggingface Repo: jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs</li> <li>model: gpt2-small</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_resid_midLoad this SAE standard gpt2-small/0-res_mid_32k-oai blocks.0.hook_resid_mid 32768 layer_norm blocks.1.hook_resid_midLoad this SAE standard gpt2-small/1-res_mid_32k-oai blocks.1.hook_resid_mid 32768 layer_norm blocks.2.hook_resid_midLoad this SAE standard gpt2-small/2-res_mid_32k-oai blocks.2.hook_resid_mid 32768 layer_norm blocks.3.hook_resid_midLoad this SAE standard gpt2-small/3-res_mid_32k-oai blocks.3.hook_resid_mid 32768 layer_norm blocks.4.hook_resid_midLoad this SAE standard gpt2-small/4-res_mid_32k-oai blocks.4.hook_resid_mid 32768 layer_norm blocks.5.hook_resid_midLoad this SAE standard gpt2-small/5-res_mid_32k-oai blocks.5.hook_resid_mid 32768 layer_norm blocks.6.hook_resid_midLoad this SAE standard gpt2-small/6-res_mid_32k-oai blocks.6.hook_resid_mid 32768 layer_norm blocks.7.hook_resid_midLoad this SAE standard gpt2-small/7-res_mid_32k-oai blocks.7.hook_resid_mid 32768 layer_norm blocks.8.hook_resid_midLoad this SAE standard gpt2-small/8-res_mid_32k-oai blocks.8.hook_resid_mid 32768 layer_norm blocks.9.hook_resid_midLoad this SAE standard gpt2-small/9-res_mid_32k-oai blocks.9.hook_resid_mid 32768 layer_norm blocks.10.hook_resid_midLoad this SAE standard gpt2-small/10-res_mid_32k-oai blocks.10.hook_resid_mid 32768 layer_norm blocks.11.hook_resid_midLoad this SAE standard gpt2-small/11-res_mid_32k-oai blocks.11.hook_resid_mid 32768 layer_norm"},{"location":"sae_table/#gpt2-small-resid-post-v5-128k","title":"gpt2-small-resid-post-v5-128k","text":"<ul> <li>Huggingface Repo: jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs</li> <li>model: gpt2-small</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_resid_postLoad this SAE standard gpt2-small/0-res_post_128k-oai blocks.0.hook_resid_post 131072 layer_norm blocks.1.hook_resid_postLoad this SAE standard gpt2-small/1-res_post_128k-oai blocks.1.hook_resid_post 131072 layer_norm blocks.2.hook_resid_postLoad this SAE standard gpt2-small/2-res_post_128k-oai blocks.2.hook_resid_post 131072 layer_norm blocks.3.hook_resid_postLoad this SAE standard gpt2-small/3-res_post_128k-oai blocks.3.hook_resid_post 131072 layer_norm blocks.4.hook_resid_postLoad this SAE standard gpt2-small/4-res_post_128k-oai blocks.4.hook_resid_post 131072 layer_norm blocks.5.hook_resid_postLoad this SAE standard gpt2-small/5-res_post_128k-oai blocks.5.hook_resid_post 131072 layer_norm blocks.6.hook_resid_postLoad this SAE standard gpt2-small/6-res_post_128k-oai blocks.6.hook_resid_post 131072 layer_norm blocks.7.hook_resid_postLoad this SAE standard gpt2-small/7-res_post_128k-oai blocks.7.hook_resid_post 131072 layer_norm blocks.8.hook_resid_postLoad this SAE standard gpt2-small/8-res_post_128k-oai blocks.8.hook_resid_post 131072 layer_norm blocks.9.hook_resid_postLoad this SAE standard gpt2-small/9-res_post_128k-oai blocks.9.hook_resid_post 131072 layer_norm blocks.10.hook_resid_postLoad this SAE standard gpt2-small/10-res_post_128k-oai blocks.10.hook_resid_post 131072 layer_norm blocks.11.hook_resid_postLoad this SAE standard gpt2-small/11-res_post_128k-oai blocks.11.hook_resid_post 131072 layer_norm"},{"location":"sae_table/#gpt2-small-resid-post-v5-32k","title":"gpt2-small-resid-post-v5-32k","text":"<ul> <li>Huggingface Repo: jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs</li> <li>model: gpt2-small</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_resid_postLoad this SAE standard gpt2-small/0-res_post_32k-oai blocks.0.hook_resid_post 32768 layer_norm blocks.1.hook_resid_postLoad this SAE standard gpt2-small/1-res_post_32k-oai blocks.1.hook_resid_post 32768 layer_norm blocks.2.hook_resid_postLoad this SAE standard gpt2-small/2-res_post_32k-oai blocks.2.hook_resid_post 32768 layer_norm blocks.3.hook_resid_postLoad this SAE standard gpt2-small/3-res_post_32k-oai blocks.3.hook_resid_post 32768 layer_norm blocks.4.hook_resid_postLoad this SAE standard gpt2-small/4-res_post_32k-oai blocks.4.hook_resid_post 32768 layer_norm blocks.5.hook_resid_postLoad this SAE standard gpt2-small/5-res_post_32k-oai blocks.5.hook_resid_post 32768 layer_norm blocks.6.hook_resid_postLoad this SAE standard gpt2-small/6-res_post_32k-oai blocks.6.hook_resid_post 32768 layer_norm blocks.7.hook_resid_postLoad this SAE standard gpt2-small/7-res_post_32k-oai blocks.7.hook_resid_post 32768 layer_norm blocks.8.hook_resid_postLoad this SAE standard gpt2-small/8-res_post_32k-oai blocks.8.hook_resid_post 32768 layer_norm blocks.9.hook_resid_postLoad this SAE standard gpt2-small/9-res_post_32k-oai blocks.9.hook_resid_post 32768 layer_norm blocks.10.hook_resid_postLoad this SAE standard gpt2-small/10-res_post_32k-oai blocks.10.hook_resid_post 32768 layer_norm blocks.11.hook_resid_postLoad this SAE standard gpt2-small/11-res_post_32k-oai blocks.11.hook_resid_post 32768 layer_norm"},{"location":"sae_table/#llama-3-8b-it-res-jh","title":"llama-3-8b-it-res-jh","text":"<ul> <li>Huggingface Repo: Juliushanhanhan/llama-3-8b-it-res</li> <li>model: meta-llama/Meta-Llama-3-8B-Instruct</li> <li>Additional Links:<ul> <li>Dashboards</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.25.hook_resid_postLoad this SAE gated llama3-8b-it/25-res-jh blocks.25.hook_resid_post 65536 expected_average_only_in"},{"location":"sae_table/#llama_scope_lxa_32x","title":"llama_scope_lxa_32x","text":"<ul> <li>Huggingface Repo: fnlp/Llama3_1-8B-Base-LXA-32x</li> <li>model: meta-llama/Llama-3.1-8B</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations l0a_32xLoad this SAE jumprelu llama3.1-8b/0-llamascope-att-131k blocks.0.hook_attn_out 131072 expected_average_only_in l1a_32xLoad this SAE jumprelu llama3.1-8b/1-llamascope-att-131k blocks.1.hook_attn_out 131072 expected_average_only_in l2a_32xLoad this SAE jumprelu llama3.1-8b/2-llamascope-att-131k blocks.2.hook_attn_out 131072 expected_average_only_in l3a_32xLoad this SAE jumprelu llama3.1-8b/3-llamascope-att-131k blocks.3.hook_attn_out 131072 expected_average_only_in l4a_32xLoad this SAE jumprelu llama3.1-8b/4-llamascope-att-131k blocks.4.hook_attn_out 131072 expected_average_only_in l5a_32xLoad this SAE jumprelu llama3.1-8b/5-llamascope-att-131k blocks.5.hook_attn_out 131072 expected_average_only_in l6a_32xLoad this SAE jumprelu llama3.1-8b/6-llamascope-att-131k blocks.6.hook_attn_out 131072 expected_average_only_in l7a_32xLoad this SAE jumprelu llama3.1-8b/7-llamascope-att-131k blocks.7.hook_attn_out 131072 expected_average_only_in l8a_32xLoad this SAE jumprelu llama3.1-8b/8-llamascope-att-131k blocks.8.hook_attn_out 131072 expected_average_only_in l9a_32xLoad this SAE jumprelu llama3.1-8b/9-llamascope-att-131k blocks.9.hook_attn_out 131072 expected_average_only_in l10a_32xLoad this SAE jumprelu llama3.1-8b/10-llamascope-att-131k blocks.10.hook_attn_out 131072 expected_average_only_in l11a_32xLoad this SAE jumprelu llama3.1-8b/11-llamascope-att-131k blocks.11.hook_attn_out 131072 expected_average_only_in l12a_32xLoad this SAE jumprelu llama3.1-8b/12-llamascope-att-131k blocks.12.hook_attn_out 131072 expected_average_only_in l13a_32xLoad this SAE jumprelu llama3.1-8b/13-llamascope-att-131k blocks.13.hook_attn_out 131072 expected_average_only_in l14a_32xLoad this SAE jumprelu llama3.1-8b/14-llamascope-att-131k blocks.14.hook_attn_out 131072 expected_average_only_in l15a_32xLoad this SAE jumprelu llama3.1-8b/15-llamascope-att-131k blocks.15.hook_attn_out 131072 expected_average_only_in l16a_32xLoad this SAE jumprelu llama3.1-8b/16-llamascope-att-131k blocks.16.hook_attn_out 131072 expected_average_only_in l17a_32xLoad this SAE jumprelu llama3.1-8b/17-llamascope-att-131k blocks.17.hook_attn_out 131072 expected_average_only_in l18a_32xLoad this SAE jumprelu llama3.1-8b/18-llamascope-att-131k blocks.18.hook_attn_out 131072 expected_average_only_in l19a_32xLoad this SAE jumprelu llama3.1-8b/19-llamascope-att-131k blocks.19.hook_attn_out 131072 expected_average_only_in l20a_32xLoad this SAE jumprelu llama3.1-8b/20-llamascope-att-131k blocks.20.hook_attn_out 131072 expected_average_only_in l21a_32xLoad this SAE jumprelu llama3.1-8b/21-llamascope-att-131k blocks.21.hook_attn_out 131072 expected_average_only_in l22a_32xLoad this SAE jumprelu llama3.1-8b/22-llamascope-att-131k blocks.22.hook_attn_out 131072 expected_average_only_in l23a_32xLoad this SAE jumprelu llama3.1-8b/23-llamascope-att-131k blocks.23.hook_attn_out 131072 expected_average_only_in l24a_32xLoad this SAE jumprelu llama3.1-8b/24-llamascope-att-131k blocks.24.hook_attn_out 131072 expected_average_only_in l25a_32xLoad this SAE jumprelu llama3.1-8b/25-llamascope-att-131k blocks.25.hook_attn_out 131072 expected_average_only_in l26a_32xLoad this SAE jumprelu llama3.1-8b/26-llamascope-att-131k blocks.26.hook_attn_out 131072 expected_average_only_in l27a_32xLoad this SAE jumprelu llama3.1-8b/27-llamascope-att-131k blocks.27.hook_attn_out 131072 expected_average_only_in l28a_32xLoad this SAE jumprelu llama3.1-8b/28-llamascope-att-131k blocks.28.hook_attn_out 131072 expected_average_only_in l29a_32xLoad this SAE jumprelu llama3.1-8b/29-llamascope-att-131k blocks.29.hook_attn_out 131072 expected_average_only_in l30a_32xLoad this SAE jumprelu llama3.1-8b/30-llamascope-att-131k blocks.30.hook_attn_out 131072 expected_average_only_in l31a_32xLoad this SAE jumprelu llama3.1-8b/31-llamascope-att-131k blocks.31.hook_attn_out 131072 expected_average_only_in"},{"location":"sae_table/#llama_scope_lxa_8x","title":"llama_scope_lxa_8x","text":"<ul> <li>Huggingface Repo: fnlp/Llama3_1-8B-Base-LXA-8x</li> <li>model: meta-llama/Llama-3.1-8B</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations l0a_8xLoad this SAE jumprelu llama3.1-8b/0-llamascope-att-32k blocks.0.hook_attn_out 32768 expected_average_only_in l1a_8xLoad this SAE jumprelu llama3.1-8b/1-llamascope-att-32k blocks.1.hook_attn_out 32768 expected_average_only_in l2a_8xLoad this SAE jumprelu llama3.1-8b/2-llamascope-att-32k blocks.2.hook_attn_out 32768 expected_average_only_in l3a_8xLoad this SAE jumprelu llama3.1-8b/3-llamascope-att-32k blocks.3.hook_attn_out 32768 expected_average_only_in l4a_8xLoad this SAE jumprelu llama3.1-8b/4-llamascope-att-32k blocks.4.hook_attn_out 32768 expected_average_only_in l5a_8xLoad this SAE jumprelu llama3.1-8b/5-llamascope-att-32k blocks.5.hook_attn_out 32768 expected_average_only_in l6a_8xLoad this SAE jumprelu llama3.1-8b/6-llamascope-att-32k blocks.6.hook_attn_out 32768 expected_average_only_in l7a_8xLoad this SAE jumprelu llama3.1-8b/7-llamascope-att-32k blocks.7.hook_attn_out 32768 expected_average_only_in l8a_8xLoad this SAE jumprelu llama3.1-8b/8-llamascope-att-32k blocks.8.hook_attn_out 32768 expected_average_only_in l9a_8xLoad this SAE jumprelu llama3.1-8b/9-llamascope-att-32k blocks.9.hook_attn_out 32768 expected_average_only_in l10a_8xLoad this SAE jumprelu llama3.1-8b/10-llamascope-att-32k blocks.10.hook_attn_out 32768 expected_average_only_in l11a_8xLoad this SAE jumprelu llama3.1-8b/11-llamascope-att-32k blocks.11.hook_attn_out 32768 expected_average_only_in l12a_8xLoad this SAE jumprelu llama3.1-8b/12-llamascope-att-32k blocks.12.hook_attn_out 32768 expected_average_only_in l13a_8xLoad this SAE jumprelu llama3.1-8b/13-llamascope-att-32k blocks.13.hook_attn_out 32768 expected_average_only_in l14a_8xLoad this SAE jumprelu llama3.1-8b/14-llamascope-att-32k blocks.14.hook_attn_out 32768 expected_average_only_in l15a_8xLoad this SAE jumprelu llama3.1-8b/15-llamascope-att-32k blocks.15.hook_attn_out 32768 expected_average_only_in l16a_8xLoad this SAE jumprelu llama3.1-8b/16-llamascope-att-32k blocks.16.hook_attn_out 32768 expected_average_only_in l17a_8xLoad this SAE jumprelu llama3.1-8b/17-llamascope-att-32k blocks.17.hook_attn_out 32768 expected_average_only_in l18a_8xLoad this SAE jumprelu llama3.1-8b/18-llamascope-att-32k blocks.18.hook_attn_out 32768 expected_average_only_in l19a_8xLoad this SAE jumprelu llama3.1-8b/19-llamascope-att-32k blocks.19.hook_attn_out 32768 expected_average_only_in l20a_8xLoad this SAE jumprelu llama3.1-8b/20-llamascope-att-32k blocks.20.hook_attn_out 32768 expected_average_only_in l21a_8xLoad this SAE jumprelu llama3.1-8b/21-llamascope-att-32k blocks.21.hook_attn_out 32768 expected_average_only_in l22a_8xLoad this SAE jumprelu llama3.1-8b/22-llamascope-att-32k blocks.22.hook_attn_out 32768 expected_average_only_in l23a_8xLoad this SAE jumprelu llama3.1-8b/23-llamascope-att-32k blocks.23.hook_attn_out 32768 expected_average_only_in l24a_8xLoad this SAE jumprelu llama3.1-8b/24-llamascope-att-32k blocks.24.hook_attn_out 32768 expected_average_only_in l25a_8xLoad this SAE jumprelu llama3.1-8b/25-llamascope-att-32k blocks.25.hook_attn_out 32768 expected_average_only_in l26a_8xLoad this SAE jumprelu llama3.1-8b/26-llamascope-att-32k blocks.26.hook_attn_out 32768 expected_average_only_in l27a_8xLoad this SAE jumprelu llama3.1-8b/27-llamascope-att-32k blocks.27.hook_attn_out 32768 expected_average_only_in l28a_8xLoad this SAE jumprelu llama3.1-8b/28-llamascope-att-32k blocks.28.hook_attn_out 32768 expected_average_only_in l29a_8xLoad this SAE jumprelu llama3.1-8b/29-llamascope-att-32k blocks.29.hook_attn_out 32768 expected_average_only_in l30a_8xLoad this SAE jumprelu llama3.1-8b/30-llamascope-att-32k blocks.30.hook_attn_out 32768 expected_average_only_in l31a_8xLoad this SAE jumprelu llama3.1-8b/31-llamascope-att-32k blocks.31.hook_attn_out 32768 expected_average_only_in"},{"location":"sae_table/#llama_scope_lxm_32x","title":"llama_scope_lxm_32x","text":"<ul> <li>Huggingface Repo: fnlp/Llama3_1-8B-Base-LXM-32x</li> <li>model: meta-llama/Llama-3.1-8B</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations l0m_32xLoad this SAE jumprelu llama3.1-8b/0-llamascope-mlp-131k blocks.0.hook_mlp_out 131072 expected_average_only_in l1m_32xLoad this SAE jumprelu llama3.1-8b/1-llamascope-mlp-131k blocks.1.hook_mlp_out 131072 expected_average_only_in l2m_32xLoad this SAE jumprelu llama3.1-8b/2-llamascope-mlp-131k blocks.2.hook_mlp_out 131072 expected_average_only_in l3m_32xLoad this SAE jumprelu llama3.1-8b/3-llamascope-mlp-131k blocks.3.hook_mlp_out 131072 expected_average_only_in l4m_32xLoad this SAE jumprelu llama3.1-8b/4-llamascope-mlp-131k blocks.4.hook_mlp_out 131072 expected_average_only_in l5m_32xLoad this SAE jumprelu llama3.1-8b/5-llamascope-mlp-131k blocks.5.hook_mlp_out 131072 expected_average_only_in l6m_32xLoad this SAE jumprelu llama3.1-8b/6-llamascope-mlp-131k blocks.6.hook_mlp_out 131072 expected_average_only_in l7m_32xLoad this SAE jumprelu llama3.1-8b/7-llamascope-mlp-131k blocks.7.hook_mlp_out 131072 expected_average_only_in l8m_32xLoad this SAE jumprelu llama3.1-8b/8-llamascope-mlp-131k blocks.8.hook_mlp_out 131072 expected_average_only_in l9m_32xLoad this SAE jumprelu llama3.1-8b/9-llamascope-mlp-131k blocks.9.hook_mlp_out 131072 expected_average_only_in l10m_32xLoad this SAE jumprelu llama3.1-8b/10-llamascope-mlp-131k blocks.10.hook_mlp_out 131072 expected_average_only_in l11m_32xLoad this SAE jumprelu llama3.1-8b/11-llamascope-mlp-131k blocks.11.hook_mlp_out 131072 expected_average_only_in l12m_32xLoad this SAE jumprelu llama3.1-8b/12-llamascope-mlp-131k blocks.12.hook_mlp_out 131072 expected_average_only_in l13m_32xLoad this SAE jumprelu llama3.1-8b/13-llamascope-mlp-131k blocks.13.hook_mlp_out 131072 expected_average_only_in l14m_32xLoad this SAE jumprelu llama3.1-8b/14-llamascope-mlp-131k blocks.14.hook_mlp_out 131072 expected_average_only_in l15m_32xLoad this SAE jumprelu llama3.1-8b/15-llamascope-mlp-131k blocks.15.hook_mlp_out 131072 expected_average_only_in l16m_32xLoad this SAE jumprelu llama3.1-8b/16-llamascope-mlp-131k blocks.16.hook_mlp_out 131072 expected_average_only_in l17m_32xLoad this SAE jumprelu llama3.1-8b/17-llamascope-mlp-131k blocks.17.hook_mlp_out 131072 expected_average_only_in l18m_32xLoad this SAE jumprelu llama3.1-8b/18-llamascope-mlp-131k blocks.18.hook_mlp_out 131072 expected_average_only_in l19m_32xLoad this SAE jumprelu llama3.1-8b/19-llamascope-mlp-131k blocks.19.hook_mlp_out 131072 expected_average_only_in l20m_32xLoad this SAE jumprelu llama3.1-8b/20-llamascope-mlp-131k blocks.20.hook_mlp_out 131072 expected_average_only_in l21m_32xLoad this SAE jumprelu llama3.1-8b/21-llamascope-mlp-131k blocks.21.hook_mlp_out 131072 expected_average_only_in l22m_32xLoad this SAE jumprelu llama3.1-8b/22-llamascope-mlp-131k blocks.22.hook_mlp_out 131072 expected_average_only_in l23m_32xLoad this SAE jumprelu llama3.1-8b/23-llamascope-mlp-131k blocks.23.hook_mlp_out 131072 expected_average_only_in l24m_32xLoad this SAE jumprelu llama3.1-8b/24-llamascope-mlp-131k blocks.24.hook_mlp_out 131072 expected_average_only_in l25m_32xLoad this SAE jumprelu llama3.1-8b/25-llamascope-mlp-131k blocks.25.hook_mlp_out 131072 expected_average_only_in l26m_32xLoad this SAE jumprelu llama3.1-8b/26-llamascope-mlp-131k blocks.26.hook_mlp_out 131072 expected_average_only_in l27m_32xLoad this SAE jumprelu llama3.1-8b/27-llamascope-mlp-131k blocks.27.hook_mlp_out 131072 expected_average_only_in l28m_32xLoad this SAE jumprelu llama3.1-8b/28-llamascope-mlp-131k blocks.28.hook_mlp_out 131072 expected_average_only_in l29m_32xLoad this SAE jumprelu llama3.1-8b/29-llamascope-mlp-131k blocks.29.hook_mlp_out 131072 expected_average_only_in l30m_32xLoad this SAE jumprelu llama3.1-8b/30-llamascope-mlp-131k blocks.30.hook_mlp_out 131072 expected_average_only_in l31m_32xLoad this SAE jumprelu llama3.1-8b/31-llamascope-mlp-131k blocks.31.hook_mlp_out 131072 expected_average_only_in"},{"location":"sae_table/#llama_scope_lxm_8x","title":"llama_scope_lxm_8x","text":"<ul> <li>Huggingface Repo: fnlp/Llama3_1-8B-Base-LXM-8x</li> <li>model: meta-llama/Llama-3.1-8B</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations l0m_8xLoad this SAE jumprelu llama3.1-8b/0-llamascope-mlp-32k blocks.0.hook_mlp_out 32768 expected_average_only_in l1m_8xLoad this SAE jumprelu llama3.1-8b/1-llamascope-mlp-32k blocks.1.hook_mlp_out 32768 expected_average_only_in l2m_8xLoad this SAE jumprelu llama3.1-8b/2-llamascope-mlp-32k blocks.2.hook_mlp_out 32768 expected_average_only_in l3m_8xLoad this SAE jumprelu llama3.1-8b/3-llamascope-mlp-32k blocks.3.hook_mlp_out 32768 expected_average_only_in l4m_8xLoad this SAE jumprelu llama3.1-8b/4-llamascope-mlp-32k blocks.4.hook_mlp_out 32768 expected_average_only_in l5m_8xLoad this SAE jumprelu llama3.1-8b/5-llamascope-mlp-32k blocks.5.hook_mlp_out 32768 expected_average_only_in l6m_8xLoad this SAE jumprelu llama3.1-8b/6-llamascope-mlp-32k blocks.6.hook_mlp_out 32768 expected_average_only_in l7m_8xLoad this SAE jumprelu llama3.1-8b/7-llamascope-mlp-32k blocks.7.hook_mlp_out 32768 expected_average_only_in l8m_8xLoad this SAE jumprelu llama3.1-8b/8-llamascope-mlp-32k blocks.8.hook_mlp_out 32768 expected_average_only_in l9m_8xLoad this SAE jumprelu llama3.1-8b/9-llamascope-mlp-32k blocks.9.hook_mlp_out 32768 expected_average_only_in l10m_8xLoad this SAE jumprelu llama3.1-8b/10-llamascope-mlp-32k blocks.10.hook_mlp_out 32768 expected_average_only_in l11m_8xLoad this SAE jumprelu llama3.1-8b/11-llamascope-mlp-32k blocks.11.hook_mlp_out 32768 expected_average_only_in l12m_8xLoad this SAE jumprelu llama3.1-8b/12-llamascope-mlp-32k blocks.12.hook_mlp_out 32768 expected_average_only_in l13m_8xLoad this SAE jumprelu llama3.1-8b/13-llamascope-mlp-32k blocks.13.hook_mlp_out 32768 expected_average_only_in l14m_8xLoad this SAE jumprelu llama3.1-8b/14-llamascope-mlp-32k blocks.14.hook_mlp_out 32768 expected_average_only_in l15m_8xLoad this SAE jumprelu llama3.1-8b/15-llamascope-mlp-32k blocks.15.hook_mlp_out 32768 expected_average_only_in l16m_8xLoad this SAE jumprelu llama3.1-8b/16-llamascope-mlp-32k blocks.16.hook_mlp_out 32768 expected_average_only_in l17m_8xLoad this SAE jumprelu llama3.1-8b/17-llamascope-mlp-32k blocks.17.hook_mlp_out 32768 expected_average_only_in l18m_8xLoad this SAE jumprelu llama3.1-8b/18-llamascope-mlp-32k blocks.18.hook_mlp_out 32768 expected_average_only_in l19m_8xLoad this SAE jumprelu llama3.1-8b/19-llamascope-mlp-32k blocks.19.hook_mlp_out 32768 expected_average_only_in l20m_8xLoad this SAE jumprelu llama3.1-8b/20-llamascope-mlp-32k blocks.20.hook_mlp_out 32768 expected_average_only_in l21m_8xLoad this SAE jumprelu llama3.1-8b/21-llamascope-mlp-32k blocks.21.hook_mlp_out 32768 expected_average_only_in l22m_8xLoad this SAE jumprelu llama3.1-8b/22-llamascope-mlp-32k blocks.22.hook_mlp_out 32768 expected_average_only_in l23m_8xLoad this SAE jumprelu llama3.1-8b/23-llamascope-mlp-32k blocks.23.hook_mlp_out 32768 expected_average_only_in l24m_8xLoad this SAE jumprelu llama3.1-8b/24-llamascope-mlp-32k blocks.24.hook_mlp_out 32768 expected_average_only_in l25m_8xLoad this SAE jumprelu llama3.1-8b/25-llamascope-mlp-32k blocks.25.hook_mlp_out 32768 expected_average_only_in l26m_8xLoad this SAE jumprelu llama3.1-8b/26-llamascope-mlp-32k blocks.26.hook_mlp_out 32768 expected_average_only_in l27m_8xLoad this SAE jumprelu llama3.1-8b/27-llamascope-mlp-32k blocks.27.hook_mlp_out 32768 expected_average_only_in l28m_8xLoad this SAE jumprelu llama3.1-8b/28-llamascope-mlp-32k blocks.28.hook_mlp_out 32768 expected_average_only_in l29m_8xLoad this SAE jumprelu llama3.1-8b/29-llamascope-mlp-32k blocks.29.hook_mlp_out 32768 expected_average_only_in l30m_8xLoad this SAE jumprelu llama3.1-8b/30-llamascope-mlp-32k blocks.30.hook_mlp_out 32768 expected_average_only_in l31m_8xLoad this SAE jumprelu llama3.1-8b/31-llamascope-mlp-32k blocks.31.hook_mlp_out 32768 expected_average_only_in"},{"location":"sae_table/#llama_scope_lxr_32x","title":"llama_scope_lxr_32x","text":"<ul> <li>Huggingface Repo: fnlp/Llama3_1-8B-Base-LXR-32x</li> <li>model: meta-llama/Llama-3.1-8B</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations l0r_32xLoad this SAE jumprelu llama3.1-8b/0-llamascope-res-131k blocks.0.hook_resid_post 131072 expected_average_only_in l1r_32xLoad this SAE jumprelu llama3.1-8b/1-llamascope-res-131k blocks.1.hook_resid_post 131072 expected_average_only_in l2r_32xLoad this SAE jumprelu llama3.1-8b/2-llamascope-res-131k blocks.2.hook_resid_post 131072 expected_average_only_in l3r_32xLoad this SAE jumprelu llama3.1-8b/3-llamascope-res-131k blocks.3.hook_resid_post 131072 expected_average_only_in l4r_32xLoad this SAE jumprelu llama3.1-8b/4-llamascope-res-131k blocks.4.hook_resid_post 131072 expected_average_only_in l5r_32xLoad this SAE jumprelu llama3.1-8b/5-llamascope-res-131k blocks.5.hook_resid_post 131072 expected_average_only_in l6r_32xLoad this SAE jumprelu llama3.1-8b/6-llamascope-res-131k blocks.6.hook_resid_post 131072 expected_average_only_in l7r_32xLoad this SAE jumprelu llama3.1-8b/7-llamascope-res-131k blocks.7.hook_resid_post 131072 expected_average_only_in l8r_32xLoad this SAE jumprelu llama3.1-8b/8-llamascope-res-131k blocks.8.hook_resid_post 131072 expected_average_only_in l9r_32xLoad this SAE jumprelu llama3.1-8b/9-llamascope-res-131k blocks.9.hook_resid_post 131072 expected_average_only_in l10r_32xLoad this SAE jumprelu llama3.1-8b/10-llamascope-res-131k blocks.10.hook_resid_post 131072 expected_average_only_in l11r_32xLoad this SAE jumprelu llama3.1-8b/11-llamascope-res-131k blocks.11.hook_resid_post 131072 expected_average_only_in l12r_32xLoad this SAE jumprelu llama3.1-8b/12-llamascope-res-131k blocks.12.hook_resid_post 131072 expected_average_only_in l13r_32xLoad this SAE jumprelu llama3.1-8b/13-llamascope-res-131k blocks.13.hook_resid_post 131072 expected_average_only_in l14r_32xLoad this SAE jumprelu llama3.1-8b/14-llamascope-res-131k blocks.14.hook_resid_post 131072 expected_average_only_in l15r_32xLoad this SAE jumprelu llama3.1-8b/15-llamascope-res-131k blocks.15.hook_resid_post 131072 expected_average_only_in l16r_32xLoad this SAE jumprelu llama3.1-8b/16-llamascope-res-131k blocks.16.hook_resid_post 131072 expected_average_only_in l17r_32xLoad this SAE jumprelu llama3.1-8b/17-llamascope-res-131k blocks.17.hook_resid_post 131072 expected_average_only_in l18r_32xLoad this SAE jumprelu llama3.1-8b/18-llamascope-res-131k blocks.18.hook_resid_post 131072 expected_average_only_in l19r_32xLoad this SAE jumprelu llama3.1-8b/19-llamascope-res-131k blocks.19.hook_resid_post 131072 expected_average_only_in l20r_32xLoad this SAE jumprelu llama3.1-8b/20-llamascope-res-131k blocks.20.hook_resid_post 131072 expected_average_only_in l21r_32xLoad this SAE jumprelu llama3.1-8b/21-llamascope-res-131k blocks.21.hook_resid_post 131072 expected_average_only_in l22r_32xLoad this SAE jumprelu llama3.1-8b/22-llamascope-res-131k blocks.22.hook_resid_post 131072 expected_average_only_in l23r_32xLoad this SAE jumprelu llama3.1-8b/23-llamascope-res-131k blocks.23.hook_resid_post 131072 expected_average_only_in l24r_32xLoad this SAE jumprelu llama3.1-8b/24-llamascope-res-131k blocks.24.hook_resid_post 131072 expected_average_only_in l25r_32xLoad this SAE jumprelu llama3.1-8b/25-llamascope-res-131k blocks.25.hook_resid_post 131072 expected_average_only_in l26r_32xLoad this SAE jumprelu llama3.1-8b/26-llamascope-res-131k blocks.26.hook_resid_post 131072 expected_average_only_in l27r_32xLoad this SAE jumprelu llama3.1-8b/27-llamascope-res-131k blocks.27.hook_resid_post 131072 expected_average_only_in l28r_32xLoad this SAE jumprelu llama3.1-8b/28-llamascope-res-131k blocks.28.hook_resid_post 131072 expected_average_only_in l29r_32xLoad this SAE jumprelu llama3.1-8b/29-llamascope-res-131k blocks.29.hook_resid_post 131072 expected_average_only_in l30r_32xLoad this SAE jumprelu llama3.1-8b/30-llamascope-res-131k blocks.30.hook_resid_post 131072 expected_average_only_in l31r_32xLoad this SAE jumprelu llama3.1-8b/31-llamascope-res-131k blocks.31.hook_resid_post 131072 expected_average_only_in"},{"location":"sae_table/#llama_scope_lxr_8x","title":"llama_scope_lxr_8x","text":"<ul> <li>Huggingface Repo: fnlp/Llama3_1-8B-Base-LXR-8x</li> <li>model: meta-llama/Llama-3.1-8B</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations l0r_8xLoad this SAE jumprelu llama3.1-8b/0-llamascope-res-32k blocks.0.hook_resid_post 32768 expected_average_only_in l1r_8xLoad this SAE jumprelu llama3.1-8b/1-llamascope-res-32k blocks.1.hook_resid_post 32768 expected_average_only_in l2r_8xLoad this SAE jumprelu llama3.1-8b/2-llamascope-res-32k blocks.2.hook_resid_post 32768 expected_average_only_in l3r_8xLoad this SAE jumprelu llama3.1-8b/3-llamascope-res-32k blocks.3.hook_resid_post 32768 expected_average_only_in l4r_8xLoad this SAE jumprelu llama3.1-8b/4-llamascope-res-32k blocks.4.hook_resid_post 32768 expected_average_only_in l5r_8xLoad this SAE jumprelu llama3.1-8b/5-llamascope-res-32k blocks.5.hook_resid_post 32768 expected_average_only_in l6r_8xLoad this SAE jumprelu llama3.1-8b/6-llamascope-res-32k blocks.6.hook_resid_post 32768 expected_average_only_in l7r_8xLoad this SAE jumprelu llama3.1-8b/7-llamascope-res-32k blocks.7.hook_resid_post 32768 expected_average_only_in l8r_8xLoad this SAE jumprelu llama3.1-8b/8-llamascope-res-32k blocks.8.hook_resid_post 32768 expected_average_only_in l9r_8xLoad this SAE jumprelu llama3.1-8b/9-llamascope-res-32k blocks.9.hook_resid_post 32768 expected_average_only_in l10r_8xLoad this SAE jumprelu llama3.1-8b/10-llamascope-res-32k blocks.10.hook_resid_post 32768 expected_average_only_in l11r_8xLoad this SAE jumprelu llama3.1-8b/11-llamascope-res-32k blocks.11.hook_resid_post 32768 expected_average_only_in l12r_8xLoad this SAE jumprelu llama3.1-8b/12-llamascope-res-32k blocks.12.hook_resid_post 32768 expected_average_only_in l13r_8xLoad this SAE jumprelu llama3.1-8b/13-llamascope-res-32k blocks.13.hook_resid_post 32768 expected_average_only_in l14r_8xLoad this SAE jumprelu llama3.1-8b/14-llamascope-res-32k blocks.14.hook_resid_post 32768 expected_average_only_in l15r_8xLoad this SAE jumprelu llama3.1-8b/15-llamascope-res-32k blocks.15.hook_resid_post 32768 expected_average_only_in l16r_8xLoad this SAE jumprelu llama3.1-8b/16-llamascope-res-32k blocks.16.hook_resid_post 32768 expected_average_only_in l17r_8xLoad this SAE jumprelu llama3.1-8b/17-llamascope-res-32k blocks.17.hook_resid_post 32768 expected_average_only_in l18r_8xLoad this SAE jumprelu llama3.1-8b/18-llamascope-res-32k blocks.18.hook_resid_post 32768 expected_average_only_in l19r_8xLoad this SAE jumprelu llama3.1-8b/19-llamascope-res-32k blocks.19.hook_resid_post 32768 expected_average_only_in l20r_8xLoad this SAE jumprelu llama3.1-8b/20-llamascope-res-32k blocks.20.hook_resid_post 32768 expected_average_only_in l21r_8xLoad this SAE jumprelu llama3.1-8b/21-llamascope-res-32k blocks.21.hook_resid_post 32768 expected_average_only_in l22r_8xLoad this SAE jumprelu llama3.1-8b/22-llamascope-res-32k blocks.22.hook_resid_post 32768 expected_average_only_in l23r_8xLoad this SAE jumprelu llama3.1-8b/23-llamascope-res-32k blocks.23.hook_resid_post 32768 expected_average_only_in l24r_8xLoad this SAE jumprelu llama3.1-8b/24-llamascope-res-32k blocks.24.hook_resid_post 32768 expected_average_only_in l25r_8xLoad this SAE jumprelu llama3.1-8b/25-llamascope-res-32k blocks.25.hook_resid_post 32768 expected_average_only_in l26r_8xLoad this SAE jumprelu llama3.1-8b/26-llamascope-res-32k blocks.26.hook_resid_post 32768 expected_average_only_in l27r_8xLoad this SAE jumprelu llama3.1-8b/27-llamascope-res-32k blocks.27.hook_resid_post 32768 expected_average_only_in l28r_8xLoad this SAE jumprelu llama3.1-8b/28-llamascope-res-32k blocks.28.hook_resid_post 32768 expected_average_only_in l29r_8xLoad this SAE jumprelu llama3.1-8b/29-llamascope-res-32k blocks.29.hook_resid_post 32768 expected_average_only_in l30r_8xLoad this SAE jumprelu llama3.1-8b/30-llamascope-res-32k blocks.30.hook_resid_post 32768 expected_average_only_in l31r_8xLoad this SAE jumprelu llama3.1-8b/31-llamascope-res-32k blocks.31.hook_resid_post 32768 expected_average_only_in"},{"location":"sae_table/#llama_scope_r1_distill","title":"llama_scope_r1_distill","text":"<ul> <li>Huggingface Repo: fnlp/Llama-Scope-R1-Distill</li> <li>model: deepseek-ai/DeepSeek-R1-Distill-Llama-8B</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations l0r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/0-llamascope-slimpj-res-32k blocks.0.hook_resid_post 32768 expected_average_only_in l1r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/1-llamascope-slimpj-res-32k blocks.1.hook_resid_post 32768 expected_average_only_in l2r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/2-llamascope-slimpj-res-32k blocks.2.hook_resid_post 32768 expected_average_only_in l3r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/3-llamascope-slimpj-res-32k blocks.3.hook_resid_post 32768 expected_average_only_in l4r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/4-llamascope-slimpj-res-32k blocks.4.hook_resid_post 32768 expected_average_only_in l5r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/5-llamascope-slimpj-res-32k blocks.5.hook_resid_post 32768 expected_average_only_in l6r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/6-llamascope-slimpj-res-32k blocks.6.hook_resid_post 32768 expected_average_only_in l7r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/7-llamascope-slimpj-res-32k blocks.7.hook_resid_post 32768 expected_average_only_in l8r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/8-llamascope-slimpj-res-32k blocks.8.hook_resid_post 32768 expected_average_only_in l9r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/9-llamascope-slimpj-res-32k blocks.9.hook_resid_post 32768 expected_average_only_in l10r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/10-llamascope-slimpj-res-32k blocks.10.hook_resid_post 32768 expected_average_only_in l11r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/11-llamascope-slimpj-res-32k blocks.11.hook_resid_post 32768 expected_average_only_in l12r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/12-llamascope-slimpj-res-32k blocks.12.hook_resid_post 32768 expected_average_only_in l13r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/13-llamascope-slimpj-res-32k blocks.13.hook_resid_post 32768 expected_average_only_in l14r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/14-llamascope-slimpj-res-32k blocks.14.hook_resid_post 32768 expected_average_only_in l15r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/15-llamascope-slimpj-res-32k blocks.15.hook_resid_post 32768 expected_average_only_in l16r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/16-llamascope-slimpj-res-32k blocks.16.hook_resid_post 32768 expected_average_only_in l17r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/17-llamascope-slimpj-res-32k blocks.17.hook_resid_post 32768 expected_average_only_in l18r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/18-llamascope-slimpj-res-32k blocks.18.hook_resid_post 32768 expected_average_only_in l19r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/19-llamascope-slimpj-res-32k blocks.19.hook_resid_post 32768 expected_average_only_in l20r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/20-llamascope-slimpj-res-32k blocks.20.hook_resid_post 32768 expected_average_only_in l21r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/21-llamascope-slimpj-res-32k blocks.21.hook_resid_post 32768 expected_average_only_in l22r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/22-llamascope-slimpj-res-32k blocks.22.hook_resid_post 32768 expected_average_only_in l23r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/23-llamascope-slimpj-res-32k blocks.23.hook_resid_post 32768 expected_average_only_in l24r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/24-llamascope-slimpj-res-32k blocks.24.hook_resid_post 32768 expected_average_only_in l25r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/25-llamascope-slimpj-res-32k blocks.25.hook_resid_post 32768 expected_average_only_in l26r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/26-llamascope-slimpj-res-32k blocks.26.hook_resid_post 32768 expected_average_only_in l27r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/27-llamascope-slimpj-res-32k blocks.27.hook_resid_post 32768 expected_average_only_in l28r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/28-llamascope-slimpj-res-32k blocks.28.hook_resid_post 32768 expected_average_only_in l29r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/29-llamascope-slimpj-res-32k blocks.29.hook_resid_post 32768 expected_average_only_in l30r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/30-llamascope-slimpj-res-32k blocks.30.hook_resid_post 32768 expected_average_only_in l31r_800m_slimpajamaLoad this SAE jumprelu deepseek-r1-distill-llama-8b/31-llamascope-slimpj-res-32k blocks.31.hook_resid_post 32768 expected_average_only_in l0r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/0-llamascope-slimpj-openr1-res-32k blocks.0.hook_resid_post 32768 expected_average_only_in l1r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/1-llamascope-slimpj-openr1-res-32k blocks.1.hook_resid_post 32768 expected_average_only_in l2r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/2-llamascope-slimpj-openr1-res-32k blocks.2.hook_resid_post 32768 expected_average_only_in l3r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/3-llamascope-slimpj-openr1-res-32k blocks.3.hook_resid_post 32768 expected_average_only_in l4r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/4-llamascope-slimpj-openr1-res-32k blocks.4.hook_resid_post 32768 expected_average_only_in l5r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/5-llamascope-slimpj-openr1-res-32k blocks.5.hook_resid_post 32768 expected_average_only_in l6r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/6-llamascope-slimpj-openr1-res-32k blocks.6.hook_resid_post 32768 expected_average_only_in l7r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/7-llamascope-slimpj-openr1-res-32k blocks.7.hook_resid_post 32768 expected_average_only_in l8r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/8-llamascope-slimpj-openr1-res-32k blocks.8.hook_resid_post 32768 expected_average_only_in l9r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/9-llamascope-slimpj-openr1-res-32k blocks.9.hook_resid_post 32768 expected_average_only_in l10r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/10-llamascope-slimpj-openr1-res-32k blocks.10.hook_resid_post 32768 expected_average_only_in l11r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/11-llamascope-slimpj-openr1-res-32k blocks.11.hook_resid_post 32768 expected_average_only_in l12r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/12-llamascope-slimpj-openr1-res-32k blocks.12.hook_resid_post 32768 expected_average_only_in l13r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/13-llamascope-slimpj-openr1-res-32k blocks.13.hook_resid_post 32768 expected_average_only_in l14r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/14-llamascope-slimpj-openr1-res-32k blocks.14.hook_resid_post 32768 expected_average_only_in l15r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/15-llamascope-slimpj-openr1-res-32k blocks.15.hook_resid_post 32768 expected_average_only_in l16r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/16-llamascope-slimpj-openr1-res-32k blocks.16.hook_resid_post 32768 expected_average_only_in l17r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/17-llamascope-slimpj-openr1-res-32k blocks.17.hook_resid_post 32768 expected_average_only_in l18r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/18-llamascope-slimpj-openr1-res-32k blocks.18.hook_resid_post 32768 expected_average_only_in l19r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/19-llamascope-slimpj-openr1-res-32k blocks.19.hook_resid_post 32768 expected_average_only_in l20r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/20-llamascope-slimpj-openr1-res-32k blocks.20.hook_resid_post 32768 expected_average_only_in l21r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/21-llamascope-slimpj-openr1-res-32k blocks.21.hook_resid_post 32768 expected_average_only_in l22r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/22-llamascope-slimpj-openr1-res-32k blocks.22.hook_resid_post 32768 expected_average_only_in l23r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/23-llamascope-slimpj-openr1-res-32k blocks.23.hook_resid_post 32768 expected_average_only_in l24r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/24-llamascope-slimpj-openr1-res-32k blocks.24.hook_resid_post 32768 expected_average_only_in l25r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/25-llamascope-slimpj-openr1-res-32k blocks.25.hook_resid_post 32768 expected_average_only_in l26r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/26-llamascope-slimpj-openr1-res-32k blocks.26.hook_resid_post 32768 expected_average_only_in l27r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/27-llamascope-slimpj-openr1-res-32k blocks.27.hook_resid_post 32768 expected_average_only_in l28r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/28-llamascope-slimpj-openr1-res-32k blocks.28.hook_resid_post 32768 expected_average_only_in l29r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/29-llamascope-slimpj-openr1-res-32k blocks.29.hook_resid_post 32768 expected_average_only_in l30r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/30-llamascope-slimpj-openr1-res-32k blocks.30.hook_resid_post 32768 expected_average_only_in l31r_400m_slimpajama_400m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/31-llamascope-slimpj-openr1-res-32k blocks.31.hook_resid_post 32768 expected_average_only_in l0r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/0-llamascope-openr1-res-32k blocks.0.hook_resid_post 32768 expected_average_only_in l1r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/1-llamascope-openr1-res-32k blocks.1.hook_resid_post 32768 expected_average_only_in l2r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/2-llamascope-openr1-res-32k blocks.2.hook_resid_post 32768 expected_average_only_in l3r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/3-llamascope-openr1-res-32k blocks.3.hook_resid_post 32768 expected_average_only_in l4r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/4-llamascope-openr1-res-32k blocks.4.hook_resid_post 32768 expected_average_only_in l5r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/5-llamascope-openr1-res-32k blocks.5.hook_resid_post 32768 expected_average_only_in l6r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/6-llamascope-openr1-res-32k blocks.6.hook_resid_post 32768 expected_average_only_in l7r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/7-llamascope-openr1-res-32k blocks.7.hook_resid_post 32768 expected_average_only_in l8r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/8-llamascope-openr1-res-32k blocks.8.hook_resid_post 32768 expected_average_only_in l9r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/9-llamascope-openr1-res-32k blocks.9.hook_resid_post 32768 expected_average_only_in l10r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/10-llamascope-openr1-res-32k blocks.10.hook_resid_post 32768 expected_average_only_in l11r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/11-llamascope-openr1-res-32k blocks.11.hook_resid_post 32768 expected_average_only_in l12r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/12-llamascope-openr1-res-32k blocks.12.hook_resid_post 32768 expected_average_only_in l13r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/13-llamascope-openr1-res-32k blocks.13.hook_resid_post 32768 expected_average_only_in l14r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/14-llamascope-openr1-res-32k blocks.14.hook_resid_post 32768 expected_average_only_in l15r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/15-llamascope-openr1-res-32k blocks.15.hook_resid_post 32768 expected_average_only_in l16r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/16-llamascope-openr1-res-32k blocks.16.hook_resid_post 32768 expected_average_only_in l17r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/17-llamascope-openr1-res-32k blocks.17.hook_resid_post 32768 expected_average_only_in l18r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/18-llamascope-openr1-res-32k blocks.18.hook_resid_post 32768 expected_average_only_in l19r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/19-llamascope-openr1-res-32k blocks.19.hook_resid_post 32768 expected_average_only_in l20r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/20-llamascope-openr1-res-32k blocks.20.hook_resid_post 32768 expected_average_only_in l21r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/21-llamascope-openr1-res-32k blocks.21.hook_resid_post 32768 expected_average_only_in l22r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/22-llamascope-openr1-res-32k blocks.22.hook_resid_post 32768 expected_average_only_in l23r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/23-llamascope-openr1-res-32k blocks.23.hook_resid_post 32768 expected_average_only_in l24r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/24-llamascope-openr1-res-32k blocks.24.hook_resid_post 32768 expected_average_only_in l25r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/25-llamascope-openr1-res-32k blocks.25.hook_resid_post 32768 expected_average_only_in l26r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/26-llamascope-openr1-res-32k blocks.26.hook_resid_post 32768 expected_average_only_in l27r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/27-llamascope-openr1-res-32k blocks.27.hook_resid_post 32768 expected_average_only_in l28r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/28-llamascope-openr1-res-32k blocks.28.hook_resid_post 32768 expected_average_only_in l29r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/29-llamascope-openr1-res-32k blocks.29.hook_resid_post 32768 expected_average_only_in l30r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/30-llamascope-openr1-res-32k blocks.30.hook_resid_post 32768 expected_average_only_in l31r_800m_openr1_mathLoad this SAE jumprelu deepseek-r1-distill-llama-8b/31-llamascope-openr1-res-32k blocks.31.hook_resid_post 32768 expected_average_only_in"},{"location":"sae_table/#mistral-7b-res-wg","title":"mistral-7b-res-wg","text":"<ul> <li>Huggingface Repo: JoshEngels/Mistral-7B-Residual-Stream-SAEs</li> <li>model: mistral-7b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.8.hook_resid_preLoad this SAE standard blocks.8.hook_resid_pre 65536 constant_norm_rescale blocks.16.hook_resid_preLoad this SAE standard blocks.16.hook_resid_pre 65536 constant_norm_rescale blocks.24.hook_resid_preLoad this SAE standard blocks.24.hook_resid_pre 65536 constant_norm_rescale"},{"location":"sae_table/#pythia-70m-deduped-att-sm","title":"pythia-70m-deduped-att-sm","text":"<ul> <li>Huggingface Repo: ctigges/pythia-70m-deduped__att-sm_processed</li> <li>model: pythia-70m-deduped</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_attn_outLoad this SAE standard pythia-70m-deduped/0-att-sm blocks.0.hook_attn_out 32768 none blocks.1.hook_attn_outLoad this SAE standard pythia-70m-deduped/1-att-sm blocks.1.hook_attn_out 32768 none blocks.2.hook_attn_outLoad this SAE standard pythia-70m-deduped/2-att-sm blocks.2.hook_attn_out 32768 none blocks.3.hook_attn_outLoad this SAE standard pythia-70m-deduped/3-att-sm blocks.3.hook_attn_out 32768 none blocks.4.hook_attn_outLoad this SAE standard pythia-70m-deduped/4-att-sm blocks.4.hook_attn_out 32768 none blocks.5.hook_attn_outLoad this SAE standard pythia-70m-deduped/5-att-sm blocks.5.hook_attn_out 32768 none"},{"location":"sae_table/#pythia-70m-deduped-mlp-sm","title":"pythia-70m-deduped-mlp-sm","text":"<ul> <li>Huggingface Repo: ctigges/pythia-70m-deduped__mlp-sm_processed</li> <li>model: pythia-70m-deduped</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_mlp_outLoad this SAE standard pythia-70m-deduped/0-mlp-sm blocks.0.hook_mlp_out 32768 none blocks.1.hook_mlp_outLoad this SAE standard pythia-70m-deduped/1-mlp-sm blocks.1.hook_mlp_out 32768 none blocks.2.hook_mlp_outLoad this SAE standard pythia-70m-deduped/2-mlp-sm blocks.2.hook_mlp_out 32768 none blocks.3.hook_mlp_outLoad this SAE standard pythia-70m-deduped/3-mlp-sm blocks.3.hook_mlp_out 32768 none blocks.4.hook_mlp_outLoad this SAE standard pythia-70m-deduped/4-mlp-sm blocks.4.hook_mlp_out 32768 none blocks.5.hook_mlp_outLoad this SAE standard pythia-70m-deduped/5-mlp-sm blocks.5.hook_mlp_out 32768 none"},{"location":"sae_table/#pythia-70m-deduped-res-sm","title":"pythia-70m-deduped-res-sm","text":"<ul> <li>Huggingface Repo: ctigges/pythia-70m-deduped__res-sm_processed</li> <li>model: pythia-70m-deduped</li> <li>Additional Links:<ul> <li>Dashboards</li> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_resid_preLoad this SAE standard pythia-70m-deduped/e-att-sm blocks.0.hook_resid_pre 32768 none blocks.0.hook_resid_postLoad this SAE standard pythia-70m-deduped/0-res-sm blocks.0.hook_resid_post 32768 none blocks.1.hook_resid_postLoad this SAE standard pythia-70m-deduped/1-res-sm blocks.1.hook_resid_post 32768 none blocks.2.hook_resid_postLoad this SAE standard pythia-70m-deduped/2-res-sm blocks.2.hook_resid_post 32768 none blocks.3.hook_resid_postLoad this SAE standard pythia-70m-deduped/3-res-sm blocks.3.hook_resid_post 32768 none blocks.4.hook_resid_postLoad this SAE standard pythia-70m-deduped/4-res-sm blocks.4.hook_resid_post 32768 none blocks.5.hook_resid_postLoad this SAE standard pythia-70m-deduped/5-res-sm blocks.5.hook_resid_post 32768 none"},{"location":"sae_table/#sae_bench_gemma-2-2b_topk_width-2pow12_date-1109","title":"sae_bench_gemma-2-2b_topk_width-2pow12_date-1109","text":"<ul> <li>Huggingface Repo: canrager/saebench_gemma-2-2b_width-2pow12_date-1109</li> <li>model: gemma-2-2b</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.12.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_0_step_final blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_0_step_0 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_308Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_0_step_308 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_3088Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_0_step_3088 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_30881Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_0_step_30881 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_97Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_0_step_97 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_976Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_0_step_976 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_9765Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_0_step_9765 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_1_step_final blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_1_step_0 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_308Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_1_step_308 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_3088Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_1_step_3088 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_30881Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_1_step_30881 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_97Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_1_step_97 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_976Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_1_step_976 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_9765Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_1_step_9765 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_2_step_final blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_2_step_0 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_308Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_2_step_308 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_3088Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_2_step_3088 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_30881Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_2_step_30881 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_97Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_2_step_97 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_976Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_2_step_976 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_9765Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_2_step_9765 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_3_step_final blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_3_step_0 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_308Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_3_step_308 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_3088Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_3_step_3088 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_30881Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_3_step_30881 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_97Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_3_step_97 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_976Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_3_step_976 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_9765Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_3_step_9765 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_4_step_final blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_4_step_0 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_308Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_4_step_308 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_3088Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_4_step_3088 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_30881Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_4_step_30881 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_97Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_4_step_97 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_976Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_4_step_976 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_9765Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_4_step_9765 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_5_step_final blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_5_step_0 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_308Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_5_step_308 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_3088Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_5_step_3088 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_30881Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_5_step_30881 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_97Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_5_step_97 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_976Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_5_step_976 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_9765Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-4k__trainer_5_step_9765 blocks.12.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_0_step_final blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_0_step_0 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_308Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_0_step_308 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_3088Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_0_step_3088 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_30881Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_0_step_30881 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_97Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_0_step_97 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_976Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_0_step_976 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_9765Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_0_step_9765 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_1_step_final blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_1_step_0 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_308Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_1_step_308 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_3088Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_1_step_3088 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_30881Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_1_step_30881 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_97Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_1_step_97 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_976Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_1_step_976 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_9765Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_1_step_9765 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_2_step_final blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_2_step_0 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_308Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_2_step_308 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_3088Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_2_step_3088 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_30881Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_2_step_30881 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_97Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_2_step_97 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_976Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_2_step_976 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_9765Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_2_step_9765 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_3_step_final blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_3_step_0 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_308Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_3_step_308 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_3088Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_3_step_3088 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_30881Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_3_step_30881 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_97Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_3_step_97 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_976Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_3_step_976 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_9765Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_3_step_9765 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_4_step_final blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_4_step_0 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_308Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_4_step_308 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_3088Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_4_step_3088 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_30881Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_4_step_30881 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_97Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_4_step_97 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_976Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_4_step_976 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_9765Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_4_step_9765 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_5_step_final blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_5_step_0 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_308Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_5_step_308 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_3088Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_5_step_3088 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_30881Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_5_step_30881 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_97Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_5_step_97 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_976Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_5_step_976 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_9765Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-4k__trainer_5_step_9765 blocks.19.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_0_step_final blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_0_step_0 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_308Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_0_step_308 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_3088Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_0_step_3088 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_30881Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_0_step_30881 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_97Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_0_step_97 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_976Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_0_step_976 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_9765Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_0_step_9765 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_1_step_final blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_1_step_0 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_308Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_1_step_308 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_3088Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_1_step_3088 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_30881Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_1_step_30881 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_97Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_1_step_97 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_976Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_1_step_976 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_9765Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_1_step_9765 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_2_step_final blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_2_step_0 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_308Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_2_step_308 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_3088Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_2_step_3088 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_30881Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_2_step_30881 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_97Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_2_step_97 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_976Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_2_step_976 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_9765Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_2_step_9765 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_3_step_final blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_3_step_0 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_308Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_3_step_308 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_3088Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_3_step_3088 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_30881Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_3_step_30881 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_97Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_3_step_97 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_976Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_3_step_976 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_9765Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_3_step_9765 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_4_step_final blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_4_step_0 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_308Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_4_step_308 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_3088Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_4_step_3088 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_30881Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_4_step_30881 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_97Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_4_step_97 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_976Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_4_step_976 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_9765Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_4_step_9765 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_5_step_final blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_5_step_0 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_308Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_5_step_308 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_3088Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_5_step_3088 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_30881Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_5_step_30881 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_97Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_5_step_97 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_976Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_5_step_976 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_9765Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-4k__trainer_5_step_9765 blocks.5.hook_resid_post 4096 none"},{"location":"sae_table/#sae_bench_gemma-2-2b_topk_width-2pow14_date-1109","title":"sae_bench_gemma-2-2b_topk_width-2pow14_date-1109","text":"<ul> <li>Huggingface Repo: canrager/saebench_gemma-2-2b_width-2pow14_date-1109</li> <li>model: gemma-2-2b</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.12.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_0_step_final blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_0_step_0 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_146Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_0_step_146 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_1464Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_0_step_1464 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_14648Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_0_step_14648 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_463Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_0_step_463 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_4632Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_0_step_4632 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_46322Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_0_step_46322 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_1_step_final blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_1_step_0 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_146Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_1_step_146 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_1464Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_1_step_1464 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_14648Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_1_step_14648 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_463Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_1_step_463 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_4632Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_1_step_4632 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_46322Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_1_step_46322 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_2_step_final blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_2_step_0 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_146Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_2_step_146 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_1464Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_2_step_1464 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_14648Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_2_step_14648 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_463Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_2_step_463 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_4632Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_2_step_4632 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_46322Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_2_step_46322 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_3_step_final blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_3_step_0 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_146Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_3_step_146 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_1464Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_3_step_1464 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_14648Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_3_step_14648 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_463Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_3_step_463 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_4632Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_3_step_4632 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_46322Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_3_step_46322 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_4_step_final blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_4_step_0 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_146Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_4_step_146 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_1464Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_4_step_1464 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_14648Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_4_step_14648 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_463Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_4_step_463 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_4632Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_4_step_4632 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_46322Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_4_step_46322 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_5_step_final blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_5_step_0 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_146Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_5_step_146 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_1464Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_5_step_1464 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_14648Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_5_step_14648 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_463Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_5_step_463 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_4632Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_5_step_4632 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_46322Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-16k__trainer_5_step_46322 blocks.12.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_0_step_final blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_0_step_0 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_146Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_0_step_146 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_1464Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_0_step_1464 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_14648Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_0_step_14648 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_463Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_0_step_463 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_4632Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_0_step_4632 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_46322Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_0_step_46322 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_1_step_final blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_1_step_0 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_146Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_1_step_146 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_1464Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_1_step_1464 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_14648Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_1_step_14648 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_463Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_1_step_463 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_4632Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_1_step_4632 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_46322Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_1_step_46322 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_2_step_final blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_2_step_0 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_146Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_2_step_146 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_1464Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_2_step_1464 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_14648Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_2_step_14648 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_463Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_2_step_463 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_4632Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_2_step_4632 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_46322Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_2_step_46322 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_3_step_final blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_3_step_0 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_146Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_3_step_146 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_1464Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_3_step_1464 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_14648Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_3_step_14648 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_463Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_3_step_463 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_4632Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_3_step_4632 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_46322Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_3_step_46322 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_4_step_final blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_4_step_0 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_146Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_4_step_146 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_1464Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_4_step_1464 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_14648Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_4_step_14648 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_463Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_4_step_463 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_4632Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_4_step_4632 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_46322Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_4_step_46322 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_5_step_final blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_5_step_0 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_146Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_5_step_146 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_1464Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_5_step_1464 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_14648Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_5_step_14648 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_463Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_5_step_463 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_4632Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_5_step_4632 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_46322Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-16k__trainer_5_step_46322 blocks.19.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_0_step_final blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_0_step_0 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_146Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_0_step_146 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_1464Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_0_step_1464 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_14648Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_0_step_14648 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_463Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_0_step_463 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_4632Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_0_step_4632 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_46322Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_0_step_46322 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_1_step_final blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_1_step_0 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_146Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_1_step_146 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_1464Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_1_step_1464 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_14648Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_1_step_14648 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_463Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_1_step_463 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_4632Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_1_step_4632 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_46322Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_1_step_46322 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_2_step_final blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_2_step_0 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_146Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_2_step_146 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_1464Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_2_step_1464 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_14648Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_2_step_14648 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_463Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_2_step_463 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_4632Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_2_step_4632 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_46322Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_2_step_46322 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_3_step_final blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_3_step_0 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_146Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_3_step_146 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_1464Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_3_step_1464 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_14648Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_3_step_14648 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_463Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_3_step_463 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_4632Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_3_step_4632 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_46322Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_3_step_46322 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_4_step_final blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_4_step_0 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_146Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_4_step_146 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_1464Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_4_step_1464 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_14648Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_4_step_14648 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_463Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_4_step_463 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_4632Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_4_step_4632 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_46322Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_4_step_46322 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_5_step_final blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_5_step_0 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_146Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_5_step_146 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_1464Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_5_step_1464 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_14648Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_5_step_14648 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_463Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_5_step_463 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_4632Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_5_step_4632 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_46322Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-16k__trainer_5_step_46322 blocks.5.hook_resid_post 16384 none"},{"location":"sae_table/#sae_bench_gemma-2-2b_topk_width-2pow16_date-1109","title":"sae_bench_gemma-2-2b_topk_width-2pow16_date-1109","text":"<ul> <li>Huggingface Repo: canrager/saebench_gemma-2-2b_width-2pow16_date-1109</li> <li>model: gemma-2-2b</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.12.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-65k__trainer_0_step_final blocks.12.hook_resid_post 65536 none blocks.12.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-65k__trainer_1_step_final blocks.12.hook_resid_post 65536 none blocks.12.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-65k__trainer_2_step_final blocks.12.hook_resid_post 65536 none blocks.12.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-65k__trainer_3_step_final blocks.12.hook_resid_post 65536 none blocks.12.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-65k__trainer_4_step_final blocks.12.hook_resid_post 65536 none blocks.12.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/12-sae_bench-topk-res-65k__trainer_5_step_final blocks.12.hook_resid_post 65536 none blocks.19.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-65k__trainer_0_step_final blocks.19.hook_resid_post 64512 none blocks.19.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-65k__trainer_1_step_final blocks.19.hook_resid_post 64512 none blocks.19.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-65k__trainer_2_step_final blocks.19.hook_resid_post 64512 none blocks.19.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-65k__trainer_3_step_final blocks.19.hook_resid_post 64512 none blocks.19.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-65k__trainer_4_step_final blocks.19.hook_resid_post 64512 none blocks.19.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/19-sae_bench-topk-res-65k__trainer_5_step_final blocks.19.hook_resid_post 64512 none blocks.5.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-65k__trainer_0_step_final blocks.5.hook_resid_post 65536 none blocks.5.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-65k__trainer_1_step_final blocks.5.hook_resid_post 65536 none blocks.5.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-65k__trainer_2_step_final blocks.5.hook_resid_post 65536 none blocks.5.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-65k__trainer_3_step_final blocks.5.hook_resid_post 65536 none blocks.5.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-65k__trainer_4_step_final blocks.5.hook_resid_post 65536 none blocks.5.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/5-sae_bench-topk-res-65k__trainer_5_step_final blocks.5.hook_resid_post 65536 none"},{"location":"sae_table/#sae_bench_gemma-2-2b_vanilla_width-2pow12_date-1109","title":"sae_bench_gemma-2-2b_vanilla_width-2pow12_date-1109","text":"<ul> <li>Huggingface Repo: canrager/saebench_gemma-2-2b_width-2pow12_date-1109</li> <li>model: gemma-2-2b</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.12.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_0_step_final blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_0_step_0 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_308Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_0_step_308 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_3088Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_0_step_3088 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_30881Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_0_step_30881 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_97Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_0_step_97 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_976Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_0_step_976 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_0_step_9765Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_0_step_9765 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_1_step_final blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_1_step_0 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_308Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_1_step_308 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_3088Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_1_step_3088 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_30881Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_1_step_30881 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_97Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_1_step_97 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_976Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_1_step_976 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_1_step_9765Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_1_step_9765 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_2_step_final blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_2_step_0 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_308Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_2_step_308 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_3088Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_2_step_3088 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_30881Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_2_step_30881 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_97Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_2_step_97 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_976Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_2_step_976 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_2_step_9765Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_2_step_9765 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_3_step_final blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_3_step_0 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_308Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_3_step_308 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_3088Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_3_step_3088 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_30881Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_3_step_30881 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_97Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_3_step_97 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_976Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_3_step_976 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_3_step_9765Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_3_step_9765 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_4_step_final blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_4_step_0 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_308Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_4_step_308 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_3088Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_4_step_3088 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_30881Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_4_step_30881 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_97Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_4_step_97 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_976Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_4_step_976 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_4_step_9765Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_4_step_9765 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_5_step_final blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_5_step_0 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_308Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_5_step_308 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_3088Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_5_step_3088 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_30881Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_5_step_30881 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_97Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_5_step_97 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_976Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_5_step_976 blocks.12.hook_resid_post 4096 none blocks.12.hook_resid_post__trainer_5_step_9765Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-4k__trainer_5_step_9765 blocks.12.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_0_step_final blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_0_step_0 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_308Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_0_step_308 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_3088Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_0_step_3088 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_30881Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_0_step_30881 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_97Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_0_step_97 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_976Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_0_step_976 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_0_step_9765Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_0_step_9765 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_1_step_final blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_1_step_0 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_308Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_1_step_308 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_3088Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_1_step_3088 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_30881Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_1_step_30881 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_97Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_1_step_97 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_976Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_1_step_976 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_1_step_9765Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_1_step_9765 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_2_step_final blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_2_step_0 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_308Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_2_step_308 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_3088Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_2_step_3088 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_30881Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_2_step_30881 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_97Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_2_step_97 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_976Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_2_step_976 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_2_step_9765Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_2_step_9765 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_3_step_final blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_3_step_0 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_308Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_3_step_308 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_3088Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_3_step_3088 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_30881Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_3_step_30881 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_97Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_3_step_97 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_976Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_3_step_976 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_3_step_9765Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_3_step_9765 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_4_step_final blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_4_step_0 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_308Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_4_step_308 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_3088Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_4_step_3088 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_30881Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_4_step_30881 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_97Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_4_step_97 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_976Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_4_step_976 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_4_step_9765Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_4_step_9765 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_5_step_final blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_5_step_0 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_308Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_5_step_308 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_3088Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_5_step_3088 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_30881Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_5_step_30881 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_97Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_5_step_97 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_976Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_5_step_976 blocks.19.hook_resid_post 4096 none blocks.19.hook_resid_post__trainer_5_step_9765Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-4k__trainer_5_step_9765 blocks.19.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_0_step_final blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_0_step_0 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_308Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_0_step_308 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_3088Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_0_step_3088 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_30881Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_0_step_30881 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_97Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_0_step_97 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_976Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_0_step_976 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_0_step_9765Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_0_step_9765 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_1_step_final blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_1_step_0 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_308Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_1_step_308 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_3088Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_1_step_3088 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_30881Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_1_step_30881 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_97Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_1_step_97 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_976Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_1_step_976 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_1_step_9765Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_1_step_9765 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_2_step_final blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_2_step_0 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_308Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_2_step_308 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_3088Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_2_step_3088 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_30881Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_2_step_30881 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_97Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_2_step_97 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_976Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_2_step_976 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_2_step_9765Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_2_step_9765 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_3_step_final blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_3_step_0 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_308Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_3_step_308 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_3088Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_3_step_3088 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_30881Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_3_step_30881 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_97Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_3_step_97 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_976Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_3_step_976 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_3_step_9765Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_3_step_9765 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_4_step_final blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_4_step_0 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_308Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_4_step_308 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_3088Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_4_step_3088 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_30881Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_4_step_30881 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_97Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_4_step_97 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_976Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_4_step_976 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_4_step_9765Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_4_step_9765 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_5_step_final blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_5_step_0 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_308Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_5_step_308 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_3088Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_5_step_3088 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_30881Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_5_step_30881 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_97Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_5_step_97 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_976Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_5_step_976 blocks.5.hook_resid_post 4096 none blocks.5.hook_resid_post__trainer_5_step_9765Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-4k__trainer_5_step_9765 blocks.5.hook_resid_post 4096 none"},{"location":"sae_table/#sae_bench_gemma-2-2b_vanilla_width-2pow14_date-1109","title":"sae_bench_gemma-2-2b_vanilla_width-2pow14_date-1109","text":"<ul> <li>Huggingface Repo: canrager/saebench_gemma-2-2b_width-2pow14_date-1109</li> <li>model: gemma-2-2b</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.12.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_0_step_final blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_0_step_0 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_146Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_0_step_146 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_1464Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_0_step_1464 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_14648Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_0_step_14648 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_463Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_0_step_463 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_4632Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_0_step_4632 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_0_step_46322Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_0_step_46322 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_1_step_final blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_1_step_0 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_146Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_1_step_146 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_1464Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_1_step_1464 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_14648Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_1_step_14648 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_463Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_1_step_463 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_4632Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_1_step_4632 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_1_step_46322Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_1_step_46322 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_2_step_final blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_2_step_0 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_146Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_2_step_146 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_1464Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_2_step_1464 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_14648Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_2_step_14648 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_463Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_2_step_463 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_4632Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_2_step_4632 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_2_step_46322Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_2_step_46322 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_3_step_final blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_3_step_0 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_146Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_3_step_146 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_1464Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_3_step_1464 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_14648Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_3_step_14648 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_463Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_3_step_463 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_4632Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_3_step_4632 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_3_step_46322Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_3_step_46322 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_4_step_final blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_4_step_0 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_146Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_4_step_146 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_1464Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_4_step_1464 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_14648Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_4_step_14648 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_463Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_4_step_463 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_4632Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_4_step_4632 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_4_step_46322Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_4_step_46322 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_5_step_final blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_5_step_0 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_146Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_5_step_146 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_1464Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_5_step_1464 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_14648Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_5_step_14648 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_463Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_5_step_463 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_4632Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_5_step_4632 blocks.12.hook_resid_post 16384 none blocks.12.hook_resid_post__trainer_5_step_46322Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-16k__trainer_5_step_46322 blocks.12.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_0_step_final blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_0_step_0 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_146Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_0_step_146 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_1464Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_0_step_1464 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_14648Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_0_step_14648 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_463Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_0_step_463 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_4632Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_0_step_4632 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_0_step_46322Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_0_step_46322 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_1_step_final blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_1_step_0 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_146Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_1_step_146 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_1464Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_1_step_1464 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_14648Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_1_step_14648 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_463Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_1_step_463 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_4632Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_1_step_4632 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_1_step_46322Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_1_step_46322 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_2_step_final blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_2_step_0 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_146Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_2_step_146 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_1464Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_2_step_1464 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_14648Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_2_step_14648 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_463Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_2_step_463 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_4632Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_2_step_4632 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_2_step_46322Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_2_step_46322 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_3_step_final blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_3_step_0 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_146Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_3_step_146 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_1464Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_3_step_1464 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_14648Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_3_step_14648 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_463Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_3_step_463 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_4632Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_3_step_4632 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_3_step_46322Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_3_step_46322 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_4_step_final blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_4_step_0 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_146Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_4_step_146 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_1464Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_4_step_1464 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_14648Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_4_step_14648 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_463Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_4_step_463 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_4632Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_4_step_4632 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_4_step_46322Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_4_step_46322 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_5_step_final blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_5_step_0 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_146Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_5_step_146 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_1464Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_5_step_1464 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_14648Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_5_step_14648 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_463Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_5_step_463 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_4632Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_5_step_4632 blocks.19.hook_resid_post 16384 none blocks.19.hook_resid_post__trainer_5_step_46322Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-16k__trainer_5_step_46322 blocks.19.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_0_step_final blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_0_step_0 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_146Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_0_step_146 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_1464Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_0_step_1464 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_14648Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_0_step_14648 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_463Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_0_step_463 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_4632Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_0_step_4632 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_0_step_46322Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_0_step_46322 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_1_step_final blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_1_step_0 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_146Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_1_step_146 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_1464Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_1_step_1464 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_14648Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_1_step_14648 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_463Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_1_step_463 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_4632Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_1_step_4632 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_1_step_46322Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_1_step_46322 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_2_step_final blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_2_step_0 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_146Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_2_step_146 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_1464Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_2_step_1464 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_14648Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_2_step_14648 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_463Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_2_step_463 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_4632Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_2_step_4632 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_2_step_46322Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_2_step_46322 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_3_step_final blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_3_step_0 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_146Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_3_step_146 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_1464Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_3_step_1464 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_14648Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_3_step_14648 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_463Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_3_step_463 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_4632Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_3_step_4632 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_3_step_46322Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_3_step_46322 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_4_step_final blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_4_step_0 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_146Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_4_step_146 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_1464Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_4_step_1464 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_14648Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_4_step_14648 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_463Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_4_step_463 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_4632Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_4_step_4632 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_4_step_46322Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_4_step_46322 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_5_step_final blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_5_step_0 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_146Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_5_step_146 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_1464Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_5_step_1464 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_14648Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_5_step_14648 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_463Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_5_step_463 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_4632Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_5_step_4632 blocks.5.hook_resid_post 16384 none blocks.5.hook_resid_post__trainer_5_step_46322Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-16k__trainer_5_step_46322 blocks.5.hook_resid_post 16384 none"},{"location":"sae_table/#sae_bench_gemma-2-2b_vanilla_width-2pow16_date-1109","title":"sae_bench_gemma-2-2b_vanilla_width-2pow16_date-1109","text":"<ul> <li>Huggingface Repo: canrager/saebench_gemma-2-2b_width-2pow16_date-1109</li> <li>model: gemma-2-2b</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.12.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-65k__trainer_0_step_final blocks.12.hook_resid_post 65536 none blocks.12.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-65k__trainer_1_step_final blocks.12.hook_resid_post 65536 none blocks.12.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-65k__trainer_2_step_final blocks.12.hook_resid_post 65536 none blocks.12.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-65k__trainer_3_step_final blocks.12.hook_resid_post 65536 none blocks.12.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-65k__trainer_4_step_final blocks.12.hook_resid_post 65536 none blocks.12.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/12-sae_bench-standard-res-65k__trainer_5_step_final blocks.12.hook_resid_post 65536 none blocks.19.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-65k__trainer_0_step_final blocks.19.hook_resid_post 64512 none blocks.19.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-65k__trainer_1_step_final blocks.19.hook_resid_post 64512 none blocks.19.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-65k__trainer_2_step_final blocks.19.hook_resid_post 64512 none blocks.19.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-65k__trainer_3_step_final blocks.19.hook_resid_post 64512 none blocks.19.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-65k__trainer_4_step_final blocks.19.hook_resid_post 64512 none blocks.19.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/19-sae_bench-standard-res-65k__trainer_5_step_final blocks.19.hook_resid_post 64512 none blocks.5.hook_resid_post__trainer_0Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-65k__trainer_0_step_final blocks.5.hook_resid_post 65536 none blocks.5.hook_resid_post__trainer_1Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-65k__trainer_1_step_final blocks.5.hook_resid_post 65536 none blocks.5.hook_resid_post__trainer_2Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-65k__trainer_2_step_final blocks.5.hook_resid_post 65536 none blocks.5.hook_resid_post__trainer_3Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-65k__trainer_3_step_final blocks.5.hook_resid_post 65536 none blocks.5.hook_resid_post__trainer_4Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-65k__trainer_4_step_final blocks.5.hook_resid_post 65536 none blocks.5.hook_resid_post__trainer_5Load this SAE standard gemma-2-2b/5-sae_bench-standard-res-65k__trainer_5_step_final blocks.5.hook_resid_post 65536 none"},{"location":"sae_table/#sae_bench_pythia70m_sweep_gated_ctx128_0730","title":"sae_bench_pythia70m_sweep_gated_ctx128_0730","text":"<ul> <li>Huggingface Repo: canrager/lm_sae</li> <li>model: pythia-70m-deduped</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.3.hook_resid_post__trainer_0Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-4k__trainer_0_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_1Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-4k__trainer_1_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_10Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-16k__trainer_10_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_12Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-4k__trainer_12_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_13Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-4k__trainer_13_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_14Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-16k__trainer_14_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_15Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-16k__trainer_15_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_16Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-4k__trainer_16_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_17Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-4k__trainer_17_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_18Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-16k__trainer_18_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_19Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-16k__trainer_19_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_2Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-16k__trainer_2_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_3Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-16k__trainer_3_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_4Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-4k__trainer_4_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_5Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-4k__trainer_5_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_6Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-16k__trainer_6_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_7Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-16k__trainer_7_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_8Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-4k__trainer_8_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_9Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-4k__trainer_9_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_11Load this SAE gated pythia-70m-deduped/3-sae_bench-gated-res-16k__trainer_11_step_final blocks.3.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_0Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-4k__trainer_0_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_1Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-4k__trainer_1_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_10Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-16k__trainer_10_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_11Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-16k__trainer_11_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_12Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-4k__trainer_12_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_13Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-4k__trainer_13_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_14Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-16k__trainer_14_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_15Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-16k__trainer_15_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_16Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-4k__trainer_16_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_17Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-4k__trainer_17_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_18Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-16k__trainer_18_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_19Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-16k__trainer_19_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_2Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-16k__trainer_2_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_3Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-16k__trainer_3_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_4Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-4k__trainer_4_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_5Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-4k__trainer_5_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_6Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-16k__trainer_6_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_9Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-4k__trainer_9_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_8Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-4k__trainer_8_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_7Load this SAE gated pythia-70m-deduped/4-sae_bench-gated-res-16k__trainer_7_step_final blocks.4.hook_resid_post 16384 none"},{"location":"sae_table/#sae_bench_pythia70m_sweep_panneal_ctx128_0730","title":"sae_bench_pythia70m_sweep_panneal_ctx128_0730","text":"<ul> <li>Huggingface Repo: canrager/lm_sae</li> <li>model: pythia-70m-deduped</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.3.hook_resid_post__trainer_16Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_16_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_17Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_17_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_18Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_18_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_19Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_19_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_2Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_2_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_20Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_20_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_21Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_21_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_22Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_22_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_15Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_15_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_23Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_23_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_25Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_25_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_26Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_26_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_27Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_27_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_3Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_3_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_4Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_4_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_5Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_5_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_6Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_6_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_7Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_7_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_24Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_24_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_14Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_14_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_13Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_13_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_12Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_12_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_0Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_0_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_1Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_1_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_10Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_10_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_11Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-16k__trainer_11_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_8Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_8_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_9Load this SAE standard pythia-70m-deduped/3-sae_bench-panneal-res-4k__trainer_9_step_final blocks.3.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_17Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_17_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_18Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_18_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_19Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_19_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_16Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_16_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_15Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_15_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_14Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_14_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_13Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_13_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_12Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_12_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_11Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_11_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_10Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_10_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_1Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_1_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_0Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_0_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_2Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_2_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_20Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_20_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_22Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_22_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_9Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_9_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_8Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_8_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_7Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_7_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_6Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_6_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_5Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_5_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_4Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_4_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_27Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_27_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_26Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_26_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_25Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_25_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_3Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_3_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_24Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_24_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_21Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-4k__trainer_21_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_23Load this SAE standard pythia-70m-deduped/4-sae_bench-panneal-res-16k__trainer_23_step_final blocks.4.hook_resid_post 16384 none"},{"location":"sae_table/#sae_bench_pythia70m_sweep_standard_ctx128_0712","title":"sae_bench_pythia70m_sweep_standard_ctx128_0712","text":"<ul> <li>Huggingface Repo: canrager/lm_sae</li> <li>model: pythia-70m-deduped</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.3.hook_resid_post__trainer_10Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-16k__trainer_10_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_11Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-16k__trainer_11_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_8Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-4k__trainer_8_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_7Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-16k__trainer_7_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_6Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-16k__trainer_6_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_5Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-4k__trainer_5_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_4Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-4k__trainer_4_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_3Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-16k__trainer_3_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_2Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-16k__trainer_2_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_19Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-16k__trainer_19_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_18Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-16k__trainer_18_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_17Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-4k__trainer_17_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_16Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-4k__trainer_16_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_15Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-16k__trainer_15_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_14Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-16k__trainer_14_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_0Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-4k__trainer_0_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_1Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-4k__trainer_1_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_13Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-4k__trainer_13_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_12Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-4k__trainer_12_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_9Load this SAE standard pythia-70m-deduped/3-sae_bench-standard-res-4k__trainer_9_step_final blocks.3.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_0Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-4k__trainer_0_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_11Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-16k__trainer_11_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_10Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-16k__trainer_10_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_1Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-4k__trainer_1_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_13Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-4k__trainer_13_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_14Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-16k__trainer_14_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_16Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-4k__trainer_16_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_17Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-4k__trainer_17_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_18Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-16k__trainer_18_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_2Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-16k__trainer_2_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_20Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-4k__trainer_20_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_21Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-4k__trainer_21_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_12Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-4k__trainer_12_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_22Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-16k__trainer_22_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_3Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-16k__trainer_3_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_4Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-4k__trainer_4_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_5Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-4k__trainer_5_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_6Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-16k__trainer_6_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_7Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-16k__trainer_7_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_8Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-4k__trainer_8_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_9Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-4k__trainer_9_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_23Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-16k__trainer_23_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_15Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-16k__trainer_15_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_19Load this SAE standard pythia-70m-deduped/4-sae_bench-standard-res-16k__trainer_19_step_final blocks.4.hook_resid_post 16384 none"},{"location":"sae_table/#sae_bench_pythia70m_sweep_topk_ctx128_0730","title":"sae_bench_pythia70m_sweep_topk_ctx128_0730","text":"<ul> <li>Huggingface Repo: canrager/lm_sae</li> <li>model: pythia-70m-deduped</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.3.hook_resid_post__trainer_0Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-4k__trainer_0_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_1Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-4k__trainer_1_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_10Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-16k__trainer_10_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_11Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-16k__trainer_11_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_12Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-4k__trainer_12_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_13Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-4k__trainer_13_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_14Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-16k__trainer_14_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_22Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-16k__trainer_22_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_19Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-16k__trainer_19_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_2Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-16k__trainer_2_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_20Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-4k__trainer_20_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_21Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-4k__trainer_21_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_16Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-4k__trainer_16_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_23Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-16k__trainer_23_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_18Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-16k__trainer_18_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_3Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-16k__trainer_3_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_5Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-4k__trainer_5_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_6Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-16k__trainer_6_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_7Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-16k__trainer_7_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_8Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-4k__trainer_8_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_9Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-4k__trainer_9_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_15Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-16k__trainer_15_step_final blocks.3.hook_resid_post 16384 none blocks.3.hook_resid_post__trainer_4Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-4k__trainer_4_step_final blocks.3.hook_resid_post 4096 none blocks.3.hook_resid_post__trainer_17Load this SAE standard pythia-70m-deduped/3-sae_bench-topk-res-4k__trainer_17_step_final blocks.3.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_13Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-4k__trainer_13_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_14Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-16k__trainer_14_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_15Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-16k__trainer_15_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_16Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-4k__trainer_16_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_17Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-4k__trainer_17_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_18Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-16k__trainer_18_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_19Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-16k__trainer_19_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_2Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-16k__trainer_2_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_20Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-4k__trainer_20_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_21Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-4k__trainer_21_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_22Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-16k__trainer_22_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_23Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-16k__trainer_23_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_3Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-16k__trainer_3_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_4Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-4k__trainer_4_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_5Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-4k__trainer_5_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_6Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-16k__trainer_6_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_7Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-16k__trainer_7_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_12Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-4k__trainer_12_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_11Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-16k__trainer_11_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_10Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-16k__trainer_10_step_final blocks.4.hook_resid_post 16384 none blocks.4.hook_resid_post__trainer_1Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-4k__trainer_1_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_0Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-4k__trainer_0_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_9Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-4k__trainer_9_step_final blocks.4.hook_resid_post 4096 none blocks.4.hook_resid_post__trainer_8Load this SAE standard pythia-70m-deduped/4-sae_bench-topk-res-4k__trainer_8_step_final blocks.4.hook_resid_post 4096 none"},{"location":"sae_table/#gemma-2-2b-res-matryoshka-dc","title":"gemma-2-2b-res-matryoshka-dc","text":"<ul> <li>Huggingface Repo: chanind/gemma-2-2b-batch-topk-matryoshka-saes-w-32k-l0-40</li> <li>model: gemma-2-2b</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_resid_postLoad this SAE jumprelu blocks.0.hook_resid_post 32768 none blocks.1.hook_resid_postLoad this SAE jumprelu blocks.1.hook_resid_post 32768 none blocks.2.hook_resid_postLoad this SAE jumprelu blocks.2.hook_resid_post 32768 none blocks.3.hook_resid_postLoad this SAE jumprelu blocks.3.hook_resid_post 32768 none blocks.4.hook_resid_postLoad this SAE jumprelu blocks.4.hook_resid_post 32768 none blocks.5.hook_resid_postLoad this SAE jumprelu blocks.5.hook_resid_post 32768 none blocks.6.hook_resid_postLoad this SAE jumprelu blocks.6.hook_resid_post 32768 none blocks.7.hook_resid_postLoad this SAE jumprelu blocks.7.hook_resid_post 32768 none blocks.8.hook_resid_postLoad this SAE jumprelu blocks.8.hook_resid_post 32768 none blocks.9.hook_resid_postLoad this SAE jumprelu blocks.9.hook_resid_post 32768 none blocks.10.hook_resid_postLoad this SAE jumprelu blocks.10.hook_resid_post 32768 none blocks.11.hook_resid_postLoad this SAE jumprelu blocks.11.hook_resid_post 32768 none blocks.12.hook_resid_postLoad this SAE jumprelu gemma-2-2b/12-res-matryoshka-dc blocks.12.hook_resid_post 32768 none blocks.13.hook_resid_postLoad this SAE jumprelu gemma-2-2b/13-res-matryoshka-dc blocks.13.hook_resid_post 32768 none blocks.14.hook_resid_postLoad this SAE jumprelu gemma-2-2b/14-res-matryoshka-dc blocks.14.hook_resid_post 32768 none blocks.15.hook_resid_postLoad this SAE jumprelu gemma-2-2b/15-res-matryoshka-dc blocks.15.hook_resid_post 32768 none blocks.16.hook_resid_postLoad this SAE jumprelu gemma-2-2b/16-res-matryoshka-dc blocks.16.hook_resid_post 32768 none blocks.17.hook_resid_postLoad this SAE jumprelu gemma-2-2b/17-res-matryoshka-dc blocks.17.hook_resid_post 32768 none blocks.18.hook_resid_postLoad this SAE jumprelu gemma-2-2b/18-res-matryoshka-dc blocks.18.hook_resid_post 32768 none blocks.19.hook_resid_postLoad this SAE jumprelu gemma-2-2b/19-res-matryoshka-dc blocks.19.hook_resid_post 32768 none blocks.20.hook_resid_postLoad this SAE jumprelu gemma-2-2b/20-res-matryoshka-dc blocks.20.hook_resid_post 32768 none blocks.21.hook_resid_postLoad this SAE jumprelu gemma-2-2b/21-res-matryoshka-dc blocks.21.hook_resid_post 32768 none blocks.22.hook_resid_postLoad this SAE jumprelu gemma-2-2b/22-res-matryoshka-dc blocks.22.hook_resid_post 32768 none blocks.23.hook_resid_postLoad this SAE jumprelu gemma-2-2b/23-res-matryoshka-dc blocks.23.hook_resid_post 32768 none blocks.24.hook_resid_postLoad this SAE jumprelu gemma-2-2b/24-res-matryoshka-dc blocks.24.hook_resid_post 32768 none"},{"location":"sae_table/#gemma-2-2b-res-snap-matryoshka-dc","title":"gemma-2-2b-res-snap-matryoshka-dc","text":"<ul> <li>Huggingface Repo: chanind/gemma-2-2b-batch-topk-matryoshka-saes-w-32k-l0-40</li> <li>model: gemma-2-2b</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_resid_postLoad this SAE jumprelu blocks.0.hook_resid_post 32768 none blocks.1.hook_resid_postLoad this SAE jumprelu blocks.1.hook_resid_post 32768 none blocks.2.hook_resid_postLoad this SAE jumprelu blocks.2.hook_resid_post 32768 none blocks.3.hook_resid_postLoad this SAE jumprelu blocks.3.hook_resid_post 32768 none blocks.4.hook_resid_postLoad this SAE jumprelu blocks.4.hook_resid_post 32768 none blocks.5.hook_resid_postLoad this SAE jumprelu blocks.5.hook_resid_post 32768 none blocks.6.hook_resid_postLoad this SAE jumprelu blocks.6.hook_resid_post 32768 none blocks.7.hook_resid_postLoad this SAE jumprelu blocks.7.hook_resid_post 32768 none blocks.8.hook_resid_postLoad this SAE jumprelu blocks.8.hook_resid_post 32768 none blocks.9.hook_resid_postLoad this SAE jumprelu blocks.9.hook_resid_post 32768 none blocks.10.hook_resid_postLoad this SAE jumprelu blocks.10.hook_resid_post 32768 none blocks.11.hook_resid_postLoad this SAE jumprelu blocks.11.hook_resid_post 32768 none blocks.12.hook_resid_postLoad this SAE jumprelu blocks.12.hook_resid_post 32768 none blocks.13.hook_resid_postLoad this SAE jumprelu blocks.13.hook_resid_post 32768 none blocks.14.hook_resid_postLoad this SAE jumprelu blocks.14.hook_resid_post 32768 none blocks.15.hook_resid_postLoad this SAE jumprelu blocks.15.hook_resid_post 32768 none blocks.16.hook_resid_postLoad this SAE jumprelu blocks.16.hook_resid_post 32768 none blocks.17.hook_resid_postLoad this SAE jumprelu blocks.17.hook_resid_post 32768 none blocks.18.hook_resid_postLoad this SAE jumprelu blocks.18.hook_resid_post 32768 none blocks.19.hook_resid_postLoad this SAE jumprelu blocks.19.hook_resid_post 32768 none blocks.20.hook_resid_postLoad this SAE jumprelu blocks.20.hook_resid_post 32768 none blocks.21.hook_resid_postLoad this SAE jumprelu blocks.21.hook_resid_post 32768 none blocks.22.hook_resid_postLoad this SAE jumprelu blocks.22.hook_resid_post 32768 none blocks.23.hook_resid_postLoad this SAE jumprelu blocks.23.hook_resid_post 32768 none blocks.24.hook_resid_postLoad this SAE jumprelu blocks.24.hook_resid_post 32768 none"},{"location":"sae_table/#gemma-2-9b-res-matryoshka-dc","title":"gemma-2-9b-res-matryoshka-dc","text":"<ul> <li>Huggingface Repo: chanind/gemma-2-9b-batch-topk-matryoshka-saes-w-32k-l0-60</li> <li>model: gemma-2-9b</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_resid_postLoad this SAE jumprelu blocks.0.hook_resid_post 32768 none blocks.1.hook_resid_postLoad this SAE jumprelu blocks.1.hook_resid_post 32768 none blocks.2.hook_resid_postLoad this SAE jumprelu blocks.2.hook_resid_post 32768 none blocks.3.hook_resid_postLoad this SAE jumprelu blocks.3.hook_resid_post 32768 none blocks.4.hook_resid_postLoad this SAE jumprelu blocks.4.hook_resid_post 32768 none blocks.5.hook_resid_postLoad this SAE jumprelu blocks.5.hook_resid_post 32768 none blocks.6.hook_resid_postLoad this SAE jumprelu blocks.6.hook_resid_post 32768 none blocks.7.hook_resid_postLoad this SAE jumprelu blocks.7.hook_resid_post 32768 none blocks.8.hook_resid_postLoad this SAE jumprelu blocks.8.hook_resid_post 32768 none blocks.9.hook_resid_postLoad this SAE jumprelu blocks.9.hook_resid_post 32768 none blocks.10.hook_resid_postLoad this SAE jumprelu blocks.10.hook_resid_post 32768 none blocks.11.hook_resid_postLoad this SAE jumprelu blocks.11.hook_resid_post 32768 none blocks.12.hook_resid_postLoad this SAE jumprelu blocks.12.hook_resid_post 32768 none blocks.13.hook_resid_postLoad this SAE jumprelu blocks.13.hook_resid_post 32768 none blocks.14.hook_resid_postLoad this SAE jumprelu blocks.14.hook_resid_post 32768 none blocks.15.hook_resid_postLoad this SAE jumprelu blocks.15.hook_resid_post 32768 none blocks.16.hook_resid_postLoad this SAE jumprelu blocks.16.hook_resid_post 32768 none blocks.17.hook_resid_postLoad this SAE jumprelu blocks.17.hook_resid_post 32768 none blocks.18.hook_resid_postLoad this SAE jumprelu blocks.18.hook_resid_post 32768 none blocks.19.hook_resid_postLoad this SAE jumprelu blocks.19.hook_resid_post 32768 none blocks.20.hook_resid_postLoad this SAE jumprelu gemma-2-9b/20-res-matryoshka-dc blocks.20.hook_resid_post 32768 none blocks.21.hook_resid_postLoad this SAE jumprelu blocks.21.hook_resid_post 32768 none blocks.22.hook_resid_postLoad this SAE jumprelu blocks.22.hook_resid_post 32768 none blocks.23.hook_resid_postLoad this SAE jumprelu blocks.23.hook_resid_post 32768 none blocks.24.hook_resid_postLoad this SAE jumprelu blocks.24.hook_resid_post 32768 none blocks.25.hook_resid_postLoad this SAE jumprelu blocks.25.hook_resid_post 32768 none blocks.26.hook_resid_postLoad this SAE jumprelu blocks.26.hook_resid_post 32768 none blocks.27.hook_resid_postLoad this SAE jumprelu blocks.27.hook_resid_post 32768 none blocks.28.hook_resid_postLoad this SAE jumprelu blocks.28.hook_resid_post 32768 none blocks.29.hook_resid_postLoad this SAE jumprelu blocks.29.hook_resid_post 32768 none blocks.30.hook_resid_postLoad this SAE jumprelu blocks.30.hook_resid_post 32768 none blocks.31.hook_resid_postLoad this SAE jumprelu blocks.31.hook_resid_post 32768 none blocks.32.hook_resid_postLoad this SAE jumprelu blocks.32.hook_resid_post 32768 none blocks.33.hook_resid_postLoad this SAE jumprelu blocks.33.hook_resid_post 32768 none blocks.34.hook_resid_postLoad this SAE jumprelu blocks.34.hook_resid_post 32768 none blocks.35.hook_resid_postLoad this SAE jumprelu blocks.35.hook_resid_post 32768 none blocks.36.hook_resid_postLoad this SAE jumprelu blocks.36.hook_resid_post 32768 none blocks.37.hook_resid_postLoad this SAE jumprelu blocks.37.hook_resid_post 32768 none blocks.38.hook_resid_postLoad this SAE jumprelu blocks.38.hook_resid_post 32768 none blocks.39.hook_resid_postLoad this SAE jumprelu blocks.39.hook_resid_post 32768 none blocks.40.hook_resid_postLoad this SAE jumprelu blocks.40.hook_resid_post 32768 none"},{"location":"sae_table/#gemma-3-1b-res-matryoshka-dc","title":"gemma-3-1b-res-matryoshka-dc","text":"<ul> <li>Huggingface Repo: chanind/gemma-3-1b-batch-topk-matryoshka-saes-w-32k-l0-40</li> <li>model: gemma-3-1b</li> <li>Additional Links:<ul> <li>Model</li> </ul> </li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations blocks.0.hook_resid_postLoad this SAE jumprelu blocks.0.hook_resid_post 32768 none blocks.1.hook_resid_postLoad this SAE jumprelu blocks.1.hook_resid_post 32768 none blocks.2.hook_resid_postLoad this SAE jumprelu blocks.2.hook_resid_post 32768 none blocks.3.hook_resid_postLoad this SAE jumprelu blocks.3.hook_resid_post 32768 none blocks.4.hook_resid_postLoad this SAE jumprelu blocks.4.hook_resid_post 32768 none blocks.5.hook_resid_postLoad this SAE jumprelu blocks.5.hook_resid_post 32768 none blocks.6.hook_resid_postLoad this SAE jumprelu blocks.6.hook_resid_post 32768 none blocks.7.hook_resid_postLoad this SAE jumprelu blocks.7.hook_resid_post 32768 none blocks.8.hook_resid_postLoad this SAE jumprelu blocks.8.hook_resid_post 32768 none blocks.9.hook_resid_postLoad this SAE jumprelu blocks.9.hook_resid_post 32768 none blocks.10.hook_resid_postLoad this SAE jumprelu blocks.10.hook_resid_post 32768 none blocks.11.hook_resid_postLoad this SAE jumprelu blocks.11.hook_resid_post 32768 none blocks.12.hook_resid_postLoad this SAE jumprelu blocks.12.hook_resid_post 32768 none blocks.13.hook_resid_postLoad this SAE jumprelu blocks.13.hook_resid_post 32768 none blocks.14.hook_resid_postLoad this SAE jumprelu blocks.14.hook_resid_post 32768 none blocks.15.hook_resid_postLoad this SAE jumprelu blocks.15.hook_resid_post 32768 none blocks.16.hook_resid_postLoad this SAE jumprelu blocks.16.hook_resid_post 32768 none blocks.17.hook_resid_postLoad this SAE jumprelu blocks.17.hook_resid_post 32768 none blocks.18.hook_resid_postLoad this SAE jumprelu blocks.18.hook_resid_post 32768 none blocks.19.hook_resid_postLoad this SAE jumprelu blocks.19.hook_resid_post 32768 none blocks.20.hook_resid_postLoad this SAE jumprelu blocks.20.hook_resid_post 32768 none blocks.21.hook_resid_postLoad this SAE jumprelu blocks.21.hook_resid_post 32768 none blocks.22.hook_resid_postLoad this SAE jumprelu blocks.22.hook_resid_post 32768 none blocks.23.hook_resid_postLoad this SAE jumprelu blocks.23.hook_resid_post 32768 none blocks.24.hook_resid_postLoad this SAE jumprelu blocks.24.hook_resid_post 32768 none"},{"location":"sae_table/#gemma-scope-2b-pt-transcoders","title":"gemma-scope-2b-pt-transcoders","text":"<ul> <li>Huggingface Repo: google/gemma-scope-2b-pt-transcoders</li> <li>model: gemma-2-2b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0/width_16k/average_l0_76Load this SAE jumprelu_transcoder gemma-2-2b/0-gemmascope-transcoder-16k blocks.0.ln2.hook_normalized 16384 none layer_1/width_16k/average_l0_65Load this SAE jumprelu_transcoder gemma-2-2b/1-gemmascope-transcoder-16k blocks.1.ln2.hook_normalized 16384 none layer_2/width_16k/average_l0_49Load this SAE jumprelu_transcoder gemma-2-2b/2-gemmascope-transcoder-16k blocks.2.ln2.hook_normalized 16384 none layer_3/width_16k/average_l0_54Load this SAE jumprelu_transcoder gemma-2-2b/3-gemmascope-transcoder-16k blocks.3.ln2.hook_normalized 16384 none layer_4/width_16k/average_l0_88Load this SAE jumprelu_transcoder gemma-2-2b/4-gemmascope-transcoder-16k blocks.4.ln2.hook_normalized 16384 none layer_5/width_16k/average_l0_87Load this SAE jumprelu_transcoder gemma-2-2b/5-gemmascope-transcoder-16k blocks.5.ln2.hook_normalized 16384 none layer_6/width_16k/average_l0_95Load this SAE jumprelu_transcoder gemma-2-2b/6-gemmascope-transcoder-16k blocks.6.ln2.hook_normalized 16384 none layer_7/width_16k/average_l0_70Load this SAE jumprelu_transcoder gemma-2-2b/7-gemmascope-transcoder-16k blocks.7.ln2.hook_normalized 16384 none layer_8/width_16k/average_l0_52Load this SAE jumprelu_transcoder gemma-2-2b/8-gemmascope-transcoder-16k blocks.8.ln2.hook_normalized 16384 none layer_9/width_16k/average_l0_72Load this SAE jumprelu_transcoder gemma-2-2b/9-gemmascope-transcoder-16k blocks.9.ln2.hook_normalized 16384 none layer_10/width_16k/average_l0_88Load this SAE jumprelu_transcoder gemma-2-2b/10-gemmascope-transcoder-16k blocks.10.ln2.hook_normalized 16384 none layer_11/width_16k/average_l0_5Load this SAE jumprelu_transcoder gemma-2-2b/11-gemmascope-transcoder-16k blocks.11.ln2.hook_normalized 16384 none layer_12/width_16k/average_l0_6Load this SAE jumprelu_transcoder gemma-2-2b/12-gemmascope-transcoder-16k blocks.12.ln2.hook_normalized 16384 none layer_13/width_16k/average_l0_8Load this SAE jumprelu_transcoder gemma-2-2b/13-gemmascope-transcoder-16k blocks.13.ln2.hook_normalized 16384 none layer_14/width_16k/average_l0_8Load this SAE jumprelu_transcoder gemma-2-2b/14-gemmascope-transcoder-16k blocks.14.ln2.hook_normalized 16384 none layer_15/width_16k/average_l0_8Load this SAE jumprelu_transcoder gemma-2-2b/15-gemmascope-transcoder-16k blocks.15.ln2.hook_normalized 16384 none layer_16/width_16k/average_l0_10Load this SAE jumprelu_transcoder gemma-2-2b/16-gemmascope-transcoder-16k blocks.16.ln2.hook_normalized 16384 none layer_17/width_16k/average_l0_12Load this SAE jumprelu_transcoder gemma-2-2b/17-gemmascope-transcoder-16k blocks.17.ln2.hook_normalized 16384 none layer_18/width_16k/average_l0_13Load this SAE jumprelu_transcoder gemma-2-2b/18-gemmascope-transcoder-16k blocks.18.ln2.hook_normalized 16384 none layer_19/width_16k/average_l0_12Load this SAE jumprelu_transcoder gemma-2-2b/19-gemmascope-transcoder-16k blocks.19.ln2.hook_normalized 16384 none layer_20/width_16k/average_l0_11Load this SAE jumprelu_transcoder gemma-2-2b/20-gemmascope-transcoder-16k blocks.20.ln2.hook_normalized 16384 none layer_21/width_16k/average_l0_13Load this SAE jumprelu_transcoder gemma-2-2b/21-gemmascope-transcoder-16k blocks.21.ln2.hook_normalized 16384 none layer_22/width_16k/average_l0_15Load this SAE jumprelu_transcoder gemma-2-2b/22-gemmascope-transcoder-16k blocks.22.ln2.hook_normalized 16384 none layer_23/width_16k/average_l0_25Load this SAE jumprelu_transcoder gemma-2-2b/23-gemmascope-transcoder-16k blocks.23.ln2.hook_normalized 16384 none layer_24/width_16k/average_l0_37Load this SAE jumprelu_transcoder gemma-2-2b/24-gemmascope-transcoder-16k blocks.24.ln2.hook_normalized 16384 none layer_25/width_16k/average_l0_41Load this SAE jumprelu_transcoder gemma-2-2b/25-gemmascope-transcoder-16k blocks.25.ln2.hook_normalized 16384 none"},{"location":"sae_table/#mwhanna-qwen3-4b-transcoders","title":"mwhanna-qwen3-4b-transcoders","text":"<ul> <li>Huggingface Repo: mwhanna/qwen3-4b-transcoders</li> <li>model: qwen3-4b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0Load this SAE transcoder qwen3-4b/0-transcoder-hp blocks.0.mlp.hook_in 163840 none layer_1Load this SAE transcoder qwen3-4b/1-transcoder-hp blocks.1.mlp.hook_in 163840 none layer_2Load this SAE transcoder qwen3-4b/2-transcoder-hp blocks.2.mlp.hook_in 163840 none layer_3Load this SAE transcoder qwen3-4b/3-transcoder-hp blocks.3.mlp.hook_in 163840 none layer_4Load this SAE transcoder qwen3-4b/4-transcoder-hp blocks.4.mlp.hook_in 163840 none layer_5Load this SAE transcoder qwen3-4b/5-transcoder-hp blocks.5.mlp.hook_in 163840 none layer_6Load this SAE transcoder qwen3-4b/6-transcoder-hp blocks.6.mlp.hook_in 163840 none layer_7Load this SAE transcoder qwen3-4b/7-transcoder-hp blocks.7.mlp.hook_in 163840 none layer_8Load this SAE transcoder qwen3-4b/8-transcoder-hp blocks.8.mlp.hook_in 163840 none layer_9Load this SAE transcoder qwen3-4b/9-transcoder-hp blocks.9.mlp.hook_in 163840 none layer_10Load this SAE transcoder qwen3-4b/10-transcoder-hp blocks.10.mlp.hook_in 163840 none layer_11Load this SAE transcoder qwen3-4b/11-transcoder-hp blocks.11.mlp.hook_in 163840 none layer_12Load this SAE transcoder qwen3-4b/12-transcoder-hp blocks.12.mlp.hook_in 163840 none layer_13Load this SAE transcoder qwen3-4b/13-transcoder-hp blocks.13.mlp.hook_in 163840 none layer_14Load this SAE transcoder qwen3-4b/14-transcoder-hp blocks.14.mlp.hook_in 163840 none layer_15Load this SAE transcoder qwen3-4b/15-transcoder-hp blocks.15.mlp.hook_in 163840 none layer_16Load this SAE transcoder qwen3-4b/16-transcoder-hp blocks.16.mlp.hook_in 163840 none layer_17Load this SAE transcoder qwen3-4b/17-transcoder-hp blocks.17.mlp.hook_in 163840 none layer_18Load this SAE transcoder qwen3-4b/18-transcoder-hp blocks.18.mlp.hook_in 163840 none layer_19Load this SAE transcoder qwen3-4b/19-transcoder-hp blocks.19.mlp.hook_in 163840 none layer_20Load this SAE transcoder qwen3-4b/20-transcoder-hp blocks.20.mlp.hook_in 163840 none layer_21Load this SAE transcoder qwen3-4b/21-transcoder-hp blocks.21.mlp.hook_in 163840 none layer_22Load this SAE transcoder qwen3-4b/22-transcoder-hp blocks.22.mlp.hook_in 163840 none layer_23Load this SAE transcoder qwen3-4b/23-transcoder-hp blocks.23.mlp.hook_in 163840 none layer_24Load this SAE transcoder qwen3-4b/24-transcoder-hp blocks.24.mlp.hook_in 163840 none layer_25Load this SAE transcoder qwen3-4b/25-transcoder-hp blocks.25.mlp.hook_in 163840 none layer_26Load this SAE transcoder qwen3-4b/26-transcoder-hp blocks.26.mlp.hook_in 163840 none layer_27Load this SAE transcoder qwen3-4b/27-transcoder-hp blocks.27.mlp.hook_in 163840 none layer_28Load this SAE transcoder qwen3-4b/28-transcoder-hp blocks.28.mlp.hook_in 163840 none layer_29Load this SAE transcoder qwen3-4b/29-transcoder-hp blocks.29.mlp.hook_in 163840 none layer_30Load this SAE transcoder qwen3-4b/30-transcoder-hp blocks.30.mlp.hook_in 163840 none layer_31Load this SAE transcoder qwen3-4b/31-transcoder-hp blocks.31.mlp.hook_in 163840 none layer_32Load this SAE transcoder qwen3-4b/32-transcoder-hp blocks.32.mlp.hook_in 163840 none layer_33Load this SAE transcoder qwen3-4b/33-transcoder-hp blocks.33.mlp.hook_in 163840 none layer_34Load this SAE transcoder qwen3-4b/34-transcoder-hp blocks.34.mlp.hook_in 163840 none layer_35Load this SAE transcoder qwen3-4b/35-transcoder-hp blocks.35.mlp.hook_in 163840 none"},{"location":"sae_table/#mwhanna-qwen3-8b-transcoders","title":"mwhanna-qwen3-8b-transcoders","text":"<ul> <li>Huggingface Repo: mwhanna/qwen3-8b-transcoders</li> <li>model: qwen3-8b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0Load this SAE transcoder qwen3-8b/0-transcoder-hp blocks.0.mlp.hook_in 163840 none layer_1Load this SAE transcoder qwen3-8b/1-transcoder-hp blocks.1.mlp.hook_in 163840 none layer_2Load this SAE transcoder qwen3-8b/2-transcoder-hp blocks.2.mlp.hook_in 163840 none layer_3Load this SAE transcoder qwen3-8b/3-transcoder-hp blocks.3.mlp.hook_in 163840 none layer_4Load this SAE transcoder qwen3-8b/4-transcoder-hp blocks.4.mlp.hook_in 163840 none layer_5Load this SAE transcoder qwen3-8b/5-transcoder-hp blocks.5.mlp.hook_in 163840 none layer_6Load this SAE transcoder qwen3-8b/6-transcoder-hp blocks.6.mlp.hook_in 163840 none layer_7Load this SAE transcoder qwen3-8b/7-transcoder-hp blocks.7.mlp.hook_in 163840 none layer_8Load this SAE transcoder qwen3-8b/8-transcoder-hp blocks.8.mlp.hook_in 163840 none layer_9Load this SAE transcoder qwen3-8b/9-transcoder-hp blocks.9.mlp.hook_in 163840 none layer_10Load this SAE transcoder qwen3-8b/10-transcoder-hp blocks.10.mlp.hook_in 163840 none layer_11Load this SAE transcoder qwen3-8b/11-transcoder-hp blocks.11.mlp.hook_in 163840 none layer_12Load this SAE transcoder qwen3-8b/12-transcoder-hp blocks.12.mlp.hook_in 163840 none layer_13Load this SAE transcoder qwen3-8b/13-transcoder-hp blocks.13.mlp.hook_in 163840 none layer_14Load this SAE transcoder qwen3-8b/14-transcoder-hp blocks.14.mlp.hook_in 163840 none layer_15Load this SAE transcoder qwen3-8b/15-transcoder-hp blocks.15.mlp.hook_in 163840 none layer_16Load this SAE transcoder qwen3-8b/16-transcoder-hp blocks.16.mlp.hook_in 163840 none layer_17Load this SAE transcoder qwen3-8b/17-transcoder-hp blocks.17.mlp.hook_in 163840 none layer_18Load this SAE transcoder qwen3-8b/18-transcoder-hp blocks.18.mlp.hook_in 163840 none layer_19Load this SAE transcoder qwen3-8b/19-transcoder-hp blocks.19.mlp.hook_in 163840 none layer_20Load this SAE transcoder qwen3-8b/20-transcoder-hp blocks.20.mlp.hook_in 163840 none layer_21Load this SAE transcoder qwen3-8b/21-transcoder-hp blocks.21.mlp.hook_in 163840 none layer_22Load this SAE transcoder qwen3-8b/22-transcoder-hp blocks.22.mlp.hook_in 163840 none layer_23Load this SAE transcoder qwen3-8b/23-transcoder-hp blocks.23.mlp.hook_in 163840 none layer_24Load this SAE transcoder qwen3-8b/24-transcoder-hp blocks.24.mlp.hook_in 163840 none layer_25Load this SAE transcoder qwen3-8b/25-transcoder-hp blocks.25.mlp.hook_in 163840 none layer_26Load this SAE transcoder qwen3-8b/26-transcoder-hp blocks.26.mlp.hook_in 163840 none layer_27Load this SAE transcoder qwen3-8b/27-transcoder-hp blocks.27.mlp.hook_in 163840 none layer_28Load this SAE transcoder qwen3-8b/28-transcoder-hp blocks.28.mlp.hook_in 163840 none layer_29Load this SAE transcoder qwen3-8b/29-transcoder-hp blocks.29.mlp.hook_in 163840 none layer_30Load this SAE transcoder qwen3-8b/30-transcoder-hp blocks.30.mlp.hook_in 163840 none layer_31Load this SAE transcoder qwen3-8b/31-transcoder-hp blocks.31.mlp.hook_in 163840 none layer_32Load this SAE transcoder qwen3-8b/32-transcoder-hp blocks.32.mlp.hook_in 163840 none layer_33Load this SAE transcoder qwen3-8b/33-transcoder-hp blocks.33.mlp.hook_in 163840 none layer_34Load this SAE transcoder qwen3-8b/34-transcoder-hp blocks.34.mlp.hook_in 163840 none layer_35Load this SAE transcoder qwen3-8b/35-transcoder-hp blocks.35.mlp.hook_in 163840 none"},{"location":"sae_table/#mwhanna-qwen3-14b-transcoders","title":"mwhanna-qwen3-14b-transcoders","text":"<ul> <li>Huggingface Repo: mwhanna/qwen3-14b-transcoders</li> <li>model: qwen3-14b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0Load this SAE transcoder qwen3-14b/0-transcoder-hp blocks.0.mlp.hook_in 163840 none layer_1Load this SAE transcoder qwen3-14b/1-transcoder-hp blocks.1.mlp.hook_in 163840 none layer_2Load this SAE transcoder qwen3-14b/2-transcoder-hp blocks.2.mlp.hook_in 163840 none layer_3Load this SAE transcoder qwen3-14b/3-transcoder-hp blocks.3.mlp.hook_in 163840 none layer_4Load this SAE transcoder qwen3-14b/4-transcoder-hp blocks.4.mlp.hook_in 163840 none layer_5Load this SAE transcoder qwen3-14b/5-transcoder-hp blocks.5.mlp.hook_in 163840 none layer_6Load this SAE transcoder qwen3-14b/6-transcoder-hp blocks.6.mlp.hook_in 163840 none layer_7Load this SAE transcoder qwen3-14b/7-transcoder-hp blocks.7.mlp.hook_in 163840 none layer_8Load this SAE transcoder qwen3-14b/8-transcoder-hp blocks.8.mlp.hook_in 163840 none layer_9Load this SAE transcoder qwen3-14b/9-transcoder-hp blocks.9.mlp.hook_in 163840 none layer_10Load this SAE transcoder qwen3-14b/10-transcoder-hp blocks.10.mlp.hook_in 163840 none layer_11Load this SAE transcoder qwen3-14b/11-transcoder-hp blocks.11.mlp.hook_in 163840 none layer_12Load this SAE transcoder qwen3-14b/12-transcoder-hp blocks.12.mlp.hook_in 163840 none layer_13Load this SAE transcoder qwen3-14b/13-transcoder-hp blocks.13.mlp.hook_in 163840 none layer_14Load this SAE transcoder qwen3-14b/14-transcoder-hp blocks.14.mlp.hook_in 163840 none layer_15Load this SAE transcoder qwen3-14b/15-transcoder-hp blocks.15.mlp.hook_in 163840 none layer_16Load this SAE transcoder qwen3-14b/16-transcoder-hp blocks.16.mlp.hook_in 163840 none layer_17Load this SAE transcoder qwen3-14b/17-transcoder-hp blocks.17.mlp.hook_in 163840 none layer_18Load this SAE transcoder qwen3-14b/18-transcoder-hp blocks.18.mlp.hook_in 163840 none layer_19Load this SAE transcoder qwen3-14b/19-transcoder-hp blocks.19.mlp.hook_in 163840 none layer_20Load this SAE transcoder qwen3-14b/20-transcoder-hp blocks.20.mlp.hook_in 163840 none layer_21Load this SAE transcoder qwen3-14b/21-transcoder-hp blocks.21.mlp.hook_in 163840 none layer_22Load this SAE transcoder qwen3-14b/22-transcoder-hp blocks.22.mlp.hook_in 163840 none layer_23Load this SAE transcoder qwen3-14b/23-transcoder-hp blocks.23.mlp.hook_in 163840 none layer_24Load this SAE transcoder qwen3-14b/24-transcoder-hp blocks.24.mlp.hook_in 163840 none layer_25Load this SAE transcoder qwen3-14b/25-transcoder-hp blocks.25.mlp.hook_in 163840 none layer_26Load this SAE transcoder qwen3-14b/26-transcoder-hp blocks.26.mlp.hook_in 163840 none layer_27Load this SAE transcoder qwen3-14b/27-transcoder-hp blocks.27.mlp.hook_in 163840 none layer_28Load this SAE transcoder qwen3-14b/28-transcoder-hp blocks.28.mlp.hook_in 163840 none layer_29Load this SAE transcoder qwen3-14b/29-transcoder-hp blocks.29.mlp.hook_in 163840 none layer_30Load this SAE transcoder qwen3-14b/30-transcoder-hp blocks.30.mlp.hook_in 163840 none layer_31Load this SAE transcoder qwen3-14b/31-transcoder-hp blocks.31.mlp.hook_in 163840 none layer_32Load this SAE transcoder qwen3-14b/32-transcoder-hp blocks.32.mlp.hook_in 163840 none layer_33Load this SAE transcoder qwen3-14b/33-transcoder-hp blocks.33.mlp.hook_in 163840 none layer_34Load this SAE transcoder qwen3-14b/34-transcoder-hp blocks.34.mlp.hook_in 163840 none layer_35Load this SAE transcoder qwen3-14b/35-transcoder-hp blocks.35.mlp.hook_in 163840 none layer_36Load this SAE transcoder qwen3-14b/36-transcoder-hp blocks.36.mlp.hook_in 163840 none layer_37Load this SAE transcoder qwen3-14b/37-transcoder-hp blocks.37.mlp.hook_in 163840 none layer_38Load this SAE transcoder qwen3-14b/38-transcoder-hp blocks.38.mlp.hook_in 163840 none layer_39Load this SAE transcoder qwen3-14b/39-transcoder-hp blocks.39.mlp.hook_in 163840 none"},{"location":"sae_table/#mwhanna-qwen3-14b-transcoders-lowl0","title":"mwhanna-qwen3-14b-transcoders-lowl0","text":"<ul> <li>Huggingface Repo: mwhanna/qwen3-14b-transcoders-lowl0</li> <li>model: qwen3-14b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0Load this SAE transcoder qwen3-14b/0-transcoder-hp-lowl0 blocks.0.mlp.hook_in 163840 none layer_1Load this SAE transcoder qwen3-14b/1-transcoder-hp-lowl0 blocks.1.mlp.hook_in 163840 none layer_2Load this SAE transcoder qwen3-14b/2-transcoder-hp-lowl0 blocks.2.mlp.hook_in 163840 none layer_3Load this SAE transcoder qwen3-14b/3-transcoder-hp-lowl0 blocks.3.mlp.hook_in 163840 none layer_4Load this SAE transcoder qwen3-14b/4-transcoder-hp-lowl0 blocks.4.mlp.hook_in 163840 none layer_5Load this SAE transcoder qwen3-14b/5-transcoder-hp-lowl0 blocks.5.mlp.hook_in 163840 none layer_6Load this SAE transcoder qwen3-14b/6-transcoder-hp-lowl0 blocks.6.mlp.hook_in 163840 none layer_7Load this SAE transcoder qwen3-14b/7-transcoder-hp-lowl0 blocks.7.mlp.hook_in 163840 none layer_8Load this SAE transcoder qwen3-14b/8-transcoder-hp-lowl0 blocks.8.mlp.hook_in 163840 none layer_9Load this SAE transcoder qwen3-14b/9-transcoder-hp-lowl0 blocks.9.mlp.hook_in 163840 none layer_10Load this SAE transcoder qwen3-14b/10-transcoder-hp-lowl0 blocks.10.mlp.hook_in 163840 none layer_11Load this SAE transcoder qwen3-14b/11-transcoder-hp-lowl0 blocks.11.mlp.hook_in 163840 none layer_12Load this SAE transcoder qwen3-14b/12-transcoder-hp-lowl0 blocks.12.mlp.hook_in 163840 none layer_13Load this SAE transcoder qwen3-14b/13-transcoder-hp-lowl0 blocks.13.mlp.hook_in 163840 none layer_14Load this SAE transcoder qwen3-14b/14-transcoder-hp-lowl0 blocks.14.mlp.hook_in 163840 none layer_15Load this SAE transcoder qwen3-14b/15-transcoder-hp-lowl0 blocks.15.mlp.hook_in 163840 none layer_16Load this SAE transcoder qwen3-14b/16-transcoder-hp-lowl0 blocks.16.mlp.hook_in 163840 none layer_17Load this SAE transcoder qwen3-14b/17-transcoder-hp-lowl0 blocks.17.mlp.hook_in 163840 none layer_18Load this SAE transcoder qwen3-14b/18-transcoder-hp-lowl0 blocks.18.mlp.hook_in 163840 none layer_19Load this SAE transcoder qwen3-14b/19-transcoder-hp-lowl0 blocks.19.mlp.hook_in 163840 none layer_20Load this SAE transcoder qwen3-14b/20-transcoder-hp-lowl0 blocks.20.mlp.hook_in 163840 none layer_21Load this SAE transcoder qwen3-14b/21-transcoder-hp-lowl0 blocks.21.mlp.hook_in 163840 none layer_22Load this SAE transcoder qwen3-14b/22-transcoder-hp-lowl0 blocks.22.mlp.hook_in 163840 none layer_23Load this SAE transcoder qwen3-14b/23-transcoder-hp-lowl0 blocks.23.mlp.hook_in 163840 none layer_24Load this SAE transcoder qwen3-14b/24-transcoder-hp-lowl0 blocks.24.mlp.hook_in 163840 none layer_25Load this SAE transcoder qwen3-14b/25-transcoder-hp-lowl0 blocks.25.mlp.hook_in 163840 none layer_26Load this SAE transcoder qwen3-14b/26-transcoder-hp-lowl0 blocks.26.mlp.hook_in 163840 none layer_27Load this SAE transcoder qwen3-14b/27-transcoder-hp-lowl0 blocks.27.mlp.hook_in 163840 none layer_28Load this SAE transcoder qwen3-14b/28-transcoder-hp-lowl0 blocks.28.mlp.hook_in 163840 none layer_29Load this SAE transcoder qwen3-14b/29-transcoder-hp-lowl0 blocks.29.mlp.hook_in 163840 none layer_30Load this SAE transcoder qwen3-14b/30-transcoder-hp-lowl0 blocks.30.mlp.hook_in 163840 none layer_31Load this SAE transcoder qwen3-14b/31-transcoder-hp-lowl0 blocks.31.mlp.hook_in 163840 none layer_32Load this SAE transcoder qwen3-14b/32-transcoder-hp-lowl0 blocks.32.mlp.hook_in 163840 none layer_33Load this SAE transcoder qwen3-14b/33-transcoder-hp-lowl0 blocks.33.mlp.hook_in 163840 none layer_34Load this SAE transcoder qwen3-14b/34-transcoder-hp-lowl0 blocks.34.mlp.hook_in 163840 none layer_35Load this SAE transcoder qwen3-14b/35-transcoder-hp-lowl0 blocks.35.mlp.hook_in 163840 none layer_36Load this SAE transcoder qwen3-14b/36-transcoder-hp-lowl0 blocks.36.mlp.hook_in 163840 none layer_37Load this SAE transcoder qwen3-14b/37-transcoder-hp-lowl0 blocks.37.mlp.hook_in 163840 none layer_38Load this SAE transcoder qwen3-14b/38-transcoder-hp-lowl0 blocks.38.mlp.hook_in 163840 none layer_39Load this SAE transcoder qwen3-14b/39-transcoder-hp-lowl0 blocks.39.mlp.hook_in 163840 none"},{"location":"sae_table/#mwhanna-qwen3-17b-transcoders-lowl0","title":"mwhanna-qwen3-1.7b-transcoders-lowl0","text":"<ul> <li>Huggingface Repo: mwhanna/qwen3-1.7b-transcoders-lowl0</li> <li>model: qwen3-1.7b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0Load this SAE transcoder qwen3-1.7b/0-transcoder-hp-lowl0 blocks.0.mlp.hook_in 163840 none layer_1Load this SAE transcoder qwen3-1.7b/1-transcoder-hp-lowl0 blocks.1.mlp.hook_in 163840 none layer_2Load this SAE transcoder qwen3-1.7b/2-transcoder-hp-lowl0 blocks.2.mlp.hook_in 163840 none layer_3Load this SAE transcoder qwen3-1.7b/3-transcoder-hp-lowl0 blocks.3.mlp.hook_in 163840 none layer_4Load this SAE transcoder qwen3-1.7b/4-transcoder-hp-lowl0 blocks.4.mlp.hook_in 163840 none layer_5Load this SAE transcoder qwen3-1.7b/5-transcoder-hp-lowl0 blocks.5.mlp.hook_in 163840 none layer_6Load this SAE transcoder qwen3-1.7b/6-transcoder-hp-lowl0 blocks.6.mlp.hook_in 163840 none layer_7Load this SAE transcoder qwen3-1.7b/7-transcoder-hp-lowl0 blocks.7.mlp.hook_in 163840 none layer_8Load this SAE transcoder qwen3-1.7b/8-transcoder-hp-lowl0 blocks.8.mlp.hook_in 163840 none layer_9Load this SAE transcoder qwen3-1.7b/9-transcoder-hp-lowl0 blocks.9.mlp.hook_in 163840 none layer_10Load this SAE transcoder qwen3-1.7b/10-transcoder-hp-lowl0 blocks.10.mlp.hook_in 163840 none layer_11Load this SAE transcoder qwen3-1.7b/11-transcoder-hp-lowl0 blocks.11.mlp.hook_in 163840 none layer_12Load this SAE transcoder qwen3-1.7b/12-transcoder-hp-lowl0 blocks.12.mlp.hook_in 163840 none layer_13Load this SAE transcoder qwen3-1.7b/13-transcoder-hp-lowl0 blocks.13.mlp.hook_in 163840 none layer_14Load this SAE transcoder qwen3-1.7b/14-transcoder-hp-lowl0 blocks.14.mlp.hook_in 163840 none layer_15Load this SAE transcoder qwen3-1.7b/15-transcoder-hp-lowl0 blocks.15.mlp.hook_in 163840 none layer_16Load this SAE transcoder qwen3-1.7b/16-transcoder-hp-lowl0 blocks.16.mlp.hook_in 163840 none layer_17Load this SAE transcoder qwen3-1.7b/17-transcoder-hp-lowl0 blocks.17.mlp.hook_in 163840 none layer_18Load this SAE transcoder qwen3-1.7b/18-transcoder-hp-lowl0 blocks.18.mlp.hook_in 163840 none layer_19Load this SAE transcoder qwen3-1.7b/19-transcoder-hp-lowl0 blocks.19.mlp.hook_in 163840 none layer_20Load this SAE transcoder qwen3-1.7b/20-transcoder-hp-lowl0 blocks.20.mlp.hook_in 163840 none layer_21Load this SAE transcoder qwen3-1.7b/21-transcoder-hp-lowl0 blocks.21.mlp.hook_in 163840 none layer_22Load this SAE transcoder qwen3-1.7b/22-transcoder-hp-lowl0 blocks.22.mlp.hook_in 163840 none layer_23Load this SAE transcoder qwen3-1.7b/23-transcoder-hp-lowl0 blocks.23.mlp.hook_in 163840 none layer_24Load this SAE transcoder qwen3-1.7b/24-transcoder-hp-lowl0 blocks.24.mlp.hook_in 163840 none layer_25Load this SAE transcoder qwen3-1.7b/25-transcoder-hp-lowl0 blocks.25.mlp.hook_in 163840 none layer_26Load this SAE transcoder qwen3-1.7b/26-transcoder-hp-lowl0 blocks.26.mlp.hook_in 163840 none layer_27Load this SAE transcoder qwen3-1.7b/27-transcoder-hp-lowl0 blocks.27.mlp.hook_in 163840 none"},{"location":"sae_table/#mwhanna-qwen3-06b-transcoders-lowl0","title":"mwhanna-qwen3-0.6b-transcoders-lowl0","text":"<ul> <li>Huggingface Repo: mwhanna/qwen3-0.6b-transcoders-lowl0</li> <li>model: qwen3-0.6b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0Load this SAE transcoder qwen3-0.6b/0-transcoder-hp-lowl0 blocks.0.mlp.hook_in 163840 none layer_1Load this SAE transcoder qwen3-0.6b/1-transcoder-hp-lowl0 blocks.1.mlp.hook_in 163840 none layer_2Load this SAE transcoder qwen3-0.6b/2-transcoder-hp-lowl0 blocks.2.mlp.hook_in 163840 none layer_3Load this SAE transcoder qwen3-0.6b/3-transcoder-hp-lowl0 blocks.3.mlp.hook_in 163840 none layer_4Load this SAE transcoder qwen3-0.6b/4-transcoder-hp-lowl0 blocks.4.mlp.hook_in 163840 none layer_5Load this SAE transcoder qwen3-0.6b/5-transcoder-hp-lowl0 blocks.5.mlp.hook_in 163840 none layer_6Load this SAE transcoder qwen3-0.6b/6-transcoder-hp-lowl0 blocks.6.mlp.hook_in 163840 none layer_7Load this SAE transcoder qwen3-0.6b/7-transcoder-hp-lowl0 blocks.7.mlp.hook_in 163840 none layer_8Load this SAE transcoder qwen3-0.6b/8-transcoder-hp-lowl0 blocks.8.mlp.hook_in 163840 none layer_9Load this SAE transcoder qwen3-0.6b/9-transcoder-hp-lowl0 blocks.9.mlp.hook_in 163840 none layer_10Load this SAE transcoder qwen3-0.6b/10-transcoder-hp-lowl0 blocks.10.mlp.hook_in 163840 none layer_11Load this SAE transcoder qwen3-0.6b/11-transcoder-hp-lowl0 blocks.11.mlp.hook_in 163840 none layer_12Load this SAE transcoder qwen3-0.6b/12-transcoder-hp-lowl0 blocks.12.mlp.hook_in 163840 none layer_13Load this SAE transcoder qwen3-0.6b/13-transcoder-hp-lowl0 blocks.13.mlp.hook_in 163840 none layer_14Load this SAE transcoder qwen3-0.6b/14-transcoder-hp-lowl0 blocks.14.mlp.hook_in 163840 none layer_15Load this SAE transcoder qwen3-0.6b/15-transcoder-hp-lowl0 blocks.15.mlp.hook_in 163840 none layer_16Load this SAE transcoder qwen3-0.6b/16-transcoder-hp-lowl0 blocks.16.mlp.hook_in 163840 none layer_17Load this SAE transcoder qwen3-0.6b/17-transcoder-hp-lowl0 blocks.17.mlp.hook_in 163840 none layer_18Load this SAE transcoder qwen3-0.6b/18-transcoder-hp-lowl0 blocks.18.mlp.hook_in 163840 none layer_19Load this SAE transcoder qwen3-0.6b/19-transcoder-hp-lowl0 blocks.19.mlp.hook_in 163840 none layer_20Load this SAE transcoder qwen3-0.6b/20-transcoder-hp-lowl0 blocks.20.mlp.hook_in 163840 none layer_21Load this SAE transcoder qwen3-0.6b/21-transcoder-hp-lowl0 blocks.21.mlp.hook_in 163840 none layer_22Load this SAE transcoder qwen3-0.6b/22-transcoder-hp-lowl0 blocks.22.mlp.hook_in 163840 none layer_23Load this SAE transcoder qwen3-0.6b/23-transcoder-hp-lowl0 blocks.23.mlp.hook_in 163840 none layer_24Load this SAE transcoder qwen3-0.6b/24-transcoder-hp-lowl0 blocks.24.mlp.hook_in 163840 none layer_25Load this SAE transcoder qwen3-0.6b/25-transcoder-hp-lowl0 blocks.25.mlp.hook_in 163840 none layer_26Load this SAE transcoder qwen3-0.6b/26-transcoder-hp-lowl0 blocks.26.mlp.hook_in 163840 none layer_27Load this SAE transcoder qwen3-0.6b/27-transcoder-hp-lowl0 blocks.27.mlp.hook_in 163840 none"},{"location":"sae_table/#mntss-gemma-2-2b-25m-clt-as-per-layer","title":"mntss-gemma-2-2b-2.5m-clt-as-per-layer","text":"<ul> <li>Huggingface Repo: mntss/clt-gemma-2-2b-2.5M</li> <li>model: gemma-2-2b</li> </ul> id architecture neuronpedia hook_name d_sae normalize_activations layer_0Load this SAE transcoder gemma-2-2b/0-clt-hp blocks.0.hook_resid_mid 98304 none layer_1Load this SAE transcoder gemma-2-2b/1-clt-hp blocks.1.hook_resid_mid 98304 none layer_2Load this SAE transcoder gemma-2-2b/2-clt-hp blocks.2.hook_resid_mid 98304 none layer_3Load this SAE transcoder gemma-2-2b/3-clt-hp blocks.3.hook_resid_mid 98304 none layer_4Load this SAE transcoder gemma-2-2b/4-clt-hp blocks.4.hook_resid_mid 98304 none layer_5Load this SAE transcoder gemma-2-2b/5-clt-hp blocks.5.hook_resid_mid 98304 none layer_6Load this SAE transcoder gemma-2-2b/6-clt-hp blocks.6.hook_resid_mid 98304 none layer_7Load this SAE transcoder gemma-2-2b/7-clt-hp blocks.7.hook_resid_mid 98304 none layer_8Load this SAE transcoder gemma-2-2b/8-clt-hp blocks.8.hook_resid_mid 98304 none layer_9Load this SAE transcoder gemma-2-2b/9-clt-hp blocks.9.hook_resid_mid 98304 none layer_10Load this SAE transcoder gemma-2-2b/10-clt-hp blocks.10.hook_resid_mid 98304 none layer_11Load this SAE transcoder gemma-2-2b/11-clt-hp blocks.11.hook_resid_mid 98304 none layer_12Load this SAE transcoder gemma-2-2b/12-clt-hp blocks.12.hook_resid_mid 98304 none layer_13Load this SAE transcoder gemma-2-2b/13-clt-hp blocks.13.hook_resid_mid 98304 none layer_14Load this SAE transcoder gemma-2-2b/14-clt-hp blocks.14.hook_resid_mid 98304 none layer_15Load this SAE transcoder gemma-2-2b/15-clt-hp blocks.15.hook_resid_mid 98304 none layer_16Load this SAE transcoder gemma-2-2b/16-clt-hp blocks.16.hook_resid_mid 98304 none layer_17Load this SAE transcoder gemma-2-2b/17-clt-hp blocks.17.hook_resid_mid 98304 none layer_18Load this SAE transcoder gemma-2-2b/18-clt-hp blocks.18.hook_resid_mid 98304 none layer_19Load this SAE transcoder gemma-2-2b/19-clt-hp blocks.19.hook_resid_mid 98304 none layer_20Load this SAE transcoder gemma-2-2b/20-clt-hp blocks.20.hook_resid_mid 98304 none layer_21Load this SAE transcoder gemma-2-2b/21-clt-hp blocks.21.hook_resid_mid 98304 none layer_22Load this SAE transcoder gemma-2-2b/22-clt-hp blocks.22.hook_resid_mid 98304 none layer_23Load this SAE transcoder gemma-2-2b/23-clt-hp blocks.23.hook_resid_mid 98304 none layer_24Load this SAE transcoder gemma-2-2b/24-clt-hp blocks.24.hook_resid_mid 98304 none layer_25Load this SAE transcoder gemma-2-2b/25-clt-hp blocks.25.hook_resid_mid 98304 none \u00d7 Copy Code"},{"location":"training_saes/","title":"Training Sparse Autoencoders","text":"<p>Methods development for training SAEs is rapidly evolving, so these docs may change frequently. For all available training options, see the LanguageModelSAERunnerConfig and the architecture-specific configuration classes it uses (e.g., StandardTrainingSAEConfig, GatedTrainingSAEConfig, JumpReLUTrainingSAEConfig, and TopKTrainingSAEConfig).</p> <p>However, we are attempting to maintain this tutorial .</p> <p>We encourage readers to join the Open Source Mechanistic Interpretability Slack for support!</p>"},{"location":"training_saes/#basic-training-setup","title":"Basic training setup","text":"<p>Training a SAE is done using the LanguageModelSAETrainingRunner class. This class is configured using a LanguageModelSAERunnerConfig. The <code>LanguageModelSAERunnerConfig</code> holds parameters for the overall training run (like model, dataset, and learning rate), and it contains an <code>sae</code> field. This <code>sae</code> field should be an instance of an architecture-specific SAE configuration dataclass (e.g., <code>StandardTrainingSAEConfig</code> for standard SAEs, <code>TopKTrainingSAEConfig</code> for TopK SAEs, etc.), which holds parameters specific to the SAE's structure and sparsity mechanisms.</p> <p>When using the command-line interface (CLI), you typically specify an <code>--architecture</code> argument (e.g., <code>\"standard\"</code>, <code>\"gated\"</code>, <code>\"jumprelu\"</code>, <code>\"topk\"</code>), and the runner constructs the appropriate nested SAE configuration. When instantiating <code>LanguageModelSAERunnerConfig</code> programmatically, you should directly provide the configured SAE object to the <code>sae</code> field. The CLI can be run using <code>python -m sae_lens.llm_sae_training_runner</code>.</p> <p>Some of the core config options available in <code>LanguageModelSAERunnerConfig</code> are:</p> <ul> <li><code>model_name</code>: The base model name to train a SAE on (e.g., <code>\"gpt2-small\"</code>, <code>\"tiny-stories-1L-21M\"</code>). This must correspond to a model from TransformerLens or a Hugging Face <code>AutoModelForCausalLM</code> if <code>model_class_name</code> is set accordingly.</li> <li><code>hook_name</code>: This is a TransformerLens hook in the model where our SAE will be trained from (e.g., <code>\"blocks.0.hook_mlp_out\"</code>). More info on hooks can be found here.</li> <li><code>dataset_path</code>: The path to a dataset on Huggingface for training (e.g., <code>\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\"</code>).</li> <li><code>training_tokens</code>: The total number of tokens from the dataset to use for training the SAE.</li> <li><code>train_batch_size_tokens</code>: The batch size used for training the SAE, measured in tokens. Adjust this to keep the GPU saturated.</li> <li><code>model_from_pretrained_kwargs</code>: A dictionary of keyword arguments to pass to <code>HookedTransformer.from_pretrained</code> when loading the model. It's often best to set <code>\"center_writing_weights\": False</code>.</li> <li><code>lr</code>: The learning rate for the optimizer.</li> <li><code>context_size</code>: The sequence length of prompts fed to the model to generate activations.</li> </ul> <p>Core options typically configured within the architecture-specific <code>sae</code> object (e.g., <code>cfg.sae = StandardTrainingSAEConfig(...)</code>):</p> <ul> <li><code>d_in</code>: The input dimensionality of the SAE. This must match the size of the activations at <code>hook_name</code>.</li> <li><code>d_sae</code>: The SAE's hidden layer dimensionality .</li> <li>Sparsity control parameters: These vary by architecture:</li> <li>For Standard SAEs: <code>l1_coefficient</code> (controls L1 penalty), <code>lp_norm</code> (e.g., 1.0 for L1, 0.7 for L0.7), <code>l1_warm_up_steps</code>.</li> <li>For Gated SAEs: <code>l1_coefficient</code> (controls L1-like penalty on gate activations), <code>l1_warm_up_steps</code>.</li> <li>For JumpReLU SAEs: <code>l0_coefficient</code> (controls L0-like penalty), <code>l0_warm_up_steps</code>, <code>jumprelu_init_threshold</code>, <code>jumprelu_bandwidth</code>.</li> <li>For TopK and BatchTopK SAEs: <code>k</code> (the number of features to keep active). Sparsity is enforced structurally.</li> <li><code>normalize_activations</code>: Strategy for normalizing activations before they enter the SAE (e.g., <code>\"expected_average_only_in\"</code>).</li> </ul> <p>A sample training run from the tutorial is shown below. Note how SAE-specific parameters are nested within the <code>sae</code> field:</p> <pre><code>import torch\nfrom sae_lens import (\n    LanguageModelSAERunnerConfig,\n    LanguageModelSAETrainingRunner,\n    StandardTrainingSAEConfig,\n    LoggingConfig,\n)\n\n# Define total training steps and batch size\ntotal_training_steps = 30_000\nbatch_size = 4096\ntotal_training_tokens = total_training_steps * batch_size\n\n# Learning rate and L1 warmup schedules\nlr_warm_up_steps = 0\nlr_decay_steps = total_training_steps // 5  # 20% of training\nl1_warm_up_steps = total_training_steps // 20  # 5% of training\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ncfg = LanguageModelSAERunnerConfig(\n    # Data Generating Function (Model + Training Distribution)\n    model_name=\"tiny-stories-1L-21M\",\n    hook_name=\"blocks.0.hook_mlp_out\",\n    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",\n    is_dataset_tokenized=True,\n    streaming=True,\n\n    # SAE Parameters are in the nested 'sae' config\n    sae=StandardTrainingSAEConfig(\n        d_in=1024, # Matches hook_mlp_out for tiny-stories-1L-21M\n        d_sae=16 * 1024,\n        apply_b_dec_to_input=True,\n        normalize_activations=\"expected_average_only_in\",\n        l1_coefficient=5,\n        l1_warm_up_steps=l1_warm_up_steps,\n    ),\n\n    # Training Parameters\n    lr=5e-5,\n    lr_warm_up_steps=lr_warm_up_steps,\n    lr_decay_steps=lr_decay_steps,\n    train_batch_size_tokens=batch_size,\n\n    # Activation Store Parameters\n    context_size=256,\n    n_batches_in_buffer=64,\n    training_tokens=total_training_tokens,\n    store_batch_size_prompts=16,\n\n    # WANDB\n    logger=LoggingConfig(\n        log_to_wandb=True,\n        wandb_project=\"sae_lens_tutorial\",\n        wandb_log_frequency=30,\n        eval_every_n_wandb_logs=20,\n    ),\n\n    # Misc\n    device=device,\n    seed=42,\n    n_checkpoints=0,\n    checkpoint_path=\"checkpoints\",\n    dtype=\"float32\"\n)\nsparse_autoencoder = LanguageModelSAETrainingRunner(cfg).run()\n</code></pre> <p>As you can see, the training setup provides a large number of options to explore. The full list of options can be found by inspecting the <code>LanguageModelSAERunnerConfig</code> class and the specific SAE configuration class you intend to use (e.g., <code>StandardTrainingSAEConfig</code>, <code>TopKTrainingSAEConfig</code>, etc.).</p>"},{"location":"training_saes/#training-topk-saes","title":"Training Topk SAEs","text":"<p>By default, SAELens will train SAEs using a L1 loss term with ReLU activation. A popular alternative architecture is the TopK architecture, which fixes the L0 of the SAE using a TopK activation function. To train a TopK SAE programmatically, you provide a <code>TopKTrainingSAEConfig</code> instance to the <code>sae</code> field. The primary parameter for TopK SAEs is <code>k</code>, the number of features to keep active. If not set, <code>k</code> defaults to 100 in <code>TopKTrainingSAEConfig</code>. The TopK architecture does not use an <code>l1_coefficient</code> or <code>lp_norm</code> for sparsity, as sparsity is structurally enforced.</p> <pre><code>from sae_lens import LanguageModelSAERunnerConfig, LanguageModelSAETrainingRunner, TopKTrainingSAEConfig\n\ncfg = LanguageModelSAERunnerConfig( # Full config would be defined here\n    # ... other LanguageModelSAERunnerConfig parameters ...\n    sae=TopKTrainingSAEConfig(\n        k=100, # Set the number of active features\n        d_in=1024, # Must match your hook point\n        d_sae=16 * 1024,\n        # ... other common SAE parameters from SAEConfig if needed ...\n    ),\n    # ...\n)\nsparse_autoencoder = LanguageModelSAETrainingRunner(cfg).run()\n</code></pre>"},{"location":"training_saes/#training-batchtopk-saes","title":"Training BatchTopk SAEs","text":"<p>A more modern version of the TopK SAE is the BatchTopK architecture, which fixes the mean L0 across a training batch rather than fixing the L0 for every sample in the batch. To train a BatchTopK SAE, provide a <code>BatchTopKTrainingSAEConfig</code> instance to the <code>sae</code> field. The primary parameter for TopK SAEs is <code>k</code>, the number of features to keep active. If not set, <code>k</code> defaults to 100 in <code>BatchTopKTrainingSAEConfig</code>. Like the TopK architecture, the BatchTopK architecture does not use an <code>l1_coefficient</code> or <code>lp_norm</code> for sparsity, as sparsity is structurally enforced.</p> <p>Also worth noting is that <code>BatchTopK</code> SAEs will save as <code>JumpReLU</code> SAEs for use at inference. This is to avoid needing to provide a batch of inputs at inference time, allowing the SAE to work consistently on any batch size after training is complete.</p> <pre><code>from sae_lens import LanguageModelSAERunnerConfig, LanguageModelSAETrainingRunner, BatchTopKTrainingSAEConfig\n\ncfg = LanguageModelSAERunnerConfig( # Full config would be defined here\n    # ... other LanguageModelSAERunnerConfig parameters ...\n    sae=BatchTopKTrainingSAEConfig(\n        k=100, # Set the number of active features\n        d_in=1024, # Must match your hook point\n        d_sae=16 * 1024,\n        # ... other common SAE parameters from SAEConfig if needed ...\n    ),\n    # ...\n)\nsparse_autoencoder = LanguageModelSAETrainingRunner(cfg).run()\n</code></pre>"},{"location":"training_saes/#training-jumprelu-saes","title":"Training JumpReLU SAEs","text":"<p>JumpReLU SAEs are a state-of-the-art SAE architecture. To train one, provide a <code>JumpReLUTrainingSAEConfig</code> to the <code>sae</code> field. JumpReLU SAEs use a sparsity penalty controlled by the <code>l0_coefficient</code> parameter. The <code>JumpReLUTrainingSAEConfig</code> also has parameters <code>jumprelu_bandwidth</code> and <code>jumprelu_init_threshold</code> which affect the learning of the thresholds.</p> <p>We support both the original JumpReLU sparsity loss and the more modern tanh sparsity loss variant from Anthropic. To use the tanh sparsity loss, set <code>jumprelu_sparsity_loss_mode=\"tanh\"</code>. The tanh sparsity loss variant is a bit easier to train, but has more hyper-parameters. We recommend using the tanh with <code>normalize_activations=\"expected_average_only_in\"</code> to match Anthropic's setup. We also recommend enabling the pre-act loss by setting <code>pre_act_loss_coefficient</code> to match Anthropic's setup. An example of this is below:</p> <pre><code>from sae_lens import LanguageModelSAERunnerConfig, LanguageModelSAETrainingRunner, JumpReLUTrainingSAEConfig\n\ncfg = LanguageModelSAERunnerConfig( # Full config would be defined here\n    # ... other LanguageModelSAERunnerConfig parameters ...\n    sae=JumpReLUTrainingSAEConfig(\n        l0_coefficient=5.0, # Sparsity penalty coefficient\n        jumprelu_sparsity_loss_mode=\"tanh\",\n        jumprelu_tanh_scale=4.0, # default value\n        jumprelu_bandwidth=2.0,\n        jumprelu_init_threshold=0.1,\n        pre_act_loss_coefficient=3e-6,\n        # Anthropic's settings assume normalized activations\n        normalize_activations=\"expected_average_only_in\",\n        # Anthropic recommends using the full training steps for the warm-up\n        l0_warm_up_steps=total_training_steps,\n        d_in=1024, # must match your hook point\n        d_sae=16 * 1024,\n        # ... other common SAE parameters from SAEConfig ...\n    ),\n    # Anthropic recommends decaying the LR for the final 20% of training\n    lr_decay_steps=total_training_steps // 5,\n    # ...\n)\nsparse_autoencoder = LanguageModelSAETrainingRunner(cfg).run()\n</code></pre> <p>If you'd like to use the original JumpReLU sparsity loss from DeepMind, set <code>jumprelu_sparsity_loss_mode=\"step\"</code>. This requires a bit more tuning to work compared with the Anthropic tanh variant. If you don't see L0 decreasing with this setup, try increasing the <code>jumprelu_bandwidth</code> and possibly also the <code>jumprelu_init_threshold</code>.</p> <pre><code>from sae_lens import LanguageModelSAERunnerConfig, LanguageModelSAETrainingRunner, JumpReLUTrainingSAEConfig\n\ncfg = LanguageModelSAERunnerConfig( # Full config would be defined here\n    # ... other LanguageModelSAERunnerConfig parameters ...\n    sae=JumpReLUTrainingSAEConfig(\n        l0_coefficient=5.0, # Sparsity penalty coefficient\n        jumprelu_bandwidth=0.05,\n        jumprelu_init_threshold=0.01,\n        d_in=1024, # must match your hook point\n        d_sae=16 * 1024,\n        # ... other common SAE parameters from SAEConfig ...\n    ),\n    # ...\n)\nsparse_autoencoder = LanguageModelSAETrainingRunner(cfg).run()\n</code></pre>"},{"location":"training_saes/#training-gated-saes","title":"Training Gated SAEs","text":"<p>Gated SAEs are another architecture option. To train a Gated SAE, provide a <code>GatedTrainingSAEConfig</code> to the <code>sae</code> field. Gated SAEs use the <code>l1_coefficient</code> parameter to control the sparsity of the SAE, similar to standard SAEs.</p> <pre><code>from sae_lens import LanguageModelSAERunnerConfig, LanguageModelSAETrainingRunner, GatedTrainingSAEConfig\n\ncfg = LanguageModelSAERunnerConfig( # Full config would be defined here\n    # ... other LanguageModelSAERunnerConfig parameters ...\n    sae=GatedTrainingSAEConfig(\n        l1_coefficient=5.0, # Sparsity penalty coefficient\n        d_in=1024, # Must match your hook point\n        d_sae=16 * 1024,\n        # ... other common SAE parameters from SAEConfig ...\n    ),\n    # ...\n)\nsparse_autoencoder = LanguageModelSAETrainingRunner(cfg).run()\n</code></pre>"},{"location":"training_saes/#cli-runner","title":"CLI Runner","text":"<p>The SAE training runner can also be run from the command line via the <code>sae_lens.sae_training_runner</code> module. This can be useful for quickly testing different hyperparameters or running training on a remote server. The command line interface is shown below. All options to the CLI are the same as the LanguageModelSAERunnerConfig with a <code>--</code> prefix. E.g., <code>--model_name</code> is the same as <code>model_name</code> in the config.</p> <pre><code>python -m sae_lens.sae_training_runner --help\n</code></pre>"},{"location":"training_saes/#logging-to-weights-and-biases","title":"Logging to Weights and Biases","text":"<p>For any real training run, you should be logging to Weights and Biases (WandB). This will allow you to track your training progress and compare different runs. To enable WandB, set <code>log_to_wandb=True</code>. The <code>wandb_project</code> parameter in the config controls the project name in WandB. You can also control the logging frequency with <code>wandb_log_frequency</code> and <code>eval_every_n_wandb_logs</code>.</p> <p>A number of helpful metrics are logged to WandB, including the sparsity of the SAE, the mean squared error (MSE) of the SAE, dead features, and explained variance. These metrics can be used to monitor the training progress and adjust the training parameters. Below is a screenshot from one training run.</p> <p></p>"},{"location":"training_saes/#best-practices-for-real-saes","title":"Best practices for real SAEs","text":"<p>It may sound daunting to train a real SAE but nothing could be further from the truth! You can typically train a decent SAE for a real LLM on a single A100 GPU in a matter of hours.</p> <p>SAE Training best practices are still rapidly evolving, so the default settings in SAELens may not be optimal for real SAEs. Fortunately, it's easy to see what any SAE trained using SAELens used for its training configuration and just copy its values as a starting point! If there's a SAE on Huggingface trained using SAELens, you can see all the training settings used by looking at the <code>cfg.json</code> file in the SAE's repo. For instance, here's the cfg.json for a Gemma 2B standard SAE trained by Joseph Bloom. You can also get the config in SAELens as the second return value from <code>SAE.from_pretrained_with_cfg_and_sparsity()</code>. For instance, the same config mentioned above can be accessed as <code>cfg_dict = SAE.from_pretrained_with_cfg_and_sparsity(\"jbloom/Gemma-2b-Residual-Stream-SAEs\", \"gemma_2b_blocks.12.hook_resid_post_16384\")[1]</code>. You can browse all SAEs uploaded to Huggingface via SAELens to get some inspiration with the SAELens library tag.</p> <p>Some general performance tips:</p> <ul> <li>If your GPU supports it (most modern nvidia-GPUs do), setting <code>autocast=True</code> and <code>autocast_lm=True</code> in the config will dramatically speed up training.</li> <li>We find that often SAEs struggle to train well with <code>dtype=\"bfloat16\"</code>. We aren't sure why this is, but make sure to compare the SAE quality if you change the dtype.</li> <li>You can try turning on <code>compile_sae=True</code> and <code>compile_llm=True</code>in the config to see if it makes training faster. Your mileage may vary though, compilation can be finicky.</li> </ul>"},{"location":"training_saes/#jumprelu-saes","title":"JumpReLU SAEs","text":"<p>JumpReLU SAEs are a state-of-the-art SAE architecture from DeepMind which at present gives the best known sparsity vs reconstruction error trade-off, and is the architecture used for Gemma Scope SAEs. However, JumpReLU SAEs are slightly trickier to train than standard SAEs due to how the threshold is learned. We recommend the following tips for training JumpReLU SAEs:</p> <ul> <li>Make sure to train on enough tokens. We've found that at least 2B tokens and ideally 4B tokens is needed for good performance with the default <code>jumprelu_bandwidth</code> setting. This may vary depending on the model and SAE size though, so make sure to monitor the training logs to ensure convergence.</li> <li>Set <code>normalize_activations=\"expected_average_only_in\"</code> in the config. This helps with convergence and is generally a good idea for all SAEs.</li> </ul> <p>You can find a sample config for a Gemma-2-2B JumpReLU SAE trained via SAELens here: cfg.json</p>"},{"location":"training_saes/#checkpoints","title":"Checkpoints","text":"<p>Checkpoints allow you to save a snapshot of the SAE and sparsitity statistics during training. To enable checkpointing, set <code>n_checkpoints</code> to a value larger than 0. If WandB logging is enabled, checkpoints will be uploaded as WandB artifacts. To save checkpoints locally, the <code>checkpoint_path</code> parameter can be set to a local directory.</p>"},{"location":"training_saes/#optimizers-and-schedulers","title":"Optimizers and Schedulers","text":"<p>The SAE training runner uses the Adam optimizer with a constant learning rate by default. The optimizer betas can be controlled with the settings <code>adam_beta1</code> and <code>adam_beta2</code>.</p> <p>The learning rate scheduler can be controlled with the <code>lr_scheduler_name</code> parameter. The available schedulers are: <code>constant</code> (default), <code>consineannealing</code>, and <code>cosineannealingwarmrestarts</code>. All schedulers can be used with linear warmup and linear decay, set via <code>lr_warm_up_steps</code> and <code>lr_decay_steps</code>.</p> <p>To avoid dead features, it's often helpful to slowly increase the L1 penalty. This can be done by setting <code>l1_warm_up_steps</code> to a value larger than 0. This will linearly increase the L1 penalty over the first <code>l1_warm_up_steps</code> training steps.</p>"},{"location":"training_saes/#training-on-huggingface-models","title":"Training on Huggingface Models","text":"<p>While TransformerLens is the recommended way to use SAELens, it is also possible to use any Huggingface AutoModelForCausalLM as the model. This is useful if you want to use a model that is not supported by TransformerLens, or if you cannot use TransformerLens due to memory or performance reasons. To use a Huggingface AutoModelForCausalLM, you can specify <code>model_class_name = 'AutoModelForCausalLM'</code> in the SAE config. Your hook points will then need to correspond to the named parameters of the Huggingface model rather than the typical TransformerLens hook points. For instance, if you were using GPT2 from Huggingface, you would use <code>hook_name = 'transformer.h.1'</code> rather than <code>hook_name = 'blocks.1.hook_resid_post'</code>. Otherwise everything should work the same as with TransformerLens models.</p>"},{"location":"training_saes/#datasets-streaming-and-context-size","title":"Datasets, streaming, and context size","text":"<p>SAELens works with datasets hosted on Huggingface. However, these datsets are often very large and take a long time and a lot of disk space to download. To speed this up, you can set <code>streaming=True</code> in the config. This will stream the dataset from Huggingface during training, which will allow training to start immediately and save disk space.</p> <p>The <code>context_size</code> parameter controls the length of the prompts fed to the model. Larger context sizes will result in better SAE performance, but will also slow down training. Each training batch will be tokens of size <code>train_batch_size_tokens x context_size</code>.</p> <p>It's also possible to use pre-tokenized datasets to speed up training, since tokenization can be a bottleneck. To use a pre-tokenized dataset on Huggingface, update the <code>dataset_path</code> parameter and set <code>is_dataset_tokenized=True</code> in the config.</p>"},{"location":"training_saes/#pretokenizing-datasets","title":"Pretokenizing datasets","text":"<p>We also provider a runner, PretokenizeRunner, which can be used to pre-tokenize a dataset and upload it to Huggingface. See PretokenizeRunnerConfig for all available options. We also provide a pretokenizing datasets tutorial with more details.</p> <p>A sample run from the tutorial for GPT2 and the NeelNanda/c4-10k dataset is shown below.</p> <pre><code>from sae_lens import PretokenizeRunner, PretokenizeRunnerConfig\n\ncfg = PretokenizeRunnerConfig(\n    tokenizer_name=\"gpt2\",\n    dataset_path=\"NeelNanda/c4-10k\", # this is just a tiny test dataset\n    shuffle=True,\n    num_proc=4, # increase this number depending on how many CPUs you have\n\n    # tweak these settings depending on the model\n    context_size=128,\n    begin_batch_token=\"bos\",\n    begin_sequence_token=None,\n    sequence_separator_token=\"eos\",\n\n    # uncomment to upload to huggingface\n    # hf_repo_id=\"your-username/c4-10k-tokenized-gpt2\"\n\n    # uncomment to save the dataset locally\n    # save_path=\"./c4-10k-tokenized-gpt2\"\n)\n\ndataset = PretokenizeRunner(cfg).run()\n</code></pre>"},{"location":"training_saes/#list-of-pretokenized-datasets","title":"List of Pretokenized datasets","text":"<p>Below is a list of pre-tokenized datasets that can be used with SAELens. If you have a dataset you would like to add to this list, please open a PR!</p> Huggingface ID Tokenizer Source Dataset context size Created with SAELens chanind/openwebtext-gemma gemma Skylion007/openwebtext 8192 Yes chanind/openwebtext-llama3 llama3 Skylion007/openwebtext 8192 Yes apollo-research/Skylion007-openwebtext-tokenizer-EleutherAI-gpt-neox-20b EleutherAI/gpt-neox-20b Skylion007/openwebtext 2048 No apollo-research/monology-pile-uncopyrighted-tokenizer-EleutherAI-gpt-neox-20b EleutherAI/gpt-neox-20b monology/pile-uncopyrighted 2048 No apollo-research/monology-pile-uncopyrighted-tokenizer-gpt2 gpt2 monology/pile-uncopyrighted 1024 No apollo-research/Skylion007-openwebtext-tokenizer-gpt2 gpt2 Skylion007/openwebtext 1024 No GulkoA/TinyStories-tokenized-Llama-3.2 llama3.2 roneneldan/TinyStories 128 Yes"},{"location":"training_saes/#caching-activations","title":"Caching activations","text":"<p>The next step in improving performance beyond pre-tokenizing datasets is to cache model activations. This allows you to pre-calculate all the training activations for your SAE in advance so the model does not need to be run during training to generate activations. This allows rapid training of SAEs and is especially helpful for experimenting with training hyperparameters. However, pre-calculating activations can take a very large amount of disk space, so it may not always be possible.</p> <p>SAELens provides a CacheActivationsRunner class to help with pre-calculating activations. See CacheActivationsRunnerConfig for all available options. This runner intentionally shares a lot of options with LanguageModelSAERunnerConfig. These options should be set identically when using the cached activations in training. The CacheActivationsRunner can be used as below:</p> <pre><code>from sae_lens import CacheActivationsRunner, CacheActivationsRunnerConfig\n\ncfg = CacheActivationsRunnerConfig(\n    model_name=\"tiny-stories-1L-21M\",\n    hook_name=\"blocks.0.hook_mlp_out\",\n    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",\n    # ...\n    new_cached_activations_path=\"./tiny-stories-1L-21M-cache\",\n    hf_repo_id=\"your-username/tiny-stories-1L-21M-cache\", # To push to hub\n)\n\nCacheActivationsRunner(cfg).run()\n</code></pre> <p>To use the cached activations during training, set <code>use_cached_activations=True</code> and <code>cached_activations_path</code> to match the <code>new_cached_activations_path</code> above option in training configuration.</p>"},{"location":"training_saes/#uploading-saes-to-huggingface","title":"Uploading SAEs to Huggingface","text":"<p>Once you have a set of SAEs that you're happy with, your next step is to share them with the world! SAELens has a <code>upload_saes_to_huggingface()</code> function which makes this easy to do. We also provide a uploading saes to huggingface tutorial with more details.</p> <p>You'll just need to pass a dictionary of SAEs to upload along with the huggingface repo id to upload to. The dictionary keys will become the folders in the repo where each SAE will be located. It's best practice to use the hook point that the SAE was trained on as the key to make it clear to users where in the model to apply the SAE. The values of this dictionary can be either an SAE object, or a path to a saved SAE object on disk from the <code>sae.save_model()</code> method.</p> <p>A sample is shown below:</p> <pre><code>from sae_lens import upload_saes_to_huggingface\n\nsaes_dict = {\n    \"blocks.0.hook_resid_pre\": layer_0_sae,\n    \"blocks.1.hook_resid_pre\": layer_1_sae,\n    # ...\n}\n\nupload_saes_to_huggingface(\n    saes_dict,\n    hf_repo_id=\"your-username/your-sae-repo\",\n)\n</code></pre>"}]}